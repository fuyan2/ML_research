# -*- coding: utf-8 -*-
"""log_gan_ninv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ogaqvuazHlL5L9pygDSROZ4E8M45Lu-i
"""

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.image
import pdb
import copy
import multiprocessing as mp
import os
import gzip
import struct
import array

import numpy as np
import tensorflow as tf
import random
import math

from gan_mi import conv_Generator, conv_Discriminator
from snwgan import snw_Generator, snw_Discriminator
inf = 1e9

tf.reset_default_graph()
tf.set_random_seed(1)

# Training Params
num_steps = 100000
learning_rate = 0.00002
x_dim = 28*28
noise_dim = 128 #20
NUM_LABEL = 10
GAN_CLASS_COE = 10
gan_batch_size = 200
num_data = 10000
INV_HIDDEN = 100
beta = 0 #1, 0.5
lemda = 0.0001

cnn_gan = False
#Fredrickson Params
ALPHA = 100000
BETA = 5
GAMMA = 1
LAMBDA = 0.06

# Load MNIST data
def mnist(type):
    def parse_labels(filename):
        with gzip.open(filename, 'rb') as fh:
            magic, num_data = struct.unpack(">II", fh.read(8))
            return np.array(array.array("B", fh.read()), dtype=np.uint8)

    def parse_images(filename):
        with gzip.open(filename, 'rb') as fh:
            magic, num_data, rows, cols = struct.unpack(">IIII", fh.read(16))
            return np.array(array.array("B", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)

    train_images = parse_images('data/emnist-'+type+'-train-images-idx3-ubyte.gz')
    train_labels = parse_labels('data/emnist-'+type+'-train-labels-idx1-ubyte.gz')
    test_images  = parse_images('data/emnist-'+type+'-test-images-idx3-ubyte.gz')
    test_labels  = parse_labels('data/emnist-'+type+'-test-labels-idx1-ubyte.gz')

    return train_images, train_labels, test_images, test_labels

one_hot = lambda x, k: np.array(x[:,None] == np.arange(k)[None, :], dtype=int)

def load_mnist(type):
    partial_flatten = lambda x : np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))    
    train_images, train_labels, test_images, test_labels = mnist(type)
    train_images = partial_flatten(train_images) / 255.0
    test_images  = partial_flatten(test_images)  / 255.0
    N_data = train_images.shape[0]

    return N_data, train_images, train_labels, test_images, test_labels

# A custom initialization (see Xavier Glorot init)
def glorot_init(shape):
    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.), dtype=tf.float32)
 
# Linear Regression 
class Generator(object):
  # G Parameters
  def __init__(self, noise_dim, NUM_LABEL, batch_size):
    self.batch_size = batch_size
    self.linear_w1 = tf.Variable(glorot_init([noise_dim+NUM_LABEL, 500]),name='glw1')
    self.linear_b1 = tf.Variable(tf.zeros([500]),name='glb1')
    self.linear_w2 = tf.Variable(glorot_init([500, 50]),name='glw2')
    self.linear_b2 = tf.Variable(tf.zeros([50]),name='glb2')
    self.linear_w3 = tf.Variable(glorot_init([50, x_dim]),name='glw3')
    self.linear_b3 = tf.Variable(tf.zeros([x_dim]),name='glb3')
    
    self.training = True

 # Build G Graph
  def __call__(self, z,y):
    z_y = tf.concat((z,y),1)
    linear_z1 = tf.nn.leaky_relu(tf.matmul(z_y,self.linear_w1) + self.linear_b1)
    linear_z2 = tf.nn.leaky_relu(tf.matmul(linear_z1,self.linear_w2) + self.linear_b2)
    out_layer = tf.matmul(linear_z2,self.linear_w3)+self.linear_b3
    return out_layer

# Logistic Regression
class Disciminator(object):
  # D Parameters
  def __init__(self):
    self.linear_w1 = tf.Variable(glorot_init([x_dim + NUM_LABEL, 100]))
    self.linear_b1 = tf.Variable(tf.zeros([100]))
    self.linear_w2 = tf.Variable(glorot_init([100, 1]))
    self.linear_b2 = tf.Variable(tf.zeros([1]))

    self.training = True

  # Build D Graph
  def __call__(self, x, y):
    x_y = tf.concat((x,y),1)
    linear1 = tf.nn.relu(tf.matmul(x_y, self.linear_w1) + self.linear_b1)
    out = tf.matmul(linear1, self.linear_w2) + self.linear_b2
    out = tf.sigmoid(out)
    return out

class Classifier(object):
  def __init__(self):
    self.linear_w1 = tf.Variable(glorot_init([x_dim, NUM_LABEL]))
    self.linear_b1 = tf.Variable(tf.zeros([NUM_LABEL]))
#     self.linear_w2 = tf.Variable(glorot_init([1, NUM_LABEL]))
#     self.linear_b2 = tf.Variable(tf.zeros([NUM_LABEL]))

    self.training = True

  # Build D Graph
  def __call__(self, x):
#     linear1 = tf.nn.relu(tf.matmul(x, self.linear_w1) + self.linear_b1)
#     out = tf.nn.softmax(tf.matmul(linear1, self.linear_w2) + self.linear_b2)
    out = tf.nn.softmax(tf.matmul(x, self.linear_w1) + self.linear_b1)
    return out
  
def lrelu(x, alpha):
  return tf.nn.relu(x) - alpha * tf.nn.relu(-x)

class Inverter_Regularizer(object):
  def __init__(self, weight_shape):
    self.w_model =  tf.Variable(glorot_init([weight_shape, INV_HIDDEN]))
    self.w_label = tf.Variable(glorot_init([NUM_LABEL, INV_HIDDEN]))
    self.w_out = tf.Variable(glorot_init([INV_HIDDEN, x_dim]))
    self.b_in = tf.Variable(tf.zeros([INV_HIDDEN]))
    self.b_out = tf.Variable(tf.zeros([x_dim]))
    
  def __call__(self, y, model_weights):
    # Input Layer
    ww = tf.matmul(model_weights, self.w_model)
    wy = tf.matmul(y, self.w_label)
    wt = tf.add(wy, ww)
    hidden_layer =  tf.add(wt, self.b_in)
    rect = lrelu(hidden_layer, 0.3)
    # Output Layer
    out_layer = tf.add(tf.matmul(rect, self.w_out), self.b_out)
    rect = lrelu(out_layer, 0.3)
    return rect
  
def plot_gan_image(name, epoch, sess):
  # Generate images from noise, using the generator network.
  fig, ax = plt.subplots(2, NUM_LABEL)
  for i in range(NUM_LABEL):   
    # Desired label
    d_label = np.zeros([gan_batch_size, NUM_LABEL])
    d_label[:, i] = 1
    # Noise input.
    z = np.random.uniform(-1., 1., size=[gan_batch_size, noise_dim])
    g = sess.run([gen_sample], feed_dict={gen_input: z, desired_label: d_label})
    g = np.reshape(g, [gan_batch_size, 28, 28])
    g = g[:2,:] #only pick one image

    # Make background black instead of grey
    g = np.where(g<0,0, g)
    g = np.where(g>1, 1, g)

    for j in range(2):
      ax[j][i].imshow(g[j], cmap="gray", origin='lower')

  plt.savefig(name+epoch)

def average_images(images, labels):
  avg_imgs = np.zeros((NUM_LABEL, x_dim))
  for i in range(NUM_LABEL):
    imgs_for_label = images[labels == i, :]
    avg_imgs[i] = np.mean(imgs_for_label, axis=0)

  return avg_imgs 

###################### Build Dataset #############################
features = tf.placeholder(tf.float32, shape=[None, x_dim])
labels = tf.placeholder(tf.float32, shape=[None, NUM_LABEL])
batch_size = tf.placeholder(tf.int64)
sample_size = tf.placeholder(tf.int64)
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
dataset = dataset.shuffle(sample_size, reshuffle_each_iteration=True)
dataset = dataset.batch(batch_size, drop_remainder=True).repeat()
iterator = dataset.make_initializable_iterator()
next_batch = iterator.get_next()

#Loading data
digits_size, digits_x_train, digits_y_train, digits_x_test, digits_y_test = load_mnist('digits')
# letters_size, letters_x_train, letters_y_train, letters_x_test, letters_y_test = load_mnist('letters')
# print('training dataset size:', digits_size)
# avg_imgs = average_images(digits_x_train, digits_y_train)
# fig, ax = plt.subplots(10)
# for i in range(10):
#   ax[i].imshow(np.reshape(avg_imgs[i], [28,28]), cmap="gray", origin='lower')
# plt.savefig('log_gan_ninv/train_avg')
# plt.close()

digits_y_train = one_hot(digits_y_train, 10)
digits_y_test = one_hot(digits_y_test, 10)

################### Build The Classifier ####################
x = tf.placeholder(tf.float32, shape=[None, x_dim])
y = tf.placeholder(tf.float32, shape=[None, NUM_LABEL])
model = Classifier()
y_ml = model(x)
# Build Inverter Regularizer
model_weights = tf.concat([tf.reshape(model.linear_w1,[1, -1]),tf.reshape(model.linear_b1,[1, -1])] ,1) #, tf.reshape(model.linear_w2,[1, -1]), tf.reshape(model.linear_b2,[1, -1])], 1)
weight_shape = int(model_weights.shape[1])
inverter = Inverter_Regularizer(weight_shape)
inv_x = inverter(y, model_weights)

# Calculate MODEL Loss
inv_loss = tf.losses.mean_squared_error(labels=x, predictions=inv_x)
class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_ml))
model_loss = class_loss - beta * inv_loss + lemda*tf.nn.l2_loss(model_weights)
y_pred = tf.argmax(y_ml, 1)
correct = tf.equal(tf.argmax(y_ml, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

# Build Optimizer !Use model_loss
model_optimizer = tf.train.AdamOptimizer(0.001).minimize(model_loss, var_list=[model.linear_w1, model.linear_b1])#, model.linear_w2, model.linear_b2])
inverter_optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(inv_loss, var_list=[inverter.w_model, inverter.w_label, inverter.w_out, inverter.b_in, inverter.b_out])
grad_model = tf.gradients(class_loss, [model.linear_w1, model.linear_b1])#, model.linear_w2, model.linear_b2])

#################### Build GAN Networks ############################
# Network Inputs
gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')
aux_x = tf.placeholder(tf.float32, shape=[None, x_dim])
aux_label = model(aux_x)
desired_label = tf.one_hot(tf.argmax(aux_label, 1), NUM_LABEL) 

if cnn_gan:
  aux_x_reshape = tf.reshape(aux_x, [-1, 28, 28, 1])
  G = snw_Generator(noise_dim, NUM_LABEL, gan_batch_size)
  gen_sample = G(gen_input,desired_label)
  gen_sample_reshape = tf.reshape(gen_sample, [gan_batch_size, 28*28])
  gen_label = model(gen_sample_reshape)

  D = snw_Discriminator()
  disc_real = D(aux_x_reshape)
  disc_fake = D(gen_sample)
  # G Network Variables
  gen_vars = [G.linear_w, G.linear_b, G.deconv_w1, G.deconv_w2, G.deconv_w3]
  # D Network Variables
  disc_vars = [D.conv_w1, D.conv_w2, D.conv_w3, D.conv_w4, D.conv_w5, D.linear_w1, D.linear_w2, D.linear_b1, D.linear_b2]
else:
  # Build G Networks
  G = Generator(noise_dim, NUM_LABEL, gan_batch_size)
  gen_sample = G(gen_input,desired_label)
  gen_label = model(gen_sample)

  # Build 2 D Networks (one from noise input, one from generated samples)
  D = Disciminator() 
  disc_real = D(aux_x, desired_label)
  disc_fake = D(gen_sample, desired_label)

  # G Network Variables
  gen_vars = [G.linear_w1, G.linear_b1, G.linear_w2, G.linear_b2, G.linear_w3, G.linear_b3]
  # D Network Variables
  disc_vars = [D.linear_w1, D.linear_b1, D.linear_w2, D.linear_b2]
  gen_weights = tf.concat([tf.reshape(G.linear_w1,[1, -1]),tf.reshape(G.linear_b1,[1, -1]), tf.reshape(G.linear_w2,[1, -1]), tf.reshape(G.linear_b2,[1, -1]), tf.reshape(G.linear_w3,[1, -1]), tf.reshape(G.linear_b3,[1, -1])], 1)

# Build Loss
gan_class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=desired_label, logits=gen_label)) + 0.01 * tf.nn.l2_loss(gen_weights) #0.007
gen_loss = -tf.reduce_mean(tf.log(tf.maximum(0.00001, disc_fake))) + GAN_CLASS_COE*gan_class_loss
disc_loss = -tf.reduce_mean(tf.log(tf.maximum(0.0000001, disc_real)) + tf.log(tf.maximum(0.0000001, 1. - disc_fake)))

# Create training operations !
train_gen = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(gan_class_loss, var_list=gen_vars)
train_disc = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(disc_loss, var_list=disc_vars)

# Fredrickson Model Inversion Attack, Gradient Attack

def fred_mi(i, y_conv, sess, iterate):
  label_chosen = np.zeros(NUM_LABEL)
  label_chosen[i] = 1
  
  cost_x = 1 - tf.squeeze(tf.gather(y_conv, i, axis=1), 0)
  gradient_of_cost = tf.gradients(cost_x, x)
  x_inv = x - tf.scalar_mul(LAMBDA, tf.squeeze(gradient_of_cost, 0))
  x_mi = np.zeros((1, x_dim))
  previous_costs = [inf,inf,inf,inf,inf]

  for i in range(ALPHA):
    x_mi = sess.run(x_inv, feed_dict={x: x_mi, y: [label_chosen] })
    cost_x_mi = sess.run(cost_x, feed_dict={x: x_mi, y: [label_chosen] })
    max_cost = max(previous_costs)
    
    if(cost_x_mi > max_cost or (iterate and cost_x_mi == 0)):
      print("Early break, no ALPHA HIT")
      break;
    else:
      previous_costs.append(cost_x_mi)
      previous_costs.pop(0)

    if(i % 5000 == 0):
      print('step %d, current cost is %g' % (i, cost_x_mi))

   # Make background black instead of grey
  for i in range(x_mi.shape[1]):
    if(x_mi[0][i] < 0):
      x_mi[0][i] = 0
    if(x_mi[0][i] > 1):
      x_mi[0][i] = 1

  print('iteration hit:', i+1)
  check_pred = sess.run(correct, feed_dict={x: x_mi, y: [label_chosen] })
  print("Prediction for reconstructed image:", check_pred)
  return x_mi

def train_gan():
  # Train Classifier
  
  # Initialize the variables (i.e. assign their default value)
  init = tf.global_variables_initializer()

  # Start training
  with tf.Session() as sess:

      # Run the initializer
      sess.run(init)

      sess.run(iterator.initializer, feed_dict = {features: digits_x_train, labels: digits_y_train, batch_size: gan_batch_size, sample_size: 60000})

      # Train the Classifier First
      for i in range(40000):
        batch = sess.run(next_batch)
        model_optimizer.run(feed_dict={ x: batch[0], y: batch[1]})
        # inverter_optimizer.run(feed_dict={ x: batch[0], y: batch[1]})
        if i % 1000 == 0:
          gradients, train_accuracy = sess.run([grad_model, accuracy], feed_dict={x: batch[0], y: batch[1] })
#           print('gradients: ', gradients)
          print('Epoch %d, training accuracy %g' % (i, train_accuracy))    

      test_acc, y_prediction = sess.run([accuracy, y_pred], feed_dict={x: digits_x_test, y: digits_y_test})
      print("test acc:", test_acc)
      
      # Train Fredrickson MI
      fig, ax = plt.subplots(NUM_LABEL)
      for i in range(NUM_LABEL):
        print("i = %d" % i)
        inverted_x = fred_mi(i, y_ml, sess, 1)[0]
        ax[i].imshow(np.reshape(inverted_x, (28, 28)), cmap="gray", origin='lower')
      plt.savefig('log_gan_ninv/fred_mi_attack')
      plt.close()


      # Initialize Aux dataset for GAN train
      sess.run(iterator.initializer, feed_dict = {features: digits_x_test, labels: digits_y_test, batch_size: gan_batch_size, sample_size: 40000})
      
      #Train GAN
      for i in range(80000):      
        # Sample random noise 
        batch = sess.run(next_batch)
        z = np.random.uniform(-1., 1., size=[gan_batch_size, noise_dim])
        #! Train Discriminator
        # train_disc.run(feed_dict={aux_x: batch[0],  gen_input: z})
        # if i % 5 == 0:
        train_gen.run(feed_dict={aux_x: batch[0],  gen_input: z})
       
        if i % 2000 == 0:
          gl,dl,cl = sess.run([gen_loss, disc_loss, gan_class_loss], feed_dict={aux_x: batch[0],  gen_input: z})
          print('Epoch %i: Generator Loss: %f, Discriminator Loss: %f, Classification loss: %f' % (i, gl, dl, cl))
          
          plot_gan_image('log_gan_ninv/gan_out',str(i), sess)
      

        
if __name__ == '__main__':
  train_gan()