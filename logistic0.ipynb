{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuyan2/ML_research/blob/master/logistic0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UbSar6wsliRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "141b50eb-e74b-45d9-9785-4b3f326ede55"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import copy\n",
        "import multiprocessing as mp\n",
        "\n",
        "from datetime import datetime\n",
        "from skimage.measure import compare_ssim\n",
        "\n",
        "#Defining Parameters\n",
        "IMG_ROWS = 28\n",
        "IMG_COLS = 28\n",
        "NUM_LABEL = 10\n",
        "INV_HIDDEN = 5000\n",
        "EPOCHS = 200\n",
        "learning_rate = 0.1\n",
        "BATCH_SIZE = 250\n",
        "\n",
        "#Flatten input dataset\n",
        "x1_0 = np.reshape(np.random.uniform(-11,11, 500), [500,-1])\n",
        "\n",
        "x2_0 = np.sqrt(15**2 - x1_0**2) \n",
        "x3_0 = np.sqrt(14**2 - x1_0**2)\n",
        "x4_0 = np.sqrt(13**2 - x1_0**2)\n",
        "x5_0 = np.sqrt(12**2 - x1_0**2)\n",
        "x6_0 = np.sqrt(11**2 - x1_0**2)\n",
        "\n",
        "x_0_0 = np.concatenate((x1_0, x2_0), axis=1)\n",
        "x_0_1 = np.concatenate((x1_0, x3_0), axis=1)\n",
        "x_0_2 = np.concatenate((x1_0, x4_0), axis=1)\n",
        "x_0_3 = np.concatenate((x1_0, x5_0), axis=1)\n",
        "x_0_4 = np.concatenate((x1_0, x6_0), axis=1)\n",
        "\n",
        "\n",
        "x1_1 = np.reshape(np.random.uniform(-1,21, 500), [500,-1])\n",
        "x2_1 = - np.sqrt(15**2 - (x1_1 -10 )**2)-0.3\n",
        "x3_1 = - np.sqrt(14**2 - (x1_1 -10 )**2)-0.3\n",
        "x4_1 = - np.sqrt(13**2 - (x1_1 -10 )**2)-0.3\n",
        "x5_1 = - np.sqrt(12**2 - (x1_1 -10 )**2)-0.3\n",
        "x6_1 = - np.sqrt(11**2 - (x1_1 -10 )**2)-0.3\n",
        "\n",
        "x_1_0 = np.concatenate((x1_1, x2_1), axis=1)\n",
        "x_1_1 = np.concatenate((x1_1, x3_1), axis=1)\n",
        "x_1_2 = np.concatenate((x1_1, x4_1), axis=1)\n",
        "x_1_3 = np.concatenate((x1_1, x5_1), axis=1)\n",
        "x_1_4 = np.concatenate((x1_1, x6_1), axis=1)\n",
        "\n",
        "# print(x_1.shape)\n",
        "x_train = np.concatenate((x_0_0,x_0_1,x_0_2, x_0_3, x_0_4, x_1_0, x_1_1, x_1_2, x_1_3, x_1_4), axis=0)\n",
        "# print(x_train.shape)\n",
        "# print(x_train)\n",
        "y_0 = np.full((2500,1),0, dtype=np.int32)\n",
        "y_1 = np.full((2500,1),1, dtype=np.int32)\n",
        "y_train = np.concatenate((y_0,y_1), axis=0)\n",
        "# print(y_train.shape)\n",
        "# print(y_train)\n",
        "# plt.plot(x_train[0:2500,0], x_train[0:2500,1],'.')\n",
        "# plt.plot(x_train[2500:5000,0], x_train[2500:5000,1],'.')\n",
        "train = np.concatenate((x_train,y_train),axis=1)\n",
        "np.random.shuffle(train)\n",
        "# print(train.shape)\n",
        "# print(train)\n",
        "plt.plot(train[:,0], train[:,1],'.')\n",
        "x_train = train[:,[0,1]]\n",
        "y_train = train[:,[2]]\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ -8.39488205   8.57472771]\n",
            " [  5.33967606  14.01741273]\n",
            " [ 10.47216807   7.70283687]\n",
            " ...\n",
            " [ -4.74595484  12.10272336]\n",
            " [ 16.96561006 -11.27635078]\n",
            " [ 10.10749142 -13.29955559]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXl4W9W19l/N8+x5jO3YjjPHjjOR\nmZQQAinQUqaEJEBLS8vt97XMFAjQNLc33N6vhdIWKGFqKYWGXuaElKQhk+3MiZPYju3YjgfZlizL\nsizJkv39oUixrOnIli1ZWr/nyfPkHOkc7e2zz1ln77XWu1iDg4ODIAiCIAgiorAj3QCCIAiCIMgg\nEwRBEERUQAaZIAiCIKIAMsgEQRAEEQWQQSYIgiCIKIAMMkEQBEFEAdxI/nhHR08kfx4AoFKJ0dVl\njnQzIgb1n/pP/Y/P/sdz34HI9j8xUeZzf9zPkLlcTqSbEFGo/9T/eCae+x/PfQeis/9xb5AJgiAI\nIhogg0wQBEEQUQAZZIIgCIKIAsggEwRBEEQUQAaZIAiCIKIAMsgEQRAEEQWQQSYIgiCIKIAMMkEQ\nBEFEAaMyyNXV1Vi1ahXeffddAMDjjz+Om266CRs2bMCGDRuwb9++cLSRIAiCIGKeEUtnms1mvPDC\nC1i4cKHH/p/97GdYsWLFqBtGEOFEqzdjV3kD7I4B5KYr4XAMIitZii8qGjE5VY6zdTpcajVCJuZi\nVn4i8jNU0BktSE+QQMiPqMIsQRBxwoifNHw+H6+99hpee+21cLaHIDwwmKw4XatDqkaM49UdyE2T\nIT9DhYa2Hhyvbsf5S13QqMTgsQaRlSJHfoYC2Sly6IwWSIU8VDUZkKoRY9u7x93nPHBGG/A3D1V2\nuP8v4rMwf2oKFk5PRV1zNwYBzMlPhL7Hgq4eC+pbe5CTKodEyIXNPgBdtwUahRBSEQ+pGgkZdYIg\nGMMaHBwcHM0JXnrpJahUKqxfvx6PP/44Ojo60N/fD41Gg6effhpqtdrvsXa7Iyr1RImxp7yyFa/9\n71mo5AKsWTAJswuS0KTtgclsw4V6PaqaupCbrsAXRxrgcHgOURYLCDRq2SxgYMjnLACjGuQjhMth\nwe4YhFIqwL03TYVGIYLFZsd7uy9gek4Cbl2ZD6GAi8Y2I7JS5BAJyGgTRDwTVoN8+PBhKJVKFBUV\n4dVXX0VbWxueeeYZv8dGQ7WnxERZVLQjUoxV/y02O6oaDThR04YLjUbMK0qCQiJEfoYCeyoaceCs\n5yx1uBGNF5JUIrR39UGjEOLh22ejTd+LVp0ZQgEXzR0mFGQqMSNXM2YzbBr/8dv/eO47ENn++6v2\nFNa7fKg/eeXKldiyZUs4T09EKQaTFWWV2ivLuQk4U9eJD/bWon/IzPbTQ40BzxGKMQ424x0+g+Zw\nWHj0zjk4fLbVy4d8sdXo5UOelKrAX/dcZN4gPzB5yWjv6gMA6LotePLVI179+texZgj4HPzse7Mg\n4HHw0f46OAYGUFKYiCSVGDmpcloOJ4gYIax38kMPPYRHH30UmZmZKCsrQ35+fjhPT0QY56y3C206\nM1I0YhRmqdDe1Yfn36xwG56/7x2ZIQtkvG5fkYfcdMWIfcgz8zRQSgXIz1B6nXvRnEx0dPR4fTYj\nNwH/OtoENpuFgYFBlE5NDtmHLBHy8NybFQGX11UyAbp6rAD8v2RYbQ4PHzgAnK3vAgDwOEDplCTc\ndE0uAODAmVYsnpGKZLXY/48SBBGVjHjJ+uzZs/j1r3+N5uZmcLlcJCcnY/369Xj11VchEokgFoux\nbds2aDQav+eIhuUSWrbx33+LzY7j1e04fqEDShkfZefb0WtxuD/XKATQG60BDU4wcpMkWDkvE1Mn\nadCq64Wprx+1zQbUNfcgP1OJb5VmQikVjPwHgjDW199gsuJYVQfkEh6kIh4sVjs+PnQJhZkKXD9/\nEgBg69tHoTNaw7psf9OibKwozoCQz0FzZ6/fwDIa//Hb/3juOxCdS9aj9iGPhmgYDDQonf3X6s3Y\nc7QJAj4bQh4XyRox3v+6BnqjbVTn5wBQKwSYPy3Z7UM+f0mPQQALpqWMqbFlQjRcf4vNjubOXmjk\nQjS0Gd0+5KpLOpRd6BzxedksQKMUoqPLArmYh++tnAwelwVjbz9KCpOglAqiov+RJJ77H899B8gg\nexENgyGeB6VWb8bB81rY+mzYfbQ55OM1CgEMPTY4BgbBYgE3LszGoumpON/YhfJzWsyfmoT5U1Oi\n2scZ7de/UduD3RVNuK40EwDw0f469NsduNTaDbNtdLdubpocGUlSJMgEWDwrLeIvR5Eg2q//WBLP\nfQfIIHsRDYMh3galcxm6A+XntDhdpw/5eDYLuHFRFnJSFSjMUsFic+B0rc7tp41WtHozdpU1wDEw\niNx0OeyOQWQly9xBXZX1nahrNkIm5mF2gXP2+O+TLbiuNBNZyb5vnkjicidUnG+HTMzH/KnJ+O/3\nT3l8h80CJEIuevrsjM65dFYyzNYB3LhwUlT2eSyIt/t/KPHcd4AMshfRMBhieVC6RDVm5mkg5HNQ\n32rEG5+dh85oZXR8glKItQuycOaiDglKIVQyIeaP8zKztzCIHPkZSjS09eBETTsq6/VIVArBZbOR\nlSJFfoYa2SmygMIgoTKvMAHXlmahtrkbLACz3UFdVtS3Gj2CuvTdFqgVAkhF/HEXBtHqzdh3shnJ\najEcjkGUFCbCanPgydeOhOznv640HXnpSkhFvJiO5I7l+z8Y8dx3gAyyF9EwGGJpULqM16QUGU5d\n7MT/HqjHwCDAZgMJCpE7xcYfS2YmQybmQ8jjIjNZisIs1Zg9iLV6Mz47XI8ecz9Ki5IxdZL6SlCX\nDbXN3ahr6UFWsgz7TjR7BTpx2Cw4AkQ/Df88UsIgrnY4/bd5UMmEzqCug1eCuhZMGpeXG1dgmUjA\nwXt7LqDXEtpfQyHhorQwGdfOzYy56O1Yuv9DJZ77DpBB9iIaBsNEH5QWmx31rUZcau3BB/tqQz5+\nyexUJMiEWDJGPsQT1e348N91mFuQCLmUj/wMJcrPteHzsiaP77HZwMBA2H8+6tn2gwXQ91gAAKka\nCRraetCqM0Mk4KBNb8by2elhNYKupe6ySi36bA40tffA2s/8EbDtBwsg4HMmhJuCCRP9/h8N8dx3\ngAyyF9EwGCbioNTqzdhzrAlWqx1HzmlhD2LIWCwgUemcISerRVg9LwtVDV1YsyAbJdPTwtJ/1+y8\nMFOJC416HDmrBZ8LnLnUPepzDyfUGbJbGORMK2MfMpfNwks7z46qnUxm5nIJD8be/oD9mpmnwa1L\nnXnGH/27FoMAbl2WFxY/r1QuwsHjjWjS9sLQa8GBUy2wOfx/f1VxBvaeanbLmRZlq5CgEOKGBdkT\ncvY8Ee//cBHPfQfIIHsRDYNhogxKrd6MA2dakZsqC8lQyMQ8PLm+BAop32c+6kj670rTSU+QwGJz\nYN+JJnx6qHFUObT+ZsgsFnDb8jzkuYVBQvchB5rJBeq/wWTFgdMt6O61YV5Rcsg+ZImQh+ffOoqB\nMdIEvWZ6IrpN/VDKBFi7MGdEBnF4/13iL606M+QSPj7YW4Pu3qtBYbcsycVH39T5PJfrxWEiBYRN\nlPt/LIjnvgNkkL2IhsEQzYPStRx9oroDe44xT0t6YF0RTH0OJCiEKMxSBvQDB+t/o7YHnx6uh1om\nhFTIg0zCx0f762A09yNJJUKnoW/UYhY3L5mEpbPSvXzIBZkKrJqbFQPCIO1XhEH4Hj7kFcWZ+H8f\nnoJW7/TtB5v5B+PeGwqx90QLZudpIBbx3LnGgQjWf4vNjjN1nahr6cHy2ekQ8Dl4+JWDAd0Ld62a\njK4eG7Q6M4qnJKC4IClqg8Ki+f4fa+K57wAZZC+iYTBE26B0+fgOnGrBxZYe2B3MLo+Yz8LUnATc\nuCi0lJXh/XcZkASFEFwO2yuVZqSsKnH6QvMzlDh3qQsGUx+6jDasDbG94SbS19/10gV4+pA5HBY+\n2l8Li23kjnUOm4Vf3j8fbfpeD7nT0a6QaPVmbH3nKEwM06lkIg7uXTst6MthJIj09Y8k8dx3gAyy\nF9EwGKJlULoezEzTkkQCNkryE9FusGDh9OQRC3BwBTx8Xd6AVI0YB0634pvTrYyPlYl56O3r95gh\nP7G+GC2dJhw5q0V6kgQJClFUKHL5I1quvy9cOeNnanVYsyAbAPDh3ouovtwNW7DAgSvIRJ55yHIx\nF0VZKvRa7chNk+O7q6bAbu0fcdsOV7bh4uVuWPuDt0cm4mHxzFTIxXzMn5YcFWMimq//WBPPfQfI\nIHsRDYMhGgalxWbHC28dRavOHPS7d6zMQ16GclS5rS71pylZCuz4vGpEKUEsFvCr7zsjbssq26JG\nCnMoQ/OwrTYHdpU1wu4YQF6Gj6CuOj3qWoyQibiYXZgIpVSAr49dxow8DYoLEqNqZucqbVnd1IUm\nbQ+UMgEKMpV44/Mqj++5CmMEo6RQg/QEKXJS5SNKdXMZ55M1Hahr6YG+J/gLJYsF3LO6IOIVq6Lh\n/o8U8dx3gAyyF9EwGCJxUVzL0icv6jB7sgYqmRDb3zsZ8JhElRB3rJiMOQVJI/7N5s5eWKx2xsvQ\nLp8mn8vC4pmpUEoEUCsE6LM6y/+NVw6tpzDIkGpPNe04X9+FBJXIGdSVLEF+hhLZKXI0tBnx8s6z\ncAwMjto3K+Jz8OAt09HU3nslqCthWFCXDBIhHza7HfpuG9QKPqQi/rgbmsp6HXbur3P7kKfnaPCb\nD06io8vC+Bw8NnDz0lzweRxGPujhuF4Wzl3SY/+pFkYzZ4WEh01riiKypB3PRime+w6QQfYiGgbD\neF8Ug8mK53aUo7v36jKhRs4Hl8uBVu9MS7p+XhaOnm9Dr8WBqTmqEQU2uaJlO7stmJ6jwe/+cRqt\nOjNjkYz7b5yCqZM0Y6o05RKsSFAIkZ0iGxLUZURdixFZyVL8+2RryMZ0eD3kSJGoEuK7y/I8Iq8t\nVjs+OXgJ+ZkKrBkHYRDXOGhqN+Gro43oMQfIaRoGm83CvTcUQiUTjujlwuWG6eqx4I3PLgQN/pMI\nubiuNHPMcuJ9Ec9GKZ77DpBB9iIaBsN4XJShObov/u2ETx/xI3fOBp/HGbHxG2qAs5JlePkfp9DT\n53z4Minr98C6ItRe7oFMwseSWalhfSC6Zk2d3RbkZyhw7pIel1q7UT6kklE4Sw8OZ7Qz5LHEtzBI\nL1J9BGCNFlfEdPn5dhyrCq2KlFjIwaN3Fo84AM9gsqKsUouuHgt2H70c9PsLixKxbknemOc2x7NR\niue+A2SQvYiGwTDWF0WrN+MXr5cFXDpNUAjw/H3zRzQDqWo0oElrwu6KepgCSCIqJHx099rA4bDg\ncAxCyOdg4/UF0Pf2ozgvIWwPPovNjrJzbThc2YaMRAnMff0ov9A5ZsbWF64ZcqJKiOvmZqKkMClk\nH7JYwMVLH54KKJIRDgIJg4j4bFw/LwstejPWzM9GkkoUsLZxKLhyrLt6beg123CmVoc+BhHdLqUu\nVyT+SF4aDCYrvjnVgo++qQ/63etK03D9/JwxmzHHs1GK574DZJC9iIbBEO6L0qjtwftf16C714Zr\npqdgV3kTjOary9NyCR/GXhsUEh5y0+QoKUwMOU9Tqzfjy7JGlJ1vY5QW40p/MVn6oZELPZahR9t/\n18zHbLWDx2Vh7/HL6DKFHrUbiNtX5CHXLQzCzIfMdKk9UP+dM0od6lqMKC5IRG2zkbEPmc/l4C97\natBpYO6/DYZSyoHB5ACfA3x7SS56Lf1oN/ThxoU5I565uvrvDsy62IGcFDm+LG9Cj9n7Ol5bko59\nJ1rcLw5cDlBaMLLZbCiG+ee3z4LOaA27XGc8G6V47jtABtmLaBgM4boojdoe/HV3FaqbjX6/M9Qw\nhjrLcS17a+QCxkFZbDZw8zW5WBxgCZpp/13+QFNfP3TdFqRqxEhRS9yz/9GyZXMpei39Hj7kiS4M\n4nIjtOnMPn3IK8MoDPLQrdPB53Hwz2/qcfOSHEzL0TA6zl//Xde7vcuMt76sdu+/dWkOdu73bUBH\nqtRlMFlxpLINnd19+Pp4S8DvsllAcWFi2EpExrNRiue+A2SQvYiGwTDai2KxOfWk3/6yKuD35BI+\nnri7OORZhMVmxxeHL+GTw42Mj3no1uno6bMzmk0EmyG6au5WX+5Cn9VzqEiFHJgszNd0l85MxsqS\nLJy7pEdLhwkXmgyYmafGjYtyI5YuFemHki9hkJrLXfj8SFOQIwOzsjgNNy7KgZDPCbjMzaT/LtnW\nxTNSIeBz8MgrhwK+ONy1ajIMJhvau8y4cVFos3et3ox/7r+IsgvBfdxPrC9GfoaS8bl9EenrH0ni\nue8AGWQvomEwjPSiuAJk3t1Vg54+/0u0SikH379pRkhRqlq9GR8fqEWzrg+NWhOjY25amAWbYzDk\n6kCu/mv1Zuw52gQBnwUhn4tktQTvf10NvTHw8vPwXFcOm4Vbl+biVG0HMhIlsNrsqG0x4bbleSNO\n2RpLovWh5KptLBPz0dTeg3lTkkIudsFhs6BWCNDRZXGXgJQIeeDzOO7xOJL+G0xWfHW0CV8cYfaS\neNvyXExKlYd8D3x+5BK+Od3m9ztCPgcPrJvq0Z9QidbrPx7Ec98BMsheRMNgGOkD6fk3y2EI4CtN\nkAtw3bxMLJ6ZFtKDolHbgy07KoJ+T8Rno7QoCWarY8TLd1q9GQfPa2Hts+Gro8y1sl0kqoT42W2z\ncbKmE2arHWIBN2oUmABvYZAvyxvgcAwgL10Ju2NgWLUnHeqajZCL+ZhVkAClVICvjjYhTSPG8jkZ\nUdEnrd6Mzw45a0hPy1Xjckcv9p9irqw2FB4byEuXY86UFJSOMKe8UduDj/bX4XyDHjZ78MeIVMjB\ndfOysHgm87SmoTW+9xxtxIEzWp/fS1QKcV1pVsj58fFslOK57wAZZC+iYTAwvShavRlfljdgYAA4\nfbEd3X7yOXkc4OE7Q19Kcy1d/n7nGZit/peB5SI2Hrh55ohnBK5l6PLz7Thdqw/5eJmYi2+VZCIz\nWRr2tJxAuGaMntWetDhXb0CCSgQem4XMJCnyM13CID14aedpDAyEJ+3poVuno72rD4MA5nhVe5JB\nIuTBZneMuzDI0OXklk4TXvnnWTiuxPlx2CyIBRwP6Ux/rCx2GsnMpNCv60iUutZdk4XlczJDMp4W\nmx1Pv34EOqPN73dYAEoKExlrusezUYrnvgNkkL2IhsHA5KKUnWvFnz4+H/A7cgkXm9cUMX6YXTUw\nzqjh7e+dCCidmSgXYHlxOlYUZ4zYEIeilQ0ACUoh1i7IwplaPVI0IuRnKMfMCF8NGhtS7SlDgXlT\nU3D6og47h5T8C2ZgxzKnORSSVCJ8Z1mOh5G2WO34+JCz2tP188MvDDI0H92V7vXU60cCVmcajkzE\nxZoF2SOSQnWl4p2p6wwaoMUCsGxWGlbPz2LsZnGNE1v/AP76r+qAKmS3Lc8Ner/Es1GK574DZJC9\niIbBEOiiaPVmfHKwDocq2/0eLxVxsWF1AWbkJjAyVK7ZxOufXjXwSgkXhl7vWQyHDZQUJOCWpZND\n8gsPjYg29vZjeo7ardQViCUzUyATcyHkc5GZJAur8XVJd0qFPJyo6UBLhwnnG7qgVgixaHoKPi9r\nDEniMRQmkjBIq64XAMI6u3ZV8OJyWHhnVzXjlxUWC7hvbREkQu6I/LQuJTYOhxU06HHRtGTMyk/A\njFwN498YOs7/9HGlz5eOBKUAz9/rP8c/no1SPPcdIIPsRTQMBl8XxWCy4sCpFuwMkB95/41TQpYU\ntNjseO7NCneay1A0CiF03RYkq0W4Y+XkUTwA2/FleQN03VeX9QLNGJfOToNGJgirXKFWb8a/jjaB\nz2fD0GPBxWYjei029AYQLgkFpjPkJJUI35qbiZLCxBH5kD85UIezlwxhabM/ZGKeO993aL9kYg6K\n85OQqBKjqb0Ha+ZnhyXNx2CyYt+Jy6huNKCqqTukwiKJSiGeu3feiF4UDCYr9h6/jE8PNwSUNRUL\nONi4ppDxC+7Q8+873oyPD13y+mz1vExMyfK9uhPPRime+w6QQfYiGgbD0IviUpl6e1e134eGgMfB\nE+tDkxB0BaYoJDz89sMzXp8rJVw8s3n+iHSjXbOE+tYefLiv1u/35GIejOZ+t1b2hcYurJmfjZLp\naaO+Dk5xEGfFp8wkadhqKA+FzQKWzEzD1BwVIx9yOIRBgKs+2pl5GtQ1dzP2IfO5XPxlTxU6Dczc\nA0x4YF0RjpzVwjE4iJLCxFFXSrLY7GjsNOPwqWYMAPiGQYDYT787E9kpMnewXKgvcS6FsC/LGmC2\n+l9H53OAeUUpWLtoUkirQyeq2/1Go4uFHGy83tPYx7NRiue+A2SQvYiGweC6KM7I6QoYTL4DRian\ny7BoRhoWTE1mvDTd3NkLHoeNF9466pbOVMkFHupNCgkXz26eP6IH25dHLmHviRb0OwJfQjYb2Hr/\nAp+CJKEOStcs3CWb2N7Vh+ffrBi1z1Yt52PDdYWw2R1ePuRLbT1hV2hyMT7CIH0+fcgr5ngKg7DZ\nCMnXCziDCEunJOOma3JGJH86tP+uuAaNXIBPDl7yGQx265Ic/O+hS3BcGXNTs5VQywRYuyi033cF\nF767uwoWW+DBs/H6gpDqfbv6YbE68O9T3n5ssYCNR+8qQVayLK6NUjz3HSCD7EU0DAapXISDx5vw\nxmfnfD6Ari1Ox6q5mYwfNo3aHvztqyrUtBjhGPBeLr77W/lIS5DA1u8Y0bK0axY/VD3JHwopD6tL\nswIG5zCZIe451gQBj4PBQWBXeaO7PxoFH3qjLaTKShqFAMtmpqG9y+z2IS+bnRayfGi4iPRDaagw\niFomxH+9dwJdDKKUfXHvDYXYe6LFXX6RSfnEYEpdXT0W/PmzCxgcdL4wfHdZHv6+1/dKzKw8NW5Z\nmhfS6pHbMO+qgqXf/0BSy/nOFZ0QSkIaTFY8/MohvzWhN15fgLVL82EyeruQ4oFIj/1IQwZ5GJEe\nDFq9Gb9+7wQMfh6AoSgBafVmvP91NU5e9J9KxGGzsP3BRSHP9E5Ut+P9ry8iLVGChrZudPUETmN5\n6NbpkEsFI1qyHWoguBw2tr17PKS2DmdugQZN7b1YND0FkzOVES1G74toeygNNYT1LSao5AI0tfdg\n9mRN0Ej/4bikWtv0ve6o6+Fjj0n/h+ZzA8DDrxwMOJO/a1U+WCxWSDnBLsP8+qcXAn6PzQJWzGH+\nkqzVm7H1nQqY+nynEmrkAjx1z9yoyDMfb6Jt7I83ZJCHEcnBoNWb8eRrR7xmd1IRB/OLUkKeFQcT\n8whVOtMZWNaKC016nGMQWCTiAdPyEkIuNDBUqWtXWQMqqtrRe0UOM1jtZI2CD0NPPxwDg2CxgJsW\nZmPh9FS06c1XDMDIBCfCxXBhkF3lDc5qT+kK72pP9c4iEnIxD7PyE6CUCvHV0SYkKYVjmu4VCo3a\nHnz071rYHANoajfBxCC/WCbmosfs/B6bDdyxcjJ0Rqtb0W0kDyWt3oxfvl3hHieBWFmchhm5GsZ/\nP2fBiVY0thuDlojcsrmU0Vh3uQ5qLht8SpLKxTxsvmFKVFzj8YQMMhlkD8b7j+Hy62rkQjy3owLd\nvZ7+YrVcgF+E8Lbsqlbz6aFLfv24D6wrQoJSzDhYy2Kz48DpFvx1z0VGbZiUIsWy2Wkh+dhcaPVm\nHDzXBpvVjt0VwWvUuhia6mWxOUYc4BNqW4fmbTe0GXGyphNnL3UiUXGl2lOKFPkZamSnyDyEQcKB\nWsbD8jnp4HI4PoK65JAIubDZHdB1W5GqkaAwSzmmD3dX+lzF+XbIxDwUZCrwxueeqUXDZU2Hk5ss\nRXaGAlMylSGlG139/XYcqdTiQmMX7EFss0LKx6brC0Myelq9Gdv+csxdnnI4Ij4bP/lOaCI5NZcN\n+M+/HPfpZgn1/p/okEEmg+zBeP4xDpxuxo7PqzAI542nHyKOwQLww5unhphLHHh5LUkuwIY1UxhX\n3bkq3HEuoBIR4BTvHzrLCQV3wYgL7TgVYHkd8PR/iwVsXFuShdw0+ZgYm6E5pbXN3ahrNiI/S4n5\nRck4XdvpUWEomPBHNOQdq2QCPHrnHLTpe72qPX1+pDGkikxMqazXYef+OrcPeXqOBr/54CSj/G4R\nn40VxemQivghC4K4xtTJGh3qWrqh7/E/fiUCNlbPmxSwAtnwc7vy1z/YexHHa7xnzRqFEE9tKAnp\nRbqssg2fH2nwihtRywW4b21R1LlWxgIyyGSQPRiPP4bFZscnB+rwRbnnDNBllF0PTqb+qH0nm1Fx\noS1g0YWf3z4rpIetxWbHC28dDSjcwQIwK1+D21fkjyia1mXw/vxZZdCCEQCglPLx2F3FbtGKcD2g\nnKX2tGju6MH5BgMS5HwsnJE6psIgkSLYi8MD64owa3Ki22c/FkbAVdP53d1V7qVrJty6JAeLR5Cb\nbrHZ8c3pVry3pybod++/cUpIwXwGkxUP//6gz7+pUsbH2gWhB31te/c4OgzeQV1iAQeP3hVaeuNE\ngwwyGWQPxvqPYTBZ8cJbR31Grf70uzMgFfMxa0pK0ChLi82Ovccv44N9dX6/wwJwy9JJWDwzndED\noVHbgy/KGjAjVw2VTIjt7530+907V+VjyczUkJcUXRKK03M0QZW6WAAWz0iBUiZAbpoiLLNgg8mK\nTw/Vo7Jej3lTksDjcfzW0g0FpjPk4cIgofqQPz90CafqQtf7DoUEpcCdq+yS2tR1WyEUcNHcYULB\nCJaTfeF6IbvYZMBHBy4xOobFAn747Wkj+v1GbQ8+PXwJahkfh8+2+62IFqrhc6XdySU8vP91rcdK\nFxB6vWRnlkUj3tld7XUuALhlSU5YRXOiCTLIZJA9GOv8zx1fXPDpf1JIeNj2wEJG5ecsNjue+NMB\ndPf6dkayWcCNC7OxvJhZRSBfaUsauQBcLhtafd8V4Y5MnKjuhELCxw0LmQsjuIKYUjVivLzz9NVg\nngAGbMnsVCTKRIyXEIP9dmGmEhcau/DNqWbUtTIrHckEDpuFxTNTMHWSipEPOVzCIEOrDZ2/pPcj\nDHLVh9ym68O+ky3o6rGGbek1qDqFAAAgAElEQVRcwOPgZ7fPgoDHwaeH65GmkYyqApWzSlMtes39\nqG3tCarWpZDyMbcwEatKmAc6DiVYjj8ALJqWdEU6k7lCl8Fkxda3j/rVZg9FyzpYcZdw1F6ONsgg\nk0H2YCz+GAaTFb98+6jPt10AuH5+BtZdk8tIqUerN+Nve6p9zpI2Xl8Au2MwpCWyQNKZj9w5G3we\nJ2SlLtd5D5xuwXt7Lvp9uCokfHT32twG/0KjISxKXU7JQqck4mhsj0bOx/rrCmGzD3j5kCeaMMjQ\n4MGGNqOHD1nX3ecOvnIKxfBHpObFYgHL56TDYLKipCBhxHncXAEPn+2/iC5TH7462hI0p3zbDxZA\nwOeEHMgXLNLZRajSma7zvrzzrM+Xn1C1rANlTITqiop2yCCTQfYg3H8Mg8mKLTsqYBwWPS0T87Bm\nfjYW+KjV61/LutWjwtBQHrp1OuYUJIXUrkDSmQkKAZ6/z/9DwxcWmx1Hzmlx6EwrmrRGWAO4B135\nqKNV6rqqPGVGikaMFLUET712JCRDzGGzcMOCbOiNfW4f8tI56XElDDI0HUvI57irF41GalMl42L9\nt6aEHDA2tP9Di1Ds/Hctenzk7l5bnIF9p5rdSl2z8tQoLUoK2Re870QTPj7Y6Pc7Ai7wxAZmaU3B\nzunUslb5dMP4uv4uLfY9x71rhF9XmoHr52fHxBI2GeQYM8jV1dV48MEHsWnTJqxfvx6tra149NFH\n4XA4kJiYiO3bt4PP5/s9Ppx/DIvNjqf/XA5dt2dgULBUhuEXJVBaBOAMxJk/NZVxu7R6M37xetkQ\n6cyrMyIBl4U7VhVgPkM5TheN2h7851+Ow2ILnGvCZgM3XxM4OCfQoHRFzx6r6oBKJkDZuTb0Wq4u\n3Q/NcQ1EgpyPhdNSIJcKQlpRGA+i6aHkeuFp1ZkhFHBx4ZIO5RcC5+L6Y2VxOm5cFLy8YyClrqrG\nLrz5xQV0D3H73Lok1+eLqkzMxXP3hiYB26jtwa//cgx9Nv+5aaEawEBa1lIRDxtW5zPWsq65bPAp\njDNSgZ9oI5rGfiSIKYNsNpvxwAMPYNKkSSgsLMT69evxxBNPYOnSpVizZg1+85vfICUlBXfddZff\nc4Tzj1Hb0o2tbx9zbzNN9h/qQxpeFnEoc/IT8O3FzEQ3Kut1+Mf+OpQWJmJXeSOMQ4yWSzoTCC2q\n1lXkIDdV5veB4+KBdUWw9g8yWlIcPiiHBs38/evaoLWTWSy4X15YLODxu4vR0tmLw2dbMTlDiVVz\nQytCP95E+0OpUduD3RVNuK40Ez1mW0iFO64qdZnRqutFqkbsdT8wiaE4U+cMeFs+Ox0CPgePvHLI\n5/Iwj8PCw3fOCcnXelU6sxqWft+GmQXgWYYiIMDVbAir1Y59PgpmiPhsPHY3My3rynqdz7/5pjVT\nsHRWGqP2RCvRPvbHmpgyyHa7HXa7Ha+99hpUKhXWr1+PlStX4ssvvwSfz8eJEyfwxhtv4KWXXvJ7\njnDPkF2pQ6HkJbqiLN/eVe0zGvva4gysmpvBOC3q3d0XUOlHWWukb9Z7jzfhnd3B00jmTE7At5eM\nXKnrs0OXcLCyjbE2daJKiJ/dNhsnajrAAjB/BAXtI81Eeyi5XphEAg5qW3qw98qy6tAXo6EMLe8I\nAHIxB8UFSSiapMKM3ARkpqtGpNT1wtvlMFt8G9A7V+Wju8eKdkMfblzELNo5WG4/iwXctnyyT7eT\nPwKlSQHAPdcX4salk4NmWQw3yjRDjg1iyiC7eOmll9wGeeHChTh8+DAAoLGxEY8++ij+9re/+T3W\nbneAy+WM5uc96LPa0dhmRFaKHCJB8Jlna6cJj770jd/oz+0PLcaUScx8cQdOXsav3znm93OlTID/\n+slipCZIGZ2vvsWA93ZdQE+fHWdrdX6/x2YB6xbn4taV+VDJhYzO7aK104RPDtSCAzb+6cdf7os7\nv1WA/GwVpucmMPo7E2NHl9GCivNtKC1KQU1TF7a9VQH7Ff9u0LrRABbPTsWimekonpIc0rXss9qx\np/wSXv1nZdDv/uDmaVg1bxKj83cZLdi5twb/3O9/PE7PVWN5cQaWFmcGPWdrpwk//3//9lk4BnCK\nivzP/1kW9N5p7TThk29qoZIJsWpedsj3GkEwYcwMckNDAx577LGABjkatawBZ93UR+9klhsZTOpS\nJeXj/pumMl6ettjs+Kq8MWiuqFjAwV3fyg85GMq1NF9+oR2nL/o39IAzQnXtgkk4XduJBKUIKqkw\npBnKRCDWZgnD88//54NTaO8KXs1IxAdWzc0OOQfdYLJiz9HGgJHTgFNoZmMI0plavRmfH76Eb860\n+f2ORs7HC/cvCHq+qxHe3fj8iHfQl0LCx7ObS2NqXDMh1sZ+qETjDDms0xuxWAyLxQKhUAitVouk\nJOaRyOOJxWb3Gbilkglwz+qCkB4av3rnqN+37zXzMnDT4lzGDzeLzY5n/lyOzm7/ilV3rMxDXoYy\n5PSoq9Kc54P6hQFPFaVls9MZ/w4RWYR8LmZNTnRvb9lciqpGA5q0Juw51gCj2XcgYJ8N+ORQAwBA\nJuLihgXZjFwQSqkA312ej3lFKfiirAE1TQaf0pkGkw2//fAMJEI2Vs/LxuKZgcU2ktVibF47Fbcs\ny0NZpRZ/33fR637VGW0oP9+OeUWBX0pdfxPnvwSvQK3uXhu27KjAljg0ykR0wdmyZcuW0ZygvLwc\nIpEIM2fOxMWLF9HX14cpU6Zgx44dKC4uxrRp0/weazYH1mwONxabHQ3aHui6Lfh6WErDj26ehjtX\n5SMjUQYuhx30POXntdj+3knY7N5+NA4LeHx9MZbMSg96Ls9ztuPQWd8zAjaAH6wrwrLZGVDLhIzO\nazBZUX6+HUI+B9vfO4Fd5U3o8yF6ADh9dIunp2BuYSK+v24a8jNUjNs+kZFIBOM+DscTLoeNFLUY\nBVlKLJ+TAYWEhzNB1Mds9gFUXurC7oompCWIoZEHH28KqQBzC5OwdHYaJqcrIBGyUe9DGKbfPojz\nDQbsLm9ColKARKUo4LmFfC4mZyiwbHYaOCygptno/owF4OTFThw5pw1qlF1o5M762wqJAI3tPbBd\nCSSz9jtw+GwbFkwLvUjLRCXWx34wItl/icT3i9+Il6zPnj2LX//612hubgaXy0VycjJefPFFPP74\n47BarUhLS8O2bdvA4/H8nmO8i0u89WW1W1KRxQK0+r6QtKxds8w3v7iADoP3LJbLYeF7K/KweGYa\no2U0l4ZxqkaC7e+dQKvO7OX343OAe9aErvl74HQLPtpfj0E405/8VT1aOjsNCTLBiHSLY4F4XLbT\n6s34/MgltOnMHgbOHwoJH3OnhK7U5Sri8NmRBr+lIkUCDh4LQTqz5rIBHx+oR3aKBJ8fuapPr5Ly\ncMPCSSFrWQ+X1qXiEvFDNC5Zx5QwiC+cxSXq8UW5p4/rp9+dCamYx0jLGnCmn/zXX4/7lNUDAIWU\nh2c3zWP0MDCYrHhuR7k7v3N49al112SjRdeL2ZOZKzC5Hn7mK9WEhgfzDFXqWj0vC1UNXVizYPRK\nXROdeH8ocQU8fLq/Fp3dZnx9vCXo97f9YEHI8pkGkxXPvVmB7gDSmYtnJGPtwhzG5/anOxCqljVX\nwMNPtn8No9lTYlci4uKRO+ZQcYkYhgzyMMb6j2Gx2fHsjnKfVYQeuXM2irLVjC5K2blW/Olj3/nJ\nQGhVawwmK5758xGYhqkgaRRC6LotSNWI8fTGuSH5nc/UdeJP/3vOb3pHuJS6YhHq/9X+u/J3NXIB\nPjl4yWdsxNqF2bi2JGPk0plNBnxR3uQ3te6GBZlYNTeL8YttIC1rJop6iYky1NR3Yus7x7yMOzCy\nF5CJAo19MsgejPUf43yD3mcVJYWYjW0/XBy0uIRWb8YnB2txqLLD67Nls1KRkSRDSWFiSA8lX2/1\nYgEHv/z+AsYFEQDnw+jLsgbsqbgM/zpHgFTIwVP3lPp9qNBNSf0PqNT1ZZXHzPaJ9cX4r/dOuKUz\ni7KVSJCLcMPC7JCKoOw7cRkfH2zw+TmLBdyzugDzpwb35wbTst4SRFBkeHGJVz46i17L1ReRKdkK\nzMxNCLlG9ESAxj4ZZA/GttqTAW/vuoCuYRGfwyOf/V0Urd6MJ1494vc3fvOTa0LyVQXSsg720BiK\nS8P67S+rAn7v1qU5yEySMlYqi1eo/6EpdVU1GfDmF77FO9YtmoTlxczKjwIuCdhjsPiRzhTw2Hhi\nfQmje8OfkRcLOLhlaa5f3zLT4hIsAL+KsdkyjX0yyB6MZ7Wn1fMysXqe91KYL+nIsso2nKrT4UKD\nt+KWgAtsuZf5jTlcy1qtEKCjywKllI+SEErauZYTK863Q+9DUWwooRh4uimp/6H032Cy4uFXDvoN\nEmQB+O8QXlZ9lSMdTiilD/1pWbNZwDObvO8Lf8Ul/rqn2isancdh46l7mL0gTARo7JNB9mBMqj29\nUe4VoKFRCPHCffN8zhSH+9D8VS/KT5Nh0cxURstolfU67Px3HeZOScSXZY0evri7v5WPSanykPKI\nmUhnSkVc3LBgUsjCHXRTUv9HIp35y7cr0GvxHeC4ZGYKEhRi8LlszGc4HoMJjITykhkoAHN4vIe/\n/gd68YiV2sg09skgezAu1Z5kfPxio/+Ef9dFMZisePr1Mg//EQDMyFXjrlUFjNOiPjlQhy/KL/v8\nPFQNXK3ejL9/XY0TF/3njU5KkWLp7HQsCLFilAu6Kan/I+m/S3v68Nl2VDV1ueU6h8NiAfetZR70\n6K/CEgDcviKPsV66y7f8u3+c8Qogk4l5eHJ9CZLV4qAxJM++UQab3fMEoa4CRCs09skgexAt1Z5q\n6jt9zqxZLOC/f8zsxmvU9mDbuxWw9vv+XC7m4YkrDwEm5/rLV1Wouew7R/SuVZOhM1qxfHb6qH1a\ndFNS/0fb/0ArSy5UMgGe3ui/DOpQDCYr9lQ04vMy79kymwWsmJOOVXOZuXr8+YQB56w7WNqfv1gS\nqvY08YlGgzxqpa7REE6VFLGAi+PVHTD19UOjEOKZTaWYlCIPqAJksdlRc7kb29456lERB3BGJz93\n73wkKERBf9t10zt8LG+pZAI89J0ZuG3FZEYPo5rLBvzy7WNePnAXG67Lx7UlWZieo4FU5F90hSmk\n1kP9H23/pSKeU2BGIUKrrhdmq3e6lMXmwDenWuAYAJLVoqBSl1NzNJiTn4D9p1ow1M4PAqhv7cG/\njl3GnPwEKILcUwqpwOd5AGDfyRasKMlAIA0yqYjnVAljw/2CzOGwsOG6wgkvHEJjP4aUusJBuN9O\nLDY7mjt7GflnLTY7tuyo8BLeZzqzBq4oYp1qwSeHLqHfx5Ld9fMysW5xDuOiEvWtRvzuH6dh9RF1\nymED999YhPlTU4OeKxToLZn6H27XUaC64i6Y+oQNJiuOVLbhH/+u80prEgu4ePHHixinCX78TT32\nnfIUP5GJeXjuXuaCPqHmX0czNPajb4YcUwY5FE5d7MRvPzztsU8u4TMWmA9ULQpgJkrgYmgtZ1/c\nvGQSrivNGpM3cropqf9jle1QVqmF2WrHv441MQqwCna+d3ZV4URNp8f+u7+Vj7QECSOpS3/FW0J5\nCY8laOyTQfYgUn8Mi82OX7x2xKMqjVzMw5YQ3pR/8doRvzKaTKIwXYXmExRC8HkcLwETPpeFjEQp\nbr82f0wjOummpP6Pdf+1ejOeer0MAz6czKFoWRtMVvz85YPupeehuu/JahGe3VTKyCifqdPhrS8u\neN2/arkAv7iHmZ87FqCxTwbZg/H+Y7iWtG39Dg8DyHTZyrWs/Nqn52AYJjjCZgHfXpyDJQyKNAyf\nXScoheCwWdDq+5CsFuGe1YXjJm5PNyX1fzz67xTuaMbHBy/5/JzpbNn5ItsBu92B9/fWenzmkvVk\n+lK9ZUcFjL2e97FGLsAL98+Pi5kyjX0yyB6M5x+j7FwrXv/0PBwD8Kj2lKgS4Ym7i4PexIFUhZbN\nSsO3l+QwfhA89dph9Fk9z/PInbPB53FCrnM8WuimpP6PZ/8btT14/s0KnxHZUhEHT23wL/M6FIvN\njuferIBW7xkDEkpxCYPJim3vHkeHwfMcm9ZMYVzOcSJDY58Msgfj9cc4cLoZb3zuKTUZSrWnQMUl\nElVCPLfZt+jIcCw2O55+vcxLDF8m5uLXP2QWnBJu6Kak/o93/w0mK/Yev4xPDvnWsmY6W3atWJ2u\n1WFXuXeKFJM4DqlchIPHm/DOrgvQ99jA4bDgcAxCLOTi0Tup2lMsE40GOeYr0Gv1Zi9jDAB8Hht5\naQqIBIFv+prLBp/GeFVxOh65czYjY2wwWbH/VAuqGru8jDGbBTy5nnl1J4KY6CilAtyyNA9bNpeC\nx2F5ff76pxfw85cPoFEb+GEp5HNRlK3G6nlZ4Pg4z0s7zwY9h0jAxazJCfjl9xdg05op7qIZZosz\nC0Or9x1oSRBjQUwbZJeu9XAkQhZyUuVBj9fqzXj9k0qv/WwWcMOiSSjKVgc1pFq9GY+8cghvfnEB\nL+88i0SVEACgkvFx+4o8vPjja2JKsJ4gmJKVLMPz980Hy9uWos82gC07KlBZrwt6HqVUgO0/WoR1\n12R5fbb9vROw2Lzzoocj5HMxrygJYqHn/fzXPdUwmAJrxxNEuIgZYZDhWGx2PLejAt29wwqPC9h4\n4fuLIBXxndt+ksNdCj3DIzFFfA6ev4+ZYIjFZseWN8phsTnPMTgI3LIkB+sW5+DmJbmYwsCgjzUk\nDkD9j2T/XcIbAi4HVU3exVwOV2oxdZIKGrkw4HmEfC6mZKuRlSRB+fl29/5+uzNWI81PbMbQ/nM5\nbEzPUWPfyau5yu1dfdhd3sSoDRONSF/7SBONwiAxOUO22Ow4eKbNY3lYLODgRzdPw/YfLw4YfGUw\nWbGrrAFv7fIsMaeRC7Dx+kL890+Cz2gr63V44a0KfLS/zkOOk8NmoaQwCXlpiogbYoKIFpRSAW5e\nmostm0t9fr7t3eNBl55dzClIwpbNpZAMmel+drgBD//+IKNzZCXLsO0HCzAjV+3eNxhiGwhipMRc\nUJev8ouB8guHOvYNJiseeeWQz0Ln2xjWQvVXmSkULevxhAI7qP/R1H+DyYovyxqwu8KzQAsLwPdC\nLC7x2eEGfHbYM3AslGpPQ3OeAac2wH/+kHlxmGgn2q79eENBXWOMxWbH1ne8daDvW1vE6CYqq2zz\nMsYzctWMjLHBZMWbX1T6NMab1kzBf/5wYdQZY4KINpRSAe64tgBPrC/22D8I4P29tXjklUM4dbEj\nqF9YyOfi2pIMsIf5p1//9AKefO1IUL+wUirA48PaYLMP4unXD5NPmRgzYsog17cavcovJqtFjAK4\nDCYrPi/zfJtmAdh8QxEjY/zzlw9i/ymt12cqGTcuchoJIpzkZyixZXOpl0F1DAzitx+eweN/OsTI\nqD6zyXsZ3NBjwwtvVQQ16vkZSqy7ZpLHvl7LAJ7fUcYoUIwgQiVmDLLFZsfbu66mN6lkAvz0uzMY\nyekZTFY8t6MCPearN5lYwMGvfrCA0cx634nLXpVkAGDNvAxs/X5k8osJYqKTlSzDiz++Bt9bkQfO\nMMts7HUKgwQzjFnJMp+GvavHhuPV7b4PGsLyOelexxp6nfnPBBFuYsYgN3f2eqj23H9jEWZNTgxo\nDC02OyrOt2HLG+Xo7vXUtf7l95ktU396sB6fH2n0+uyBdUW4bWUBGWOCGAVKqQDXz8/G9gcXISfV\n0+/WbbKhubM36Dlchn3ulESP/a9/egGtnaagv+9rlk0QY0HMGOT0BAlSNU4DmqoRB12mttjseHZH\nOZ5/vcwjEloh4TMqMqHVm/Hz3x/Ezm/qYR9WevGhW6eHvUwiQcQzSqkAG6+f4rFPIxfA1u9gtHys\nlApw16oCr9nuz3+7P+jStyvyWi5x1h9n6gYjiFCJqSjrUOohn7rYgd9+eMZjH9OKTxabHY//8bCH\nIQcAAY+NJ9aXTCi5PYq0pP5PpP43anvwxZEGFGarsKu8EVp9HzQKIZ7aUMLIvdSo7cGWHRUe+5gW\nlAjl+TIRmGjXPtxQlPUYI+RzGeX4Wmx2vLO72mOfXMK8/GJVY5eXMWazgS2b500oY0wQE42sZBke\n+PZ0ZCZJ3S4qXbcFL7xZjn8da2I825WIrj4jdEYrDp5pYxS5TRoCxFgSUwaZKc2dvR6pUXIxD1s2\nM1umfv/rGrz5pac29rJZqXjxQZLAJIjxIj1BAo3iqnJWl6kff/mqBo/8IXj0dbJajBfumw+1jO/e\n95evqvELBulQBDGWxJVBttjsqG3phkYudPubE1UiRjPjynodnnj1CHaVN6HbdDUATC0X4PZr82NG\nLIAgJgJCPhdPbSiBRu553zkcg/j9zjNBVbWUUgHuu3Gqxz59jw1b3zlGKU1ExIibtRet3oxt7x6D\n0dyPZLUIj91VDJ3Rwqj84onqdry086zXfpfvipawCGL8UUqdvt+qxi68vPOsW9SntsWILTsqsGVz\naUAXUk6qHOmJEjR3XI3U1nVb0NzZi7w0xZi3nyCGExczZFehCJffV6vvQ6uul1H5xUZtj09j/KOb\np+GF+5j5nAmCGBuEfC5mTU7E9gcXIS/NM/L5r3uqAs52hXwu/uf/LsdPvzsT6isz7WS1CCZzP843\n6GmmTIw7MW+QnVWfjozoWIPJire+uOC1/65Vk1E6JZlmxgQRJSilAmxYXeixr7rJiGfeCKyq5a6H\nfP98PHLnbAwOAr/98DS2v3cy6LEEEW5i3iBXNXbB4hkQDTYLQfMIXYUm6ts8fVEyMQ+LZ6aFu5kE\nQYySrGQZFk9P9tjXabAyEg8R8rng8zho77rqvuo0WFHV2BX2dhKEP2LaIPtKbwKAH98yPejs9lhV\nu1ehCYWEj+funUczY4KIUtYuyvHYVst5uNRqZBQ9nZ4gcYt/uOgcpo1PEGNJTBvk+lajR3oTm+VU\n0ZpTkBTwOIvN7i5s7uL2FXnY9gAzbWuCICJDslqMbT9YgNXzMrHx+gJ0m+yM06GEfC6euLvErebF\nZgF2+wClQhHjRswaZIPJij9/dt69rZYL8OKPrwlqjBu1PXj49wfx9721cKnsJaqEWDYnnWbGBDEB\nSFaLcfvKfLBYbPcql8MxiLLKNkbHugpasFgsvL+3Fj9/+SBqLhvGutkEEd60p7KyMvz0pz9Ffn4+\nAKCgoABPP/10OH+CEQaTFVveKPdQ02JSE1mrN3vI6g0CuPtbBbhmRgoZY4KYYMzM04DDZrmN8vt7\nayGT8FBcELgcqlIqgFjIcx83CGDbu8eDplERxGgJu5WZN28efve734X7tIyx2OzY+s4xD2OsUQgZ\nicHvO9nstS8tQUzGmCAmIEqpAN9Zlou/761173v90wtIVF3Cc5sDx4LMzNOABXiUVf2vvx7Hiz++\nhp4HxJgRc0vW9a1G6IYEYsjFPEbiHQaTFWXntB77NAo+VXUhiAnMgmkpYA2r8NTRZcGZus6Axyml\nAjy+vthjn9nqQNm54MveBDFSwm6QL168iB/+8Ie48847cfDgwXCfPiAWmx1v77qqM62WCxhXb9r6\n9lEYhkhi3rp0El64bwG9DRPEBEYpFeDZTaVeZRf/8M9zQf3C+RlKrFs0yWPfW19WB5XlJIiREtby\ni1qtFseOHcOaNWvQ1NSEe+65B7t37wafz/f5fbvdAS6XE66fx+mLHXjqD4fc21t/tAgzJycGOOLK\ncTUdeOqPV49LUonw8iMrg6p4EQQxMegyWvA/fzuOE1Ud7n1sFvDmM6uhkgsDHrfx+V0Y+pSUirh4\n4+nV9Hwgwk5YR1RycjJuuOEGAEBWVhYSEhKg1WqRmZnp8/tdXeaw/bbFZsfv3j9xtS1qEVQibtB6\nl1wBD//912PubbVcgMfvLobJ2AdT2FoXvVBNVOp/vPT/e8vyPAzywCBw6EwL5hUEfml//O5ibHv3\nuHvb1GdHxZlmFGWrx6yt40E8XXtfxHw95I8//hh//vOfAQAdHR3Q6XRITk4OclR4aO7sdddHBYB7\nVhcGXW7W6s348favPXKVmURjEwQx8UhWi/HE+mIPn/IH/6oOmmecn6HE/TcWjXHrCCLMBnnlypWo\nqKjAXXfdhQcffBBbtmzxu1wdbtITJO6SiqkaMSNpzF+8XoaeodHYcgEFcRFEDJOfocR/fGeGe1vX\nbcXWt48G1awuLkhEsloEwLn6Rs8JYiwI65K1VCrFH//4x3CekjFCPhdPb5yL5s5epCdIgs6OT9fq\nPKQxJUIunrpnLgVxEUSMU5ilgkYhdGdj6IzWoCUXhXwunt1Uyvj5QhAjIabSnoR8LvLSFIxulsJM\nJThXQi85bBZ+cc9cWqomiDhAyOfiqQ0l0FwpuaiRC6AJENg19DimzxeCGAkxZZCZYrHZ8bt/nIZj\nYBBKmQC/vH8+ktXiSDeLIIhxQikV4Kl75iJJJYLOaMXWd46RZjURceLSIDd39qJV54zwNvRYYRpe\nn5EgiJhHZ7S4yy3qui3YsqOCjDIRUeLSIA8NAMtIkiI9QRLhFhEEMd6kJ0iQpBK5t429NkYBXgQx\nVsSVQbbY7Kht6QYAPL1xLp66pwS/+T/LyCdEEHGIkM/Fi/+xFHLJ1UwQV4AXQUSCuDHIFpsdz71Z\nga1vH8NzbzorOuWlKUhthyDiGJVciC2bS90BXqkaMa2YEREjbqxR2TmtWzhEq+9DfatxwivtEAQx\nepRSAV64fz6aO3uhkQsptYmIGHEx4mouG/DWl1XBv0gQRFwi5HORniDBC28dRavOjFSNGE9vJF0C\nYnyJ+SVri82OF9877rGPywYp7RAE4cHQ7ItWnRlVjYGrQRFEuIl5g9zc2Yt+h+e+KdkqevMlCMKD\n4VHXL/3jNKVBEeNKzBtkqZDnte+7yydHoCUEQUQzQj4Xy2enubcHBoFjVe0RbBERb8S0QbbY7Hjx\n/ZMe++6/cQqykn2XvhIVj6cAACAASURBVCIIIr5x6RO4SFAEl9QkiHAR0wa5ubPXLSAPODVriwuS\nItgigiCimcIsFRJVTiOcqBKiMEsV4RYR8URMO1JdilytOjM0CiGe2lBCvmOCIPwi5HPx3OZ5lPpE\nRISYH23rrysA4IyqppuLIIhguKo6GUxWlJ9vx8w8DVWCI8aFmLVQFpvdK6eQIAiCCQaTFY/84RAc\njkFwOCxs/9EiMsrEmBOzPuThOYWkT0sQBFNO1+rgcAwCAByOQZRVtkW4RUQ8ELMGeWhFJ9KnJQgi\nFGbmacBhs9zbf99bSznJxJgTk0vWFpsd9a1GfG9FHvg8DvmPCYIICaVUgHXXTMJH39QDAAYBHKvq\nwLUlGZFtGBHTxJyVclV1chWSSFaL8Oym0gi3iiCIicZwvQK5xFtkiCDCScwtWTd39rqNMeCs7ET+\nY4IgQqUwS4lE5VVhkJ3762Cx2SPYIiLWiTmDnJ4gQbL6qh5tslpE/mOCIEJGyOdi05op7m16uSfG\nmphbshbyuXh2UynqW40AKP+YIIiRo5Z5Smf60sYniHARk5ZKyOeiKFsd6WYQBDHBqWoyeG0nq8V+\nvk0QoyPmlqwtNjtqW7rJ10MQxKiZmacBh+NMf+JwWJiZp4lwi4hYJqZmyL7UuWi5miCIkaKUCrD9\nR4twrKodCQohhHxOpJtExDAxZa18qXPlpSki3CqCICYyQj4Hu482oaPLgkSVEM9tnkcv+sSYEFNL\n1qTORRBEuKlq7EJHl7OMa0eXBVWNXRFuERGrxNRrnpDPxdMb51LpNIIgwkbnkJrqvrYJIlzE1AwZ\nuFo6jYwxQRDhoKQwya1rzWGzUFKYFOEWEbEKWS2CIIgAKKUCbH+QAruIsYcMMkEQRBCEfA6+Pt5M\nGRzEmBJzS9YEQRDhhuqrE+MBGWSCIIggaORCD4EQjVwY5AiCCB0yyARBEEHQGS1wOAYBAA7HIHRG\nirQmwk9MGWSSzSQIYiwgjQNiPAh7VMKvfvUrnDp1CiwWC08++SRmzpwZ7p/wCclmEgQxVpDGATEe\nhHWGXF5ejoaGBrz//vvYunUrtm7dGs7TB4SCLgiCGEtI44AYa8JqkA8fPoxVq1YBAPLy8tDd3Q2T\nyRTOn/ALLSkRBEEQE5mwvup1dnZi2rRp7m21Wo2Ojg5IpdJw/oxPaEmJIAiCmMiMqdUaHBwM+LlK\nJQaXG17Vm8x0VcjHJCbKwtqGiQb1n/ofz8Rz/+O570D09T+sBjkpKQmdnZ3u7fb2diQmJvr9fleX\nOZw/PyISE2Xo6OiJdDMiBvWf+k/9j8/+x3Pfgcj239+LQFh9yNdccw127doFAKisrERSUtK4LFcT\nBEEQxEQnrDPk4uJiTJs2DXfccQdYLBaeffbZcJ6eIAiCIGKWsPuQH3744XCfkiAIgiBinphS6iII\ngiCIiQoZZIIgCIKIAsggEwRBEEQUQAaZIAiCIKIAMshEzEDVvgiCmMiQviQRE1C1L4IgJjo0QyZi\nAqr2RRDERIcMMhETaORCcDgsAACHw4JGLoxwiwiCIEKDDDIRE+iMFjgczmImDscgdEZLhFtEEAQR\nGmSQiZiAx2EH3CYIgoh26KlFxASHK9s8tr853RKhlhAEQYwMMshETJCbJvfYPlbVQelPBEFMKMgg\nEzHBjFwNlFK+e9tgslGkNUEQEwoyyERMIORz8dhdxWCzr0RasynSmiCIiQUZZCJm0PdYMDBwJdJ6\nYBCtOpohEwQxcsZb/Y+kjAiCIAhiGJFQ/6MZMhEz5KTKkawWAQCS1SLkpMqDHEEQBOGbSKj/0QyZ\niBmEfC6e3VSK+lZjpJtCEMQERyrkBdweC8ggEzHHu7urqcgEQRCj4kRNh8f22XodktXiMf1NWrIm\nYgoqMkEQxGix2Oz46uhlj30JirHP2iCDTMQU6QkSpGqcb7GpGjHSEyQRbhFBEBON5s5edPVY3dtK\nGR+FWaox/11ayyNiCiGfi6c3zkVzZy/SEyS0XE0QRMi4XuxbdWZoFEI8taFkXJ4l9LQiYg4hn4u8\nNEWkm0EQxATFYnNgZXEGEhRCFGYpx+3FngwyQRAEQVzBYLLikT8cgsMxCA6Hhe0/WjRuBpl8yERM\nM95KOwRBTGyOVbV71FY/Xasbt9+mGTIRs0RCaYcgiImLwWTFp4cuubc5bBZm5mnG7fdphkzELJQC\nRRAEU5wv8BXo7u1379t8QyGUUsG4tYEMMhGzDE2BUkj446K0QxDExKS+1YiuHpvHvj6rY1zbQAaZ\niFmEfC7+4zszwWYD3b02PPnaETRqeyLdLIIgogyLzY4/f1bpsY/DZqGkMGlc20EGmYhpqpoMGBhw\n/n9wEHjuzQpo9ebINoogiKjiTF0n9MZ+j30/uXX6uC5XA2SQiRhnZp4GLNbV7cFBYNu7xyjqmiAI\nN3UtnitnYgFnXJS5hkMGmYhplFIBnt1U6mGUjeZ+qghFEISb5bPTPbYfvas4IhkZZJCJmCcrWYYf\nfnuqx772Llq2JgjC6T82WfqxZXMp1i7MxrYfLEBWsiwibaGkTCIumJGbALWcD73RGUX51pfVyElV\nIDExMjceQRCRx2Cy4pdvH4XeaEWyWoRnN5VGVKuAZshEXCDkc5GfofTY98WRhgi1hiCISNNndeYd\n643Oqk5afV/EXVlkkIm4Yc38bI/t2hYj+qwU3EUQ8cjZuk6vvONIEzaDvHPnTixbtgwbNmzAhg0b\n8Ic//CFcpyaIsJCVLMP9Nxa5tzu7LWhso+Augog3LDY7fv/3kx77lDI+clLlEWqRk7Ault9www14\n7LHHwnlKgggrxQWJSNU0oFVnRrJaBKvNAQuXRRrXBBFH1LcaobuyVA0AMjEPz2yMrP8YoCVrIs4Q\n8rl4euNcPHLnbAwMDuKpPx7Cz14+iJrLhkg3jSCIccBis6NlmK79vTcUjbsIiC/CapDLy8tx3333\nYePGjTh37lw4T00QQWFaatH1FtzRZblynAPb3j1OspoEEeO4KsD95asacDlOcYJktQiFWcogR44P\nI5qff/DBB/jggw889q1duxYPPfQQli9fjhMnTuCxxx7DJ598EvA8KpUYXC5nJE0IK/Ge+hIL/e+z\n2vHo9n+hs8uCBJUQrzxyLUQC/8O71WDx2rf3ZDMeXl86ls2MSmLh+o+GeO5/vPX96Lk2dwU4u2MQ\nD31vFpbMzgj4rBhPRtSK2267Dbfddpvfz+fMmQO9Xg+HwwEOx7/B7YoCcYbERBk6OuJ3ZhQr/T90\ntgWdV2a8nV0WfLa/BstmZ/j9vkrMhUbOh854Ncqysk6HpuauiPuRxpNYuf4jJZ77H299b9T24Pkd\nFe7t1AQJijIUMBn7YBrntvh7EQrbkvVrr72GTz/9FABQXV0NtVod0BgTRDg5VaPz2H5vT03ApWsh\nn4sX7l+AO79V4N7X2W2lmskEEYMYTFY8/2YFBofsW7ckJ+jLN1M3WLgI21TgpptuwiOPPIK//e1v\nsNvt2Lp1a7hOTRBBmZWvQUVVh3vbZh9EVWMXZk1O9HuMkM/FLSvyse/4ZXfUtd7Yh0utRpQUJkVF\nkAdBEKPDYrNjV3kjBoZYYzYLuGZmOuzW/oDHbdlRgfauPiSpRNiyeeyjsMN29pSUFLzzzjvhOh1B\nhERxQRLEgmqYrQPufW06M2ZNDnycSOCMuq5vNWLHF+fxh386gxH/9q+L2P7gIjLKBDGBsdjsePaN\ncnQMiRlhs4BnNpVCJReio8O/QT5Tp0N7Vx8AoL2rD1WNBsyanDCm7aW0JyImcKYzzQN7yIj++mQz\no6UmIZ8LPo+DTsPVvETHwCCOVLaNRVMJghgnvjnd6mGMAeCh78wIWjzCYrPj3V1VHvtadWPvziKD\nTMQMyWoxHrp1hnu7o8vC2CecniCBSsb32Pf5kUYYTFY/RxAEEc2cqG7He3tqPPbJxDxGdY6rGrvQ\n0+f5Mp+qEYe1fb4gg0zEFIVZKveNk6oRQyMXMs5NfnpjKaSiq4GIpr5+PP9WxbgFdBAEER4atT14\naedZr/1Pri8J6gfW6s1exyYoBYwM+WiJn/wOIi5wKXE1d/ZCIxdi+3sn0KozI1UjxtMb5wa8GZVS\nAe5bOxW//fCMe5+hx4b6ViOKstXj0XyCIEaJwWTFtnePee3/+e2zkKwOPMs1mKx44a0KDAyJAFs9\nLxPfXhw8Ijsc0AyZiDmEfC7y0hTQGS1uEYBWnRn/PhHcp1yYpfJauq5q1OP9r2ug1Uc+b54gCP8Y\nTFZseaMc1v4Bj/23Lc/FtBxNwGMbtT144k+HYbY63PvYbBZWz8saN20CMshEzJKeIEGSSuTefn9v\nLZ589UhAv7Br6Votd0ZXs1nAxwcbsau8CU+8eoTkNQkiSnHJYhrNnpHTCgkPK4r/f3vnHdjUee7/\nr/aWbcmyPLCxMcaDPcyeCYEEaGa5ZEB22qY33PzaNA1kAFnQjObeprm3TcIIaUYpNJMQICRhgwdm\nGw+wwVsesqxlSZbs3x/iGG0dCdkS0vv5jyPp+Dyc8Zz3Gd/Hu0gQYA9Tr9tS4ubI75iRNaidFsQh\nE6IWPpeNWyalO23T6C14dWupz5VyvJiH1x6fgodvy3PqXQSAP31aRgq9CIQIw2SxYl9pPTp1zvdm\ngoSLtY9M9rvC3X+q0eP2WWNTQnaMdCAOmRDVTMxVgMlkOG3r1JlRVO67pYnPZWNyfhLixByn7SaL\nDes2FxOnTCBECBq9GS9uLMIXB2udtt88MQ2vPzHV7wpXoze7vaDzuUxs+JX/34Ya4pAJUU28mIfX\nH58CAdf5Ut+6u8rvyEU+l421D09GnMjZKWuNpPqaQIgENHoz1m0pgVrr/ILMZACLp2X6XRnXqXT4\nw/8ewYFTzf3b4sUcrP/VNL8FYAMBcciEqEcpE+L1X00Dh+W8Un5n2yl0m3071XgxDxt+PQ3/MS/b\nabtGZ8HZmvaQHyuBQKCHRm/Gmk3HoTVYnLZLBGxaK+Pmdj1e3lLilJZaPG0o1v9qWtgU+ohDJsQE\n8WIe/nDfeKdt5p5eHDrVQKtHee74NLfq679/VU6KvAiEMGCyWPHKRyXQd1+riBbxWXj6l6PxxpPT\n/a5uTRYr/vjXw07DJpgM4OaJQ8I67Y04ZELMkDMkHquXTwCPY7/sWUwG/vqv0/jde4dpha9feqgQ\nQv61m7UPwMsfleDbI7Ukp0wgDBImixVHzjZDo3deGU8rSMbY4QpaE5zsv3e+Z9c8XBh27XrikAkx\nRc6QePz3ypl44JYc2K7GqsyWXmz4pMyvU44X8/DSg5PgGPju6wO+PFSLP/zvEeKUCYQBxmSx4uWP\nSvDpD9Vun93s0lHhCY3ejJc2Frn9/sk7C/zqWw8GxCEToga6s0v5XDZSE0Vu2//7X6f9/lYpE2Lt\nI4VgOKej0dsHnHAY/0ggEEKLSm3EJ3uroFJ3O22fkp+EDb+a6jdMrVIb8eKHx9HhUgCmlAkwetjA\nTnGiC5HOJEQFJosVL248BrW2BxIhCy8/6ruoIytFCpmUA7W2x2EfNjS2G5CdGufzb2UoJVj/xFS8\nsrUE3Q6qPgIeEwdPN2FMtjzsoS8CIZqoU+mwbkuJ23alTICHbsvzG6bW6M144cPjTgVccikPjy7O\nR1aKNKx5Y0fICpkQFRSVt/Q7V53RhjWbjvtc7fK5bLz2+DQ8tXQMeFdbopQyAdRaE348Ue83/KyU\nCfH6E1ORILE73sQ4HrbsqsRH31fg2f87SmQ2CYQQYbJY8dY/Tzptm1KQhGfvG4e1DxfScsbbfqp2\nE/l5auk45A+V+fw93ahbqIiM1wIC4ToprXAOF+u7bThb04HCPKXX3/C5bCycmoWC9HjUNmux5fsK\n/O2r8wCAz/ZVY/0TvsNg8WIeXn9iChrbDbjcrO3PS9l6+/Dq1hIsmZ6JqSOTyWqZQAgCk8WK2mYt\nmtoNMLiMQrxz5jBafcLnazvw522n3bYr4vkYmZ0Ivbbbw6+u/f21m4vRpjFBEc/Hy4/6V/y6XsgK\nmRAV3D4zy23bp3srab3Z8rlscDkstDsMMu/rA9ZsOu53pUsNspiYmwSWgyKY0WzDv36+hGf+9whZ\nLRMIAWKyWLFmcxHe+vwUPv2huv/eEgnYWPdIIS1nXFTe7NEZL5uXjZcfnQwBz7dzLatqQ9vVZ0Kb\nxoSzNR1BWBIYxCETooKcIfFYefcop21aoxWN7QZav09LFCHepc+4xwa88KHvYRQUlP61Y1sUYHfs\nr31citMX24iyF4FAA5XaiA+/PY92zbX7ztbbh4dvy8NbT073Ww1t17Wuw/vfXHD7LEHCxZzxabRm\nIm/9vsJpW02TNgArgoM4ZELUMH6EvdpSKrRLXabIhZBL+bQrr9c8VAixwPlG7e0Ddh29jB9PNNDK\nK7/2+JT+vDKFwWTFX3acxdotxcQpEwheoBzp6g+O42S182pUKuJgcn4SrR7jtVuK8dm+i26fSYR2\nLQF/OeMDpxqw+oPj6LE5J53njksLwJrgYPT19fX5/9rA0NYWfpUjhUISEccRLqLRfpPFvjKWS/l4\n6/OTaO4wIk7ExaoHJriFulzt1+jNeOWjEjfRAQo67RUmixVnazrw8e5KGEzODvjxJXmYPio1SMtC\nTzSe/0CIZfsjyfbqBg3e/rwMPTb3zxgM+K3noCipaMHfvip3237XzCzcMjndyRl7uvfXbSmC1uB8\nz3LZTDy/YmJI+5QVCs/7Yq1bt25dyP5KgBiNnh96g4lIxIuI4wgX0Wg/m8WETMJHY7sBu47XAQDM\nPTb8eKIBingeFPECsFn24JCr/XwuG3PGpWJ4WhzixVxcanQOUx0+04jCPCXEAueBE65/Py1RjOmj\nknHsXIvTjNWyqnb0WK3osfYiXszrP45wEY3nPxBi2f5Isb26QYMNn5S5VUEDgEjAwsuPTKH1Elx8\noQUfeAhTP7NsLGaOTXW71xztN1mseGmjuzMGgNcen4I0hTgAi/wjEnku9CRV1oSoJS1RhDgRF10O\n4vMbd1YgXlzjUyaPz2Vj7PBE5GbE4+i5FugcBp6be/rw/IfHsfbhQr9vzPFiHtY9Ohmr3z/m5JR3\nHa8HUI84EQdrH5lMqrAJMYnJYkVReQs+3l3l8fP75w/HzDGptNqaXvu41G3i08jMBDy2pMDv/WWy\nWPFDST26DD1un61e7h5VG0hIDpkQtfC5bKx6YAJcRLWg0VuwbksRrbzy88snwmWcMvr6gHVbSmgN\nlogX87B6+USPn3UZevDCh8eJFjYh5qhT6fD79w5j6+4qeMqZrl4+AfMnZdDI9zZi1d+PujljAFi+\nIJfWLOQXNxbhy0POs5RZTGDdI4XIGRJPy55QQRwyIarxJnWpNVhRVtVK6/dv/+cM3D0ry20fdJ1y\nhlKCdY8Uus1kBoBusw1fHqrF79874ldLm0CIBlRqI9ZtKYHJ0uu0nQFg0dQheOepGX4doUZvxur3\nj2Hr7kpYrM4uXchj0WqN6tSasHZzkUdn/tJD/iNgAwHJIUdIHiVcxIL9cWIephYocfBUo1OequJK\nJ3IzEsBhwmcul89lY0RGAsbnJGL/qSanz/afagKL2YekBKHPt/k4MQ83TxyCFLkAZVWe5ygfPtOM\neDEHyTLhoOWWY+H8+yKW7R9M2zV6M46cbYHOaEFpZatbbQZgXxXPGuO/JUmjN2Pd5mJojc4hZi6b\niafuHoX7bxkBeZzA5z5MFitW/e2oW/HmlIIkPHXX6JDnjF3xlkMmVdYRVGkYDmLJfm96uFwOE88s\nG0crPOVtHywmA2/9djqtfLBGb8ah00346lCtx3CdiM/Ciw/SEz+4XmLp/Hsilu0fDNvtHQftTpXP\ncinPacDDmGEy3D0nm9aK1GSx4qVNxejoMrl9tnr5BFr3MDV+0XXiU7yEi/VPTB0UXWtvVdbEIcfw\nDQnEnv11Kh3e+vykWzsSQP+GrlPp8PKWEjdnevesLEjFPNrDJVRqI17dWgKj2b3Xg8EA/uue0cjN\nSBjQB0SsnX9XYtn+gbbdWxsRADx550jUqfSYOTqF1osn5djLqtpRVH4t1STiMZGfJceSaZm0BEPO\n1rTj0x+qoXUp4JII2Xj50SmDVmBJHLIXYvmGBGLTfm9vyDwuC//91AxaDtCTM2Uygd5e+2r5tcf9\nt2pQx1JU3oKtXipN5XF8vLBi4oA9KGLx/DsSy/YPpO0mixXPf3DcYz+/kMfC2/9J7z6j9rVmc5GT\nchcQ2L1h30exkzwuhUzCxYsPee+6GAhIH7IXYjmHBESX/edrO/Dht+WQS3lISvDuDNksJlITRTh+\nvsVpfKLN1gcRn40rKh1kUr7PB4ZYwMH00SkovqCCyWKDRMiG+WqRSl8fcOxcM5hMBhLjBT73w2Yx\nkZksxficRBw43eT2ebfZipILKihlQnTqTJAIOSHNL0fT+Q+GWLZ/IGxXqY347thlNKj0OHnRs/bz\nCw9O8pvjBa7lnWubulDiMjxm+qhk/L+lYyAWcL382nk/Xx+uxfnaTqft8WIeVt4zGvfMyaa1n1BC\ncsheiOU3ZCB67Hed6vLoolzMHONb6s5kseJCfRfe/+osLD29YDLQX/TFYIBWrzGlCibmc/DixiLY\nXNQNAlEZ0ujNKDrfgh5rL34+1YhOnYfVBZ+Fh27NxehhiSEJZUfL+Q+WWLY/VLZT163OaMGuovr+\n7QzAKa0zc1QyFk/P9HsvaPRmHD7dhC8cWpGo6BMFHcU8wPu0JwYDeH/VzWD7cX/U/Z2WKApp6oiE\nrL0QyzckED32v/5xKS65iL+ve8S/Q1UoJKhv7ETxhVZ85CImT3cfFCq1Ea9+XAqjS35axGfj8SX5\nAeWDNXozXv+41Kn4xRF5HA+PLrr+4erRcv6DJZbtD4Xt3oocKe6elQWD2Yq549JoOVBKtcvjvmZn\nwmCy0d5XUXmzxwETQh4LLz1UiFG5Sp/2O8roJsbz8UoIxy+SkLUXYjlkBUSP/XIpD8fOq5y2FV9Q\n4aaJQ3yGeEUiHixmK5QyAY6ea4bJ4lxgdfB0E2aP868WBFwNY49KRvGFVqf99Fh7UVTeip/LGjBq\nmBxxNHJVfC4bs8emoiAzAWdrOmDpce7Z7DbbcPRcCw6dboQ8jg+5lB9UKDtazn+wxLL9wdpuslhR\n3aBBdYMG7/zLffVJkRjPw2NLCjBuuMKn1Cxgf5n9dM8F/POnSx4/T5Dw8MQvRtLal8lixc8nG/HR\n95Vun0mEHLz6+BQkxgl82m+yWLFmU3F/DtxosiJDKUZaYmjaobyFrIlDjuEbEoge+5MShJBLuU5T\nYnpsfSjITIDWaIGQx/bosCj72SwmphQocfy8s/Z0HwBjdw+OlTejx2pz0sH2BKWFnaEUo/yyGj0O\nogU9tj7sP9UES48NVlsv4sVcn/tis5hQxAswIUeBn8oaPH7H3NOL0oq2gJy9J/tjlVi2PxjbTRYr\nXv6oBHtLGrz2088dl4Lbpmbg3ptG0HqRrVPp8NKmYjS2e54bHifmYs3DhbTyvHUqHdZuLsbJavdj\nkwiYePmxqf3FW97sN1msOHCyESdc7JNL+RiVJfd7DHQgOWQvxHLICog+++tUOrz5WRmMZhuUMnvh\niErdDamIi9U0pj2ZLFaUVbVh084L190j7E3AgCJOzMVaH5rarvs6fLoJe0rrYOi2ueXnKB5fkocE\nCZ92KDvazn+gxLL9gdiu0Ztx5lIH4kQc/GXHWa/fo9s6CFzrMPjH3iqn/DCFRMDCo4sLaKV67Pdt\nKzbudE87AcBdMzNxy+QMn9OeAPtK/bWPS2Awubci0s1b04HkkL0QyzckEJ32U4UYlh4b3vr8VP92\nBgN4bHEeJoy4NlfVm/0avRk/nmjAd8euuH3GALCWZm5Zozfj5S3FHoXrAYDHAaaNTMHCyUNpt0k1\nthvAYTHxytZS9HoakQMgTsRGYa4SN09K97nfaDz/gRDL9vuzvU6lw85jlyGT8LCvtAG9ffaWvgQp\n16kFSchjYnK+kvY1TO37T5+UwtTj+fqlO1gC8D5cguKZZWMx0sPK1tV+bwVgIzMTsHxBbkiFekLu\nkIuLi/H0009j/fr1mDdvHgCgoqICVAQ8NzcXL7/8ss99RMKNEMs3JBDd9pssVqx6/zi0BuewFJ/L\nwKoHJiFDKfFpP5VHavegCkS3CpvaT1lVKz79odqpzcqVQB5CgP1BtP9kPb45Uufze5PzEiHgcXDr\nFPcHZjSffzrEsv2ebKde+Exmq0fnBAAP3JKD1EQRLD294HKYARcWqtRGrP7guMfPeFwGVl+9N/1B\nrbA/+6EaPTZ3N3b7jKGYO36I1wgUZb+v1TWDAfz5P2eEvEc5pA65rq4OGzZsAJPJxC9/+ct+h7xi\nxQo8++yzGDNmDJ555hncfvvtmDNnjtf9RMKNEMs3JBD99qvURjz/4XF4usrXPVKIiaNSfdpPhbCL\ny1U4U6N2+/y++TlgMoCJuUm0xryVVbVh4073yk8KHpuBhVPSMXd8Ou2HgEptxP5TjThR2Yr2Lt9T\no1bePQpSMa+/jSPaz78/Ytl+R4d0tqYDJypUOH1J7VRD4QqLxcBbT9KTiKWgHGdReSsmFyihUhux\np7je7XsP3ToCUwqSaYWnK+s6sWXXBWiN7ipgIj4bz943nlaHxblKFTZ8esJNuQsILBIWKCF1yN3d\n3eByuXjhhRewcOFCzJs3DxaLBbfeeit++uknAMDOnTtx7tw5rFq1yut+IuFGiOUbEogN+71JXQp5\nLHy09lbotd309/NRiUfnHqiWtWufpStMBvB2gG/m9geVBh99X44uD3KFjsSJ2Lh1ciYWz86G1ew5\nnB4LxML17w02j4MdP1Rgb2mDW3eBK0wG8Mu52Zg6Mpn2NUk54k/3VsHq4OPjxVwnBa+x2TLcNZu+\nlvXLH5VApXa/ZzksBn5712jkZsTTWrHre2z4rz8f8PjZgklpuHVq5oAq5HkiqKYqgcBdZaWzsxNS\nqbT/33K5HG1tUtHt5gAAIABJREFUbW7fIxAGmwylBOt/NRWvbC1xChkbzTZ88PUZaLpMtLRwM5QS\nrH240GPfpa23D0XnVRieHudXRCBezMOSGVmYOTYVu4tqsbfEXaGrtw/YU1yHMdly2iFBPpeNscMT\nseHX03G2ph3natQ4dKbF43e7DFZs+/kidhy8hDumZyJdKR5w3WxC+Klu0ODrw7UYkR6Hrw9d9lgY\n6AiPw8TNE9Mwf1JGwC+HL206jo4u9ypmjd5yXVrWnpwxAPzhvvG0h0tU1nXir/92L04bmZmApfOG\nh2X0IkDDIW/fvh3bt2932rZy5UrMmjXL5+/oLLwTEoRgs1l+vzfQeHtbiRViwX6FQoL3V83Hf/33\nz9Borz0k9l1VFiqtaMNbK2ciL9N3W4NCIcG7z8zB0+8ccFsp7ztRj20/X4Qino8/Pz0HCVK+333l\nZCViyWwN/vjuQbcClz3F9dhTXI9kuRAPLy5Ap86EGWPS/O4XANLTErBoFrCiXY9/fF+OQ6eaPX7P\nZuvrX6lLhRzcu2AEWju7sWh6FlJC1HMZ6UT79d9ttqKsQoWDJxtw9Kz9Ba38cqfP38jjufj9fZOQ\nk54AAY/+S1pzux7fH62FRMj16IwBIClBgHmTM2nvt9tsxaq3f0KrB0ccL+ZgZHYils0fgaxU386Y\n+n/42xdn0OVBY1spF2Dtr6YHZG+oua4q61WrVvWHrHt6enDLLbdg//79AIAvv/wSVVVVeO6557z+\nPhJCRbEcsgJiz36TxYraZi3OXOpwy2MFMlxCozfj+PkW7C6+Aq3BingxBxr9tdCvWMDC4qlZmDJS\nSVv8/mxNOy7UagD0Yf9pzw4U8F+s4gmV2ogfSxuwz0s/syemFyiRmGBvoYrW1XO0Xv/UKrC5w4gf\nTzR4VXxzZNxwGfoAFOYlOXUi+IOqxpYKOfip7Fq0x1GKFgCylGLMGp+GqQVK2gNc9p9qhIjPwhcH\nLzt9NqUgCbPHptKOHpksVqzdXIw2D8MlmAzgUZfui4EmpCFrT3A4HAwbNgylpaWYNGkS9u7dixUr\nVoRq9wRCSOBz2cgfKkOKXIQfSuqdHhhmiw37SuppSf3Fi3m4dcpQzB2fhsZ2A/RGi1N/pr7bhm0/\nX8T2A5fw69vz/WpP87lsFOYlozAv+arGtsZraO6bI1fw3bE6PHX3KNqOUikT4v4FI7Bo+lCcqGyF\ngMeCWmvGV4drPfaAAsDR8mvKZ2I+E1MLUvy2URHCA/WCyAAwLC0O731xBjoPBU+uiHgMFOalYOGU\njIDPq8lixeEzTfhs30WPn/f22V8eVZ3duG3K0IAkaL87WovD565df65a1nfOHEb7eFVqI/598JKb\nM44TcbBkeiatgszBIqgV8v79+7Fp0ybU1NRAJpNBoVBg8+bNuHjxItasWYPe3l6MHTsWq1ev9rmf\nSHgzjdY3ZLpEk/3nazvwxcEaTBuppNU+pNGbcfS8Ct8eugSztc/tjZ5u1Sdw9Q18SzHaOt3fwAFA\nzGdhxa15GD1MTnt/tc1aWHps2Lq70uMYOwCQCtm4a3Y2rLY+TMxVBPxgYfM4+PKnKnx9uBZeWprd\nuGl8KjR6CybkKjBhhOKGXjnfqNc/VRldVa9BgoSPf++/5DcfTMFhM3HnzEyMy0tGgpAd8PmjVt9b\nd1c4RYVcUcTz8XIA+s8avRm7i65gb4nnKE6gWtYqtRE7DlzEiUp31S4GgPdX+x8uMVAQYRAv3Kg3\nZKiIFvtdm/rjRWys//V0vw8Df8MlEuN5eOXRKbSdqL8HlVRo7weeSjOUDVCD3os9tmY4wgDwX78c\nHVB4mTr/Gr0ZJypbwWYxcKlRh8NnvYfMHREL2PjDvePRqTOhvcsUUasNOtwo1z/1gqbvtuBKiw77\nShtgsdJ7dDMZwPKFuWhs02NEenz/S2EwSl256fH4nx2nvUZvGABmj0tBQaaM9ssn4F2UgyIQ526y\nWHG8XIWPd7trWQOAgMfCGhrDJQC7Uz98tpl28RldiEP2wo1yQw4U0WK/p2lPT945EoV5Sp+/c+zF\n9DbAfNHUdHR0mXHbVHphNyof/Mneaui8yGYCga/AK+s6Ua8yAIxe/FDa4DUkKZPycPfsLHSbe/2u\nmr2df5XaiF3HruAQTcdMwWQAU/IUsAFAbx+EAk5ACk6DTSRe/1S/+tlLHZicn4QzlzpQWtnqUc7R\nF0wmcOfVan5P14A/21VqI/adqAePw8TuInt6xzV07IiAA6x5hL68JCXIcfRMM8rrujx+h89lYPmC\nPNqRmDqVDm9+ftJt4hpF/FVd7Hgxj5ZSmWNHBZHOHAQi8YYcTKLFfk9v2FIRF+seKUSH1uS1FcnR\nfnvfpApbvbxZA3bnTLcFhAor/u2r816/I+AyMCU/8ByeSm3ECxuLvEpnOjJ3XAryMxM85rH9nX/7\nyrkNbBZwpUWH/V6qtf2xdO4w2Hr70NcHyOP46Dbbggqxh5pwX//Uyre1sxtnajqgiBPg2Llm6Lr9\n5389cfuMoZgwIgmXW3QYky0P+GWMUurq7e3zOgZRKuL2q9/Fizm4acIQpCcF1jZXp9Jhw6elMFu8\nX7+BKNf507IG7NfgvAlD/MrmUvftpp0XYHFooF44OR3Lbsrxeyx0IA7ZC+G+IcNNNNnvaf6pXMpD\nh9ZMe7gEYHd2Gz454XUoRCCymYD/mbEUwUhnFp1vwe7iOnQZenyuXgCAywKeuW8CmExG0EpdGr0Z\nP5c1oLZZi/O1nbTzlp5gMoHfLR2Lg6eakCwXYmIuPUcSSgbj+qecrqqzG6eq28BlMSDgcREv5uD7\n4npYPcg+BsJdMzMhFHADfsFxVupqx4nKNpyq7oDF2ut1eAmLxcBrj02BWmePJAUzj9uXdCYAcJjA\nCw/Rl6WtrOvE1j2V0Og811mwmcCz97sPvfA0WOZsTTu27q6E0UM0gk7EjS7EIXshmhxSMESb/Sq1\nEX/6tAxdBgvkcXx0OOhQMwA8toTecAnqIbr5uwteW0Z+MX0o5k2g135ErTQtPVZs31/j9Xs8FrBw\namBtTdSqRszn4K1/nvQqsu9InIiDhZMzsGT28KCVuhzzii1qA2qbtfjmiPswjkBhMRmYNEIOg8mK\nYWlx/f8X1N8LpcMOxfWvUhuxr7QeIgELSQlCqLssAKMXSpkQ7RoTvjhQg+v0uQDs1++Dt45Ap86M\nNrURPC4bCz3ok9PBZLGiRWtG6bkm/HiiHv4ugWCUuhz/VlF5C4rLWzG5IAkt6m6P0pkcFnD/LfTT\nOPbBLUVeVekWTc1AzpA4ryt31+jYixuPQa31/B8hj+Ph1cfo1ZLQgThkL0SbQwqUaLSfclByKR/r\ntpS4DZdwfGP2Zz9VIPLPHyrhTV3w7lne83SeUKmN2FN0BQdON3tdYTIZwMp7AivQoo63sk5ztbDM\n/6xbJgO4c1YWlDIBxAJuUCseR1RqI3YXX0G32YqeHhtOXnTX/w4UJgNY83AhXv24FDZbHxgM+8tQ\nVooUCRI+jp1vwbBUKTKSJDhV3Qad0YJOnQlsNhPJMhEuNnUhRSbE0GQJxAIuZBI+ztWqwWIx0Kw2\nQq83IVkuweT8JFxq0uB0dQfG5shRkClHh9YErd6Mb49cRlqiAAIeF4UFSlRe6USL2oBuszUkNnqC\nx2Hi4dtycbFBizgRN6BrzBvVDRp8eagGDa066Lvp5aQDGTnqCHXvfLanEo71Z3EiJroM10I5ozLj\nMXVUMu0+YGpV/OHOco8rWSGPhT/eP4GWljVV0Pj1oVocOO2umAfYV8aBFKjRgThkL0SjQwqEaLdf\npTbi+Q+Oe3R8q5dPwPTx6bTs95ejCkZ72t7mcRl7Sxq9fkcu5eGFBycFtSrxN7XHE1KhvR96/qQh\nISlgoaq3qb5nxxxyskxA+9imj0rG0XOeZUCjASGXgeHpCRiiEGNosjgkL0cUVE60+HwLTlR30PpN\ngoSDBxfmBzXNiU506ck7C1CnMgRUvUwJhZRWqjyqgPG5LPz6dnrzkwFALBVg79Ear7PPb5jhEqEi\nEhxBtDskf8SC/d6GQnBYwH0L8mAyWWm3IdWpdHjloxKPPbt3z86EgMdFYhyftsA9tc8/fXoCJovn\nBDBVNX2xXosEKR+zxqYE7PgPn2lEq9qEo+UtPvPMjkwfmQSxkAOzpRcFWYG1sdClTqXDd0cv9+eQ\nz1xqxxcHnYdu9K+Qt5bCRrdZOgK5dXI6mjoM/TnkxDgeLtRrMCU/iXaYlg6OL0E6Qw/20VTqGjtc\nhr5eBiYXKAJWraLSCXIpD3//5jwMPorSZFIOXnt8WkD7p1OHsY6m86ReGD7aU4k2L+1bgRRvBgNx\nyF6IBYfki1ixX6U24rWPS2Hw0g4B0G9rsE9rasb3xVe8zjeOE3Fw6+QMTKGZc6NWMeW1nYgTc3Do\nTDM6vRSpAPbVvWNxFl0cH9YdXWZ8c/QybDQTnAIeC7+9cxRa1MarLx0DI6epUhvx3bFaaHRmpxyy\nSm3E+n+UBl2BPNAsKExDhlLilEPu0ltw6mIH7piZRWvwQaA4tsNZe21AHwPfHb9C68VFKmJj5ugU\nmHt6MX9icApsGr0ZO4/W4Kcy/9X3mclizBmXGlCrHyWO8+G35TB6uNfixFxMylXQPn7HGhNP3Dwh\nDfMHQY2OOGQvxIpD8kYs2d+fD95X5VFUQSLk4I3f0H9z1+jNeGVrCTQ6CyQCtkdHwWQCv769wK90\npqd9v/6PE05FaZ6QCjl4YEFOwPunYPM4+O7gxf5K7UAQC5hYPDUL43IU0Jt6An45CAbKAbV0GJEs\nF4Y9h4w+QCkXYVYI8rt07T9b04FTVe3o1JtQ09jltbbBGxw2E0vnDsOdN+XSHj3qegy1zVp06kzY\ntLPCax2EkM/CL+cOR+WVTto9/MC1/ueSC61eOx0eX5KHBAk/oHC6tzGsFHRX2KGAOGQvxJJD8kQs\n2k891N/dcdbt5rx7dhYMJv9a1o77oiqcX9h43Gs4WCpkY/XySQG9eVMPvi27LqC9y3fIUcBl4rkH\nJgb8QHFsfaEeshWXNTgcRL5WKuTgiV8UoLSyFb29wKhhnnufI4lIv/77K7j5bCTJhPjXT9XQ0tCo\ndoTFZGD5ghFoGASlLkcCdXD+pDMBQMhn44/3jQ9ovyq1EbuvFlE6wmQw0NvX57UlciAhDtkLkX5D\nDjSxbL9rPthVyzrQPNK1CVB1HmUuGQDumZsNHocVUM8otSo6Xd2Go+dbfX738SV5EPE5AAAuh+V3\nBeFLqWtfaT1YLECjNaPYgx4wHThM4JbJ6UhNFEGtNQF9TKQrRREzPSpSrn9qYlJSPB+piSI0tRtQ\n12LAucuBV3AzANw1OxMyKSXA4lnONBClru+L6tHXZ3fuvsLhKxbkQK2zBCw16U86UyJg4zd3jgp4\nRfzPfZWoqNe6fcYA8Jdn5kDVph+UyI4rxCF7IVJuyHAR6/azeRzsOVqDHmsv/vXzJbfPWUwG3vrt\n9ICHs5+t6cAneyt9TtwJRDqTglqppMiFeOOzk36VumQSLsbnKDAiIy4opS4KStN3iELoJr4SDBIh\nG88vnwRzjw07j9UiVS7CtJEpgxb6phjM679OpcM3h2vBYPZByLUXdZVf7oRMysWx8rbr2veCwjTw\nufZBEXR7hYNW6hJynELJQh4LQ5JEuGfO8IDy5FTnwrGzzTh/xbN0JmAX4ln/RGCSnL6mUFHV0xNH\npYbt2Uccshdi3SER+6+FbL3NS505SglTTy/G5cgDqj6lHPPfvzrvNW8l4DIxcYQCo4cnBlzFrNGb\nsf9kA20xDh4H+P2y61PqAq61nyTLhLDaepGhlODNz04GXAHtTQ1KHsfDiCFx0HX3ICtZ2i++EqnC\nIIDn9q5kuQBd+h6UVbchSynG98X0Z1H7g8tmYFiqFIlSARZPzwwq3Oqq1FVW0Yqyi+3o8RERd1Tq\nsvTYaEVhPFGn0uGNT0+g20tnAQDcNTMLQgE7oIEl/vYrFrDxwgp76iiczz7ikL1AHBKx31Gtx9eU\nGAAQ8hh46LaCgJxndYPG62rDERGfjQdvHRFw3pVykCcqW/3mmimoKvDF16HU5Yjjyn1/Wf11r/hc\nWfeIP2EQCTKSpPairm4zNDozmEwWUuRCh6IuKcQCztWirg6wWQw0qbthMJihlIn7i7pOXezAuOHO\nRV3fHLmMIQr7S8zkAiUqrnRCpTYAANhsFg6eDk7jmw6T8xVIThAiLUkMLpsZtBN0xGSxQtVlRvG5\nJvxc1gBTj283YFfqGh7QlDLXv1fbbA8di/gcny1MfC4Lqx7wL+zhuu/WTiO27q7y+r2lc7Mxb0Ka\nX5W+wYA4ZC8Qh0Tsd7WfygWru0zYV+ZZtEMi4OD5FRMDFjUQcln48vBln98V8ll4KUhlpP7q1+8q\n3PquPcFiMXDH9EzI4ng+842B0t/73NkNmYSPJJkAaq0JJosNu4vr0dvrnrP3RTiEQQI5vlDAZTEw\nd0IqMpQSNLUb0KrpxpJpWSGp/KWujcY2PU5UtKGhXQeDiV5DerBKXQCVG69FeW07jFffFYU8lscW\nJhYDeGBhLqYWKGm3RRWVt+CfP16Euce7LQVD4/EfN+W4/T8Sh+xCJDgC4pCI/d7sN1mseGlTETp8\nrDrvnz8cDAYzoCItSl7y4Cnf0pm/vqMgaMUmasUqEbDx1y/O0f4dkwHMGpuCSblJOFerxrBUacgF\nQRxDz1qDBTuP1SIpXoDiijaP4y+BqyvkG1wYBLCHmguy4iHh8/tzyENTJMgfmhDyQjfq/zkzWYL/\n2X7K64xuT9iVuvKuazVONzI0OiseU0YGKp2pwZZd56E1eu/5un2Gb0144pBdiARHQBwSsd+flnVZ\nVStKLrTi9CXvFa/BSmfuK63DruPuQvuOJCUIcPuMoahvNdBux3L9O4dPN6FV042j51oCXvXxOAzc\nPiMLWmNPUH+fLlRBEYfFxL7SemgMZqcc8mALgwSzQl46Nxu23l63HPKEnESkKcQhk8N0hWrla+8y\nIWdIPM5casOXhy7TipJQjMuRo68XKMwPXKmLgooEpSeJ8MneKo/qczwOE6uXT0RJRWtA1djUKn/r\n7kq0dnpvuxLyWfjjffS1rMMBccheIA6J2E/XfpXaiNf/UeJVlP/uWVkQ8DkBS2dq9GbsL2vAN0fp\nFWfdNiUdtxQGJ+vnWnz01eFa2lKaFDeNT4VGb0be0ISAowPXyzVhkG4kywURkUMWCzmw9SJotatg\nqFPp8PXhWgi4DGSmxOHrw7UweBi04Is4ERvTR6Wgp6cXNwepTkW9VFbVdyE1UYCDp1V+f7N6ufso\nRDp/59WtpejU+Y5WBfLSQxyyC5HgCIhDIvYHYj+1YvakUMRgoH9FEifmYmFhRkBFMPbctQpWmw0H\nzzShXeO7QOvxJXnQGnqQIhcF9ALgCNX2xWYxsONAjU8NYm8wGcCji/PQbbYhQylBWVXbgIS6B4JI\nv/4dX0BkcVwADJyracehM/4dnycYALLTpJgxOhmLZ+dcp1KXGRt3+m+B43OZmD9pCNo6TQEpdlEj\nS6UiDj7bV4UuDyF3qZCF3HQZFk/PDFoUJxwQh+yFSL8hBxpif3D2XxMBuQKtwQqJkO2x55jBAH5z\nR+DSmdd0fHvx6Q+Vfqun6Y6cc8W1yvxsTQdqmrQYMSQOG3eW+2xL8QeHBUzISYRSJkKSTBDSorFQ\nEUnXv2P0orHNCJXagEtNnU6jCgNFImDjmXvHo1NnQnuXyen/f6CVuuRxPDy6KD/gML1KbcSe4ivY\nf8p75frNE9IwIVdxXSkA4pBdiIQbIZJuyHBA7L8+++lKZ4r4LBTmJgU1UJ6qJv14T5XfnKBdqYsL\ni9UKraHHrwP0V9R2tqYDVXUaKGUCNLYZvc6MpQuDAdx383AwGAxMzE2C1mC5Ou1JhHkT0gbdWQ/m\n9U/9f5691A4GgwG5VIDyK/aiKxGPg68OX/Za5EcXsYCFlfeMRZ1K53cACB2lrh9LG8DhMLC3pAG2\n3j6/Sl0AMHdcCvIzA5dNVamN+PLgJRRX+G6ZC6ZewxPEIbsQCY6AOCRif6jsp1bNe/wMalgwKQ0J\nEgGmBNjTqdGbUXReBYu1FwfPNPqs/qZgMYHJuUng8VhYONn9ZSBQ+zV6M34ua0Rjmw55QxNgtfXh\nx7J6j/Npg2HZvGwMS4vD4TNN6NRZMCxV0l8pe8MIg3RZAPRBKReiS29GWVU7Rg+TYXdxPXRehiUE\ny7gcGcZmJ8Jk7kWyXBBQpba3lj9qjKI3KUvXQSoCHhPJMjHyh8YFNbLQZLHi57IGbN9f4/U78WIu\n7ps/nNZLJl2IQ3YhEhwBcUjE/lDb3y+duafSZ0UwkwEsnpaJYanSgHPAVG6xukHjt0rbkUcX5ULV\naeqvbg2F/Y4VvjIJD+99ce66V3qOsJjASw85CIMA+MUM78IgWqMFXXozmAwGUhJFuNSohVImQGaK\n47Qne1FXY2c3DHozUmRiFPqY9vTNkctIV4jAu1rUVVmnRkuHEX29AJvDCDqnSxcGA3hw4QgI+Rw0\ntOrRbbIGXYhFoVBIUF3bjhOVrWCzGDhXo8aJKt+a5c5KXb3gcphBhY2v9eWzsfNoLXy0ESNezMWa\nhwtDHj0hDtmFSHAExCER+wfKfsoxv//1eb/tM1wOAw8uzA2q3YR6uJVWttJaNVPcPiMD86dmoaxc\nFdJVJyUK0tJuBBhAYpwAYiEHn/94MaA2HEfCIQwSLu6cmQVzjw3tmm7MHpeKDq05ZOfHsTbBBuB/\nd5yh/fK0bF427fnevlCpjVj9wXG/31s6dxgyU6QD1ipGHLILkeAIiEMi9g+0/ZTm9N6SOpgsvm83\nHoeBBYUZ/b23gUA9bPXdPfh8X1VAQhCUHGWChIcWdfeA9Bs7hnWpAi8qhxwv4eKnsiaP+UlqhfzK\n1pKAW7QilYm5iUhXSPpzyIlxAlxq0uK2KfSrkOlCaVVfqO1EaVUb9AFW0ot4TLz40OSg26JOVLbC\n0mPD2Ro1phQkoUXdjT3F3qM6k0bIcc/cnAFvISMO2YVIcATEIRH7B7uo5+Pvy2Ew+7/tnrxzJMQC\nTlArBEeRjV1Ha4ManzgtXwGJmAdzjw0FmbIBb2Ny1MP2lENWqY1Y/0mpzwla4Wbp3GGw2QDXHPKE\nEYkQ8tlBi7sEguOLj85gxQ8nrkCtDSx3vfLuUeDz7Oc62BWqSm3ECx8ed4sOxYlY6DK490yPzorH\nPXPdJS4HCuKQXYgER0AcErF/sO2/JobfjY/3VPoN4yoS+LhjRuZ1Pcyp8YnDUiTY9N0Fj1rC/hDw\nWPjtnaNQ3aBBU4cRS6YF3vt5vVzryzUiWS6MiByyWMRFXy9w86QhgzrknurTFfBY6OgywdZrA8DA\nzqNXAlYYE/OZKMiUw2rtw+2zgtfPdlTq+ueP1V5FdJ68swA1TTpIhRzUteoHJDLgD+KQXYgER0Ac\nErE/nPbbw9mN+P74ZfTQ9JGLpqYHVc1K4ViENS5PiVc2Hg9ajnLR1HTMGpOGFrXBrc/1RiDc558u\n19IRFjS2GVDXqsep6o7r2ucjS/JgNFgDrs52pF+pq64LqQohDp72n+eXSbl47fGpYReNIQ7ZhUi4\nEW6UG3KgIPZHhv39eb7LnZAIuThervI4m5mCxWTgkUW50Bl6kCwXBv1AVSgkqG/s7F918nks1DR1\nXZcS1LSRSbBa7eOcWAxAkSBCVook5MMTQkGknH9HqJqDpg4jphUoceZSB04EkfulYDEZWL4gB42t\nBiRIef0r0omjUoOy3XGq2MadFbR+wwCQnSrFjDHJmFKQHBHXAXHILkTCjRCJN+RgQuyPTPuvVcPa\n8MkPlX77fIU8Fp5eOhZWm73q6Xr1fFVqI/adqAeLyYBGawoqB+2KiM/EzRPTkZQgQLe5FxNzFdAa\nLPi+6ApGD5MFPdDgegjH+afCugIuG+WX1chMESMxToALdZ1IEPPwU9n1ia+wmAzcMcP/SM2BVupK\njONh6bzskPYOhxLikF2IhAdhpD6QBwtif+TbTyl1+Rq+7kqciINJuUmY76dXla79VA56TLYcpy+2\n44eSOtohdrrEi1lYPG0YMpQSHD7TBLXWjOxUKeYGUXFOl1Cdf8eQsqqjG2AASpkAXXoLyqrbMD4n\nCVw2A0XnVaio7wrBkV9DLGBj3vg0MBh9EHA5tFuT/NlOzTKWSfjYV9qA3j6AyYTfSvc5Y1NRkJUQ\n8VrmxCG7EAkPwhvhgTyQEPtvHPsdh08cON1IWx1rcr4C6O2DUMBxU+sK1n7HPPSoLDla1AbUNmlp\nT6wKBAaAPz81AwD6q4epoRoJEt7Voi4pMpIkDkVdFoeiLg2UMqFLUZf66rQnIwx6M5LlIhTmK/1M\nexJCwOWg8GpRV3ObAQwmA3weG0fPtXgcNRjq/4cHbx2BTp0Zqg4DTD29KMxPCjqy4KpjXlbVihMV\nbUhPEmNossTrHG2piAOtgxIdn8tAqlyEvKEJ11XbMNgQh+xCJDwIb6QH8kBA7L8x7b+m1NWJXccb\nAvrtzFFKZA+Jh9XWh4XTs2A1h07OkRoMYOzu6c8hS0Q87C0J7Bhd+Y95w/HvA5f86iiHEjq6zQOF\ngMtAXoYcM8cko/xyJ+JEXMwcmxoyZ2eyWNHSZcapihaotSacrG6nNb7RWanLBi6HNWDCHQMNccgu\nRMKD8EZ9IIcKYv+Nbz8VTi7MS0Knzowt35dDa6BXAMRk2JWhZHH8q/nGgZltTOl8W229kEl5/Tnk\nNk033vnXKZh9rC4ZsDvkbT9fDPlxhZP5k1JR12JwyiEny0QYMSRuQArgqNTHsXMqNLXroDfRX9Ez\nmcAv52RjagiUuiIF4pBdiIQHYTQ8kK8HYn/02U9VbJ+92IHD5wKvlp4zNgWT8pJwurodEhEXs0K4\nMvOE48SsynpNvzCIYw4ZAJ79v6MRv0KWCjmYOy4VbDbTYw75VFU7JEIuFk/PHPCeZcdCrEtNXfh4\n9wVYAixq77B/AAALVklEQVTU5nNZuGlC6g0ViqYLccguRMKDMBofyIFA7I9u+1VqI/YUXYHRZA8h\nF1/wPdrOG3fNzITRYsOEEQo0dxhDqn1NF0cFqkjKIYuFHJh7egdFzcwTVPqiXqUHh81EfqYMpy+2\n4+vDtQELhADAgsI0zBufDr2pB2mJohsyHE2HqHLIxcXFePrpp7F+/XrMmzcPALBixQoYjUYIhfY3\nv+eeew6jRo3yuo9IeBBG+wPZH8T+2LKfarlJlgmhNVjw9ZHaoPWh75yVCRGfi1FZMqh19p7pGy2f\neKOdf43ejEOnm9HYroPOYIFUyEX5lQ7ougM/iVIhCzPHpMFg6sHQZClstr4BS1lEIpHokIO6c+rq\n6rBlyxZMmDDB7bMNGzZgxIgRweyWQCAMMEqZEMtuyun/9103jcCeo7VgsRj49/6LtAp7KL46dNlt\nm0zKxchMGcxmKxgMgMViIlkmCmlBUrSj0Zuxv6wRTR0GTBupRPmVToj4bCRIeAG1vnkjK0WC2WNT\nsHh2DvRa//3EhMEjKIesUCjw3nvv4YUXXgj18RAIhEEkQcrHzRPtOdqpBUqcrelATZMWo7JkOHau\nGUfPtwa0P7XWgkNn3OUTvzxUi9umpoPPYTsVkGkNFuwtqceCwvRB1zIOF1TPcmObHqUVbchMkSAx\nju9RGKS0MrgUA4VEyMEzy8ZB1WlETZPWSQtdwGNDf117J4SaoByyQCDw+tm7776Lzs5OZGdn4/nn\nnwefzw/64AgEwuDB57JRmKdEYZ4SADAyS45fzLCHuOVSHjq0ZqQlCrF5V2XA++4DsOu488i9z/dV\n9ec4j55rwa9vz4e5pw8pciEOnWqERm/BsLQ4zB2fFvGraydhELUR6GNCKeejS2/Byap2jB+RCDab\nhbLKNlTXq2F2KK6qagiNUIiIz8LCyekYk61AdUMXEuP4yM2IB5/LRoZS0n9eCZGL3xzy9u3bsX37\ndqdtK1euxKxZs7Bq1SosXLiwP4f8ww8/IDc3FxkZGVi7di0yMjLw2GOPed231WoDm80KgRkEAmGw\nqG3SYMdP1cgfKgMYwMhhcryysQjtXd61t68HFpOBLS8tAAAcOdMIEZ8DjcGCIQoxEuP5+Km0HvlD\nZchKi8Px8y3Q6szo0HaDzWJiiFKEitpODEmSIHtIPCQiLhTxApyoUIHLZqHiSgdsNiAtSYRZ44ag\n4ooaRedbMKUgGeNGJKG104guvRmf763A0GQphAI2Zo8bgrOX2lGn0gEMQMzn4KeS+qAmaAWDRMjG\nmOGJ0JuskEl5GD8iCXKpADkZCRDwbpz8PcGd66qydnXIjhw4cAC7du3CG2+84fX3kVBMcaMVdYQa\nYj+xP1TSkdT85f0nG2Ds7gGDAfQxgHM1ahjN7kVHTAZoVwEvmzcc2w9cDLoALRgCOb5Qw+cC+Rly\nzByT0p9DTlOIIBZwQ1Y4R679KCnq8kRfXx8eeeQRvPvuu5BKpSgqKkJOTo7/HxIIhBsePpeN7NQ4\nAMCDt+Y7febYlgPAKYfc2KbHO/867XMmNIvJQB/8ayiHmsFwxsNSJRieFtefQ/YkDDJ+RNLAHwgh\nIghqhbx//35s2rQJNTU1kMlkUCgU2Lx5M3bt2oWNGzdCIBBAqVTi9ddf95lvjoS3M/KWSOwn9ofX\nfkrAIjNZgsstOo85ZAD4w/8difgVslTIwdzxqWAzWR5zyKcutoHPYSEtUYJZY1PCmhuPhHMfTiJx\nhUyEQchFSewn9of7MGhhFwZpg4DH7J8DnSDhuwmD6Lot6Owyg81hIlkmxMWmLiTLRMhMFjsJg7BY\nDDSrDdAbepAsE2FyfhIuNWlwuroDY3OchUG+PXIZaYkC8HlcTHYQBgEAsZgPS481bMIgwXIjnfuB\ngDhkFyLhYiAXJbGf2E/sj0Vi2XYgMh0yc5CPg0AgEAgEggeIQyYQCAQCIQIgDplAIBAIhAiAOGQC\ngUAgECIA4pAJBAKBQIgAiEMmEAgEAiECIA6ZQCAQCIQIgDhkAoFAIBAiAOKQCQQCgUCIAIhDJhAI\nBAIhAgirdCaBQCAQCAQ7ZIVMIBAIBEIEQBwygUAgEAgRAHHIBAKBQCBEAMQhEwgEAoEQARCHTCAQ\nCARCBEAcMoFAIBAIEUDMOuTi4mJMmzYNP//8c/+2iooK3Hvvvbj33nuxdu3aMB7d4PHFF19gzpw5\nWLFiBVasWIG//e1v4T6kQWH9+vVYtmwZ7r33Xpw5cybchzOoFBUVYerUqf3n/NVXXw33IQ0aVVVV\nmD9/Pj755BMAQHNzM1asWIH7778fTz/9NCwWS5iPcOBwtX3VqlX4xS9+0X8d7N+/P7wHOMC8+eab\nWLZsGe655x7s3bs3Is89O9wHEA7q6uqwZcsWTJgwwWn766+/jueffx5jxozBM888gwMHDmDOnDlh\nOsrBY9GiRXjuuefCfRiDRnFxMa5cuYJt27bh0qVLeP7557Ft27ZwH9agMnnyZLz77rvhPoxBxWg0\n4tVXX8W0adP6t7377ru4//77cdttt+Gdd97Bjh07cP/994fxKAcGT7YDwO9//3vMmzcvTEc1eBw/\nfhzV1dXYtm0bOjs7cdddd2HatGkRd+5jcoWsUCjw3nvvQSKR9G+zWCxobGzEmDFjAADz5s3DsWPH\nwnWIhAHk2LFjmD9/PgAgOzsbXV1d0Ov1YT4qwkDD5XLx4YcfIikpqX9bUVERbr75ZgDRfc97sj2W\nKCwsxF/+8hcAgFQqRXd3d0Se+5h0yAKBACwWy2lbZ2cnpFJp/7/lcjna2toG+9DCQnFxMR577DE8\n9NBDKC8vD/fhDDjt7e1ISEjo/7dMJouZc01x8eJF/OY3v8F9992HI0eOhPtwBgU2mw0+n++0rbu7\nG1wuF0B03/OebAeATz75BA8++CB+97vfQa1Wh+HIBgcWiwWhUAgA2LFjB2bPnh2R5z7qQ9bbt2/H\n9u3bnbatXLkSs2bN8vm7aFQU9fR/sXjxYqxcuRJz587FyZMn8dxzz+Hbb78N0xGGh2g8177IzMzE\nU089hdtuuw319fV48MEHsXfv3v6HU6wSa9fBHXfcgfj4eOTn5+ODDz7Ae++9hzVr1oT7sAaUffv2\nYceOHdi8eTMWLFjQvz1Szn3UO+SlS5di6dKlfr8nk8mg0Wj6/61SqaIuvOPv/2L8+PFQq9Ww2Wxu\nEYRoIikpCe3t7f3/bm1thUKhCOMRDS5KpRKLFi0CAGRkZCAxMREqlQrp6elhPrLBRygUwmQygc/n\nR+U97wvHfPJNN92EdevWhe9gBoFDhw7h73//OzZu3AiJRBKR5z4mQ9ae4HA4GDZsGEpLSwEAe/fu\n9buKjgY+/PBD7Ny5E4C9ClMmk0W1MwaAGTNmYM+ePQCA8+fPIykpCWKxOMxHNXh888032LRpEwCg\nra0NHR0dUCqVYT6q8DB9+vT+ayFW7nmKlStXor6+HoA9l56TkxPmIxo4dDod3nzzTbz//vuIj48H\nEJnnPianPe3fvx+bNm1CTU0NZDIZFAoFNm/ejIsXL2LNmjXo7e3F2LFjsXr16nAf6oDT0tKCZ599\nFn19fbBarf1V5tHO22+/jdLSUjAYDKxduxZ5eXnhPqRBQ6/X4w9/+AO0Wi16enrw1FNPxUQ3wblz\n5/DGG2+gsbERbDYbSqUSb7/9NlatWgWz2YzU1FRs2LABHA4n3IcacjzZvnz5cnzwwQcQCAQQCoXY\nsGED5HJ5uA91QNi2bRv++te/Iisrq3/bn/70J7z44osRde5j0iETCAQCgRBpkJA1gUAgEAgRAHHI\nBAKBQCBEAMQhEwgEAoEQARCHTCAQCARCBEAcMoFAIBAIEQBxyAQCgUAgRADEIRMIBAKBEAEQh0wg\nEAgEQgTw/wEbaPv+psURAQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jejegzbaxmRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12084
        },
        "outputId": "ecabb998-9e5b-40e7-f93d-42b287fbedc7"
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = np.reshape(x_train, [x_train.shape[0], -1])\n",
        "x_test = np.reshape(x_test, [x_test.shape[0], -1])\n",
        "y_train = np.reshape(y_train, [y_train.shape[0], -1])\n",
        "y_test = np.reshape(y_test, [y_test.shape[0], -1])\n",
        "\n",
        "#construct dataset\n",
        "features = tf.placeholder(tf.float32, shape=[None, IMG_ROWS * IMG_COLS])\n",
        "labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "batch_size = tf.placeholder(tf.int64)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size).repeat()\n",
        "iter = dataset.make_initializable_iterator()\n",
        "x, y_ = iter.get_next()\n",
        "y = tf.one_hot(tf.reshape(y_,[-1]), NUM_LABEL)\n",
        "\n",
        "# print(x)\n",
        "# print(y_)\n",
        "# print(y)\n",
        "# print(y_train)\n",
        "# print(y_train.shape)\n",
        "# print(x_train.shape)\n",
        "# xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "def layer(input, num_units):\n",
        "  W = tf.Variable(tf.zeros([input.shape[1], num_units], tf.float32), name=\"w\")\n",
        "  B = tf.Variable(tf.zeros([num_units], tf.float32), name=\"b\")\n",
        "  output = tf.matmul(input,W)+B\n",
        "  return W, B, output\n",
        "\n",
        "def lrelu(x, alpha):\n",
        "  return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n",
        "\n",
        "def inverter(y, model_weights):\n",
        "  # Input layer\n",
        "  ww = tf.matmul(model_weights, inv_weights['w_model'])\n",
        "  wy = tf.matmul(y, inv_weights['w_label'])\n",
        "  wt = tf.add(wy, ww)\n",
        "  hidden_layer =  tf.add(wt, inv_weights['b_in'])\n",
        "  rect = lrelu(hidden_layer, 0.3)\n",
        "  # Output Layer\n",
        "  out_layer = tf.add(tf.matmul(rect, inv_weights['w_out']), inv_weights['b_out'])\n",
        "  rect = lrelu(out_layer, 0.3)\n",
        "  return tf.tanh(rect)\n",
        "\n",
        "#Build Logistic Layer\n",
        "with tf.name_scope(\"logistic_layer\"):\n",
        "#   w,b,z = layer(x,NUM_LABEL)\n",
        "  w = tf.Variable(tf.zeros([x.shape[1], NUM_LABEL], tf.float32), name=\"w\")\n",
        "  b = tf.Variable(tf.zeros([NUM_LABEL], tf.float32), name=\"b\")\n",
        "  y_ml = tf.nn.softmax(tf.matmul(x,w)+b)\n",
        "\n",
        "#Build Inverter Regularizer\n",
        "model_weights = tf.concat([tf.reshape(w,[1, -1]),tf.reshape(b,[1, -1])], 1)\n",
        "# print(model_weights)\n",
        "inv_weights = {\n",
        "  'w_model': tf.Variable(tf.zeros([tf.reshape(model_weights, [-1]).shape[0], INV_HIDDEN])),\n",
        "  'w_label': tf.Variable(tf.zeros([NUM_LABEL, INV_HIDDEN])),\n",
        "  'w_out': tf.Variable(tf.zeros([INV_HIDDEN, IMG_ROWS * IMG_COLS])),\n",
        "  'b_in': tf.Variable(tf.zeros([INV_HIDDEN])),\n",
        "  'b_out': tf.Variable(tf.zeros([IMG_ROWS * IMG_COLS]))\n",
        "}\n",
        "\n",
        "inv_x = inverter(y, model_weights)\n",
        "# print(inv_x)\n",
        "# Calculate loss\n",
        "class_loss = tf.losses.softmax_cross_entropy(y,y_ml)\n",
        "inv_loss = tf.losses.mean_squared_error(labels=x, predictions=inv_x)\n",
        "# calculate prediction accuracy\n",
        "correct = tf.equal(tf.argmax(y_ml, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "def train(loss_beta, learning_rate, Epoch):\n",
        "  total_loss = class_loss - loss_beta * inv_loss\n",
        "  model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss, var_list=[w,b])\n",
        "  inverter_optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(inv_loss, var_list=[inv_weights])\n",
        "  init_vars = tf.global_variables_initializer()\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init_vars)\n",
        "   \n",
        "    # initialise iterator with train data\n",
        "    sess.run(iter.initializer, feed_dict = {features: x_train, labels: y_train, batch_size: 250})\n",
        "    \n",
        "    print('Training...')\n",
        "    for i in range(Epoch):\n",
        "      sess.run(model_optimizer)\n",
        "      sess.run(inverter_optimizer)\n",
        "      train_acc = sess.run(accuracy)\n",
        "      train_total_loss = sess.run(total_loss)\n",
        "      train_inv_loss = sess.run(inv_loss)\n",
        "      train_class_loss = sess.run(class_loss)\n",
        "      print(\"step %g train accuracy is %g, total_loss is %g, inv_loss is %g, class_loss is %g\"%(i, train_acc,train_total_loss, train_inv_loss, train_class_loss))\n",
        "    # initialise iterator with test data\n",
        "#     sess.run(iter.initializer, feed_dict = {features: x_train, labels: y_train, batch_size: y_train.shape[0]})\n",
        "    sess.run(iter.initializer, feed_dict = {features: x_test, labels: y_test, batch_size: y_test.shape[0]})\n",
        "    test_acc = sess.run(accuracy)\n",
        "    print(\"beta is %g, test accuracy is %g\"%(beta, test_acc))\n",
        "      \n",
        "    return test_acc\n",
        "\n",
        "betas = [0.0, 0.001, 0.01, 0.1, 0.5, 1., 2., 5., 7., 10., 15., 20.]\n",
        "test_accs = np.zeros(len(betas))\n",
        "# Setup a list of processes that we want to run\n",
        "# pool = mp.Pool(processes=len(betas))\n",
        "# results = [pool.apply(train, args=(beta, 0.01, 200)) for beta in betas]\n",
        "# print(results)\n",
        "for i,beta in enumerate(betas):\n",
        "  test_accs[i] = train(beta,0.01,200)\n",
        "\n",
        "np.save(\"logreg_acc\", test_accs)\n",
        "plt.plot(betas, test_accs)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "step 0 train accuracy is 0.456, total_loss is 2.03986, inv_loss is 6940.06, class_loss is 2.02891\n",
            "step 1 train accuracy is 0.644, total_loss is 1.84723, inv_loss is 7460.22, class_loss is 1.83367\n",
            "step 2 train accuracy is 0.604, total_loss is 1.8592, inv_loss is 7189.29, class_loss is 1.86305\n",
            "step 3 train accuracy is 0.48, total_loss is 2.03775, inv_loss is 7642.83, class_loss is 1.96985\n",
            "step 4 train accuracy is 0.588, total_loss is 2.00645, inv_loss is 6475.77, class_loss is 2.01436\n",
            "step 5 train accuracy is 0.576, total_loss is 1.94071, inv_loss is 8591.91, class_loss is 1.86913\n",
            "step 6 train accuracy is 0.584, total_loss is 1.8343, inv_loss is 7885.59, class_loss is 1.86179\n",
            "step 7 train accuracy is 0.616, total_loss is 1.85884, inv_loss is 7153.93, class_loss is 1.87765\n",
            "step 8 train accuracy is 0.58, total_loss is 1.91636, inv_loss is 8427.17, class_loss is 1.91599\n",
            "step 9 train accuracy is 0.52, total_loss is 1.85265, inv_loss is 6947.24, class_loss is 1.94477\n",
            "step 10 train accuracy is 0.588, total_loss is 1.9538, inv_loss is 7788.32, class_loss is 1.89338\n",
            "step 11 train accuracy is 0.488, total_loss is 1.93863, inv_loss is 6407.04, class_loss is 1.92438\n",
            "step 12 train accuracy is 0.616, total_loss is 1.82705, inv_loss is 7001.99, class_loss is 1.90812\n",
            "step 13 train accuracy is 0.58, total_loss is 1.84133, inv_loss is 7627.03, class_loss is 1.89722\n",
            "step 14 train accuracy is 0.636, total_loss is 1.78829, inv_loss is 7222.49, class_loss is 1.89091\n",
            "step 15 train accuracy is 0.524, total_loss is 1.87351, inv_loss is 6573.41, class_loss is 1.92325\n",
            "step 16 train accuracy is 0.508, total_loss is 1.83354, inv_loss is 6973, class_loss is 1.79644\n",
            "step 17 train accuracy is 0.632, total_loss is 1.84402, inv_loss is 6764.58, class_loss is 1.79423\n",
            "step 18 train accuracy is 0.64, total_loss is 1.81102, inv_loss is 8043.23, class_loss is 1.78606\n",
            "step 19 train accuracy is 0.64, total_loss is 1.81997, inv_loss is 6669.33, class_loss is 1.89092\n",
            "step 20 train accuracy is 0.62, total_loss is 1.83688, inv_loss is 6761.9, class_loss is 1.87894\n",
            "step 21 train accuracy is 0.624, total_loss is 1.9393, inv_loss is 6791.78, class_loss is 1.83275\n",
            "step 22 train accuracy is 0.66, total_loss is 1.77781, inv_loss is 6902.72, class_loss is 1.82255\n",
            "step 23 train accuracy is 0.604, total_loss is 1.84951, inv_loss is 7052.17, class_loss is 1.82072\n",
            "step 24 train accuracy is 0.652, total_loss is 1.81768, inv_loss is 8219.83, class_loss is 1.83649\n",
            "step 25 train accuracy is 0.664, total_loss is 1.77048, inv_loss is 7421.45, class_loss is 1.79727\n",
            "step 26 train accuracy is 0.564, total_loss is 1.89344, inv_loss is 7377.56, class_loss is 1.80197\n",
            "step 27 train accuracy is 0.6, total_loss is 1.85785, inv_loss is 6614.92, class_loss is 1.82647\n",
            "step 28 train accuracy is 0.62, total_loss is 1.79648, inv_loss is 7132.26, class_loss is 1.7414\n",
            "step 29 train accuracy is 0.7, total_loss is 1.74994, inv_loss is 6925.05, class_loss is 1.85482\n",
            "step 30 train accuracy is 0.656, total_loss is 1.77811, inv_loss is 6272.18, class_loss is 1.80501\n",
            "step 31 train accuracy is 0.656, total_loss is 1.87585, inv_loss is 8220.43, class_loss is 1.78297\n",
            "step 32 train accuracy is 0.692, total_loss is 1.81994, inv_loss is 7135.57, class_loss is 1.79882\n",
            "step 33 train accuracy is 0.696, total_loss is 1.80442, inv_loss is 7019.34, class_loss is 1.83054\n",
            "step 34 train accuracy is 0.736, total_loss is 1.77132, inv_loss is 6302.98, class_loss is 1.74554\n",
            "step 35 train accuracy is 0.732, total_loss is 1.6951, inv_loss is 6655.69, class_loss is 1.79599\n",
            "step 36 train accuracy is 0.688, total_loss is 1.79665, inv_loss is 7331.62, class_loss is 1.7962\n",
            "step 37 train accuracy is 0.72, total_loss is 1.74084, inv_loss is 7197.07, class_loss is 1.72667\n",
            "step 38 train accuracy is 0.724, total_loss is 1.67373, inv_loss is 6571.7, class_loss is 1.73707\n",
            "step 39 train accuracy is 0.848, total_loss is 1.78451, inv_loss is 7088.2, class_loss is 1.68815\n",
            "step 40 train accuracy is 0.684, total_loss is 1.7964, inv_loss is 6940.06, class_loss is 1.76401\n",
            "step 41 train accuracy is 0.776, total_loss is 1.719, inv_loss is 7460.22, class_loss is 1.72761\n",
            "step 42 train accuracy is 0.756, total_loss is 1.71805, inv_loss is 7189.29, class_loss is 1.72442\n",
            "step 43 train accuracy is 0.728, total_loss is 1.69616, inv_loss is 7642.83, class_loss is 1.69336\n",
            "step 44 train accuracy is 0.808, total_loss is 1.71607, inv_loss is 6475.77, class_loss is 1.71569\n",
            "step 45 train accuracy is 0.744, total_loss is 1.70369, inv_loss is 8591.91, class_loss is 1.74916\n",
            "step 46 train accuracy is 0.8, total_loss is 1.63707, inv_loss is 7885.59, class_loss is 1.71171\n",
            "step 47 train accuracy is 0.768, total_loss is 1.72393, inv_loss is 7153.93, class_loss is 1.78255\n",
            "step 48 train accuracy is 0.684, total_loss is 1.77145, inv_loss is 8427.17, class_loss is 1.77395\n",
            "step 49 train accuracy is 0.68, total_loss is 1.76379, inv_loss is 6947.24, class_loss is 1.74805\n",
            "step 50 train accuracy is 0.732, total_loss is 1.72477, inv_loss is 7788.32, class_loss is 1.7052\n",
            "step 51 train accuracy is 0.724, total_loss is 1.64296, inv_loss is 6407.04, class_loss is 1.66468\n",
            "step 52 train accuracy is 0.784, total_loss is 1.65884, inv_loss is 7001.99, class_loss is 1.71474\n",
            "step 53 train accuracy is 0.7, total_loss is 1.68349, inv_loss is 7627.03, class_loss is 1.74676\n",
            "step 54 train accuracy is 0.72, total_loss is 1.64648, inv_loss is 7222.49, class_loss is 1.77327\n",
            "step 55 train accuracy is 0.716, total_loss is 1.6907, inv_loss is 6573.41, class_loss is 1.74777\n",
            "step 56 train accuracy is 0.712, total_loss is 1.75544, inv_loss is 6973, class_loss is 1.74154\n",
            "step 57 train accuracy is 0.732, total_loss is 1.77823, inv_loss is 6764.58, class_loss is 1.72506\n",
            "step 58 train accuracy is 0.772, total_loss is 1.69593, inv_loss is 8043.23, class_loss is 1.71744\n",
            "step 59 train accuracy is 0.712, total_loss is 1.6879, inv_loss is 6669.33, class_loss is 1.78204\n",
            "step 60 train accuracy is 0.752, total_loss is 1.68183, inv_loss is 6761.9, class_loss is 1.75592\n",
            "step 61 train accuracy is 0.692, total_loss is 1.77185, inv_loss is 6791.78, class_loss is 1.72162\n",
            "step 62 train accuracy is 0.76, total_loss is 1.69527, inv_loss is 6902.72, class_loss is 1.70359\n",
            "step 63 train accuracy is 0.768, total_loss is 1.67363, inv_loss is 7052.17, class_loss is 1.71194\n",
            "step 64 train accuracy is 0.796, total_loss is 1.67972, inv_loss is 8219.83, class_loss is 1.69968\n",
            "step 65 train accuracy is 0.776, total_loss is 1.74014, inv_loss is 7421.45, class_loss is 1.70459\n",
            "step 66 train accuracy is 0.784, total_loss is 1.72532, inv_loss is 7377.56, class_loss is 1.65081\n",
            "step 67 train accuracy is 0.792, total_loss is 1.72958, inv_loss is 6614.92, class_loss is 1.6674\n",
            "step 68 train accuracy is 0.728, total_loss is 1.72231, inv_loss is 7132.26, class_loss is 1.61145\n",
            "step 69 train accuracy is 0.78, total_loss is 1.70731, inv_loss is 6925.05, class_loss is 1.7551\n",
            "step 70 train accuracy is 0.716, total_loss is 1.71245, inv_loss is 6272.18, class_loss is 1.74186\n",
            "step 71 train accuracy is 0.776, total_loss is 1.76226, inv_loss is 8220.43, class_loss is 1.69555\n",
            "step 72 train accuracy is 0.828, total_loss is 1.721, inv_loss is 7135.57, class_loss is 1.69352\n",
            "step 73 train accuracy is 0.8, total_loss is 1.73301, inv_loss is 7019.34, class_loss is 1.67498\n",
            "step 74 train accuracy is 0.796, total_loss is 1.67803, inv_loss is 6302.98, class_loss is 1.66361\n",
            "step 75 train accuracy is 0.756, total_loss is 1.67769, inv_loss is 6655.69, class_loss is 1.70954\n",
            "step 76 train accuracy is 0.776, total_loss is 1.71533, inv_loss is 7331.62, class_loss is 1.68349\n",
            "step 77 train accuracy is 0.752, total_loss is 1.74356, inv_loss is 7197.07, class_loss is 1.70353\n",
            "step 78 train accuracy is 0.776, total_loss is 1.65662, inv_loss is 6571.7, class_loss is 1.68831\n",
            "step 79 train accuracy is 0.884, total_loss is 1.68499, inv_loss is 7088.2, class_loss is 1.6135\n",
            "step 80 train accuracy is 0.732, total_loss is 1.73035, inv_loss is 6940.06, class_loss is 1.76768\n",
            "step 81 train accuracy is 0.796, total_loss is 1.67698, inv_loss is 7460.22, class_loss is 1.68585\n",
            "step 82 train accuracy is 0.788, total_loss is 1.64783, inv_loss is 7189.29, class_loss is 1.66452\n",
            "step 83 train accuracy is 0.78, total_loss is 1.65709, inv_loss is 7642.83, class_loss is 1.64855\n",
            "step 84 train accuracy is 0.816, total_loss is 1.69143, inv_loss is 6475.77, class_loss is 1.68988\n",
            "step 85 train accuracy is 0.752, total_loss is 1.69208, inv_loss is 8591.91, class_loss is 1.77681\n",
            "step 86 train accuracy is 0.736, total_loss is 1.64424, inv_loss is 7885.59, class_loss is 1.67769\n",
            "step 87 train accuracy is 0.82, total_loss is 1.66556, inv_loss is 7153.93, class_loss is 1.68693\n",
            "step 88 train accuracy is 0.736, total_loss is 1.70219, inv_loss is 8427.17, class_loss is 1.67338\n",
            "step 89 train accuracy is 0.744, total_loss is 1.70847, inv_loss is 6947.24, class_loss is 1.70718\n",
            "step 90 train accuracy is 0.776, total_loss is 1.73489, inv_loss is 7788.32, class_loss is 1.65244\n",
            "step 91 train accuracy is 0.78, total_loss is 1.62005, inv_loss is 6407.04, class_loss is 1.63512\n",
            "step 92 train accuracy is 0.832, total_loss is 1.61736, inv_loss is 7001.99, class_loss is 1.69646\n",
            "step 93 train accuracy is 0.768, total_loss is 1.63627, inv_loss is 7627.03, class_loss is 1.73635\n",
            "step 94 train accuracy is 0.804, total_loss is 1.61445, inv_loss is 7222.49, class_loss is 1.69346\n",
            "step 95 train accuracy is 0.776, total_loss is 1.63955, inv_loss is 6573.41, class_loss is 1.68861\n",
            "step 96 train accuracy is 0.768, total_loss is 1.71244, inv_loss is 6973, class_loss is 1.64214\n",
            "step 97 train accuracy is 0.78, total_loss is 1.7611, inv_loss is 6764.58, class_loss is 1.69316\n",
            "step 98 train accuracy is 0.76, total_loss is 1.69455, inv_loss is 8043.23, class_loss is 1.72576\n",
            "step 99 train accuracy is 0.748, total_loss is 1.66438, inv_loss is 6669.33, class_loss is 1.72486\n",
            "step 100 train accuracy is 0.764, total_loss is 1.66455, inv_loss is 6761.9, class_loss is 1.77998\n",
            "step 101 train accuracy is 0.76, total_loss is 1.79345, inv_loss is 6791.78, class_loss is 1.7227\n",
            "step 102 train accuracy is 0.824, total_loss is 1.63583, inv_loss is 6902.72, class_loss is 1.70695\n",
            "step 103 train accuracy is 0.748, total_loss is 1.64869, inv_loss is 7052.17, class_loss is 1.66581\n",
            "step 104 train accuracy is 0.804, total_loss is 1.69107, inv_loss is 8219.83, class_loss is 1.72438\n",
            "step 105 train accuracy is 0.772, total_loss is 1.67935, inv_loss is 7421.45, class_loss is 1.66074\n",
            "step 106 train accuracy is 0.74, total_loss is 1.70128, inv_loss is 7377.56, class_loss is 1.66253\n",
            "step 107 train accuracy is 0.772, total_loss is 1.75185, inv_loss is 6614.92, class_loss is 1.70295\n",
            "step 108 train accuracy is 0.76, total_loss is 1.70135, inv_loss is 7132.26, class_loss is 1.61207\n",
            "step 109 train accuracy is 0.772, total_loss is 1.6926, inv_loss is 6925.05, class_loss is 1.73067\n",
            "step 110 train accuracy is 0.772, total_loss is 1.69111, inv_loss is 6272.18, class_loss is 1.72805\n",
            "step 111 train accuracy is 0.808, total_loss is 1.71982, inv_loss is 8220.43, class_loss is 1.70463\n",
            "step 112 train accuracy is 0.824, total_loss is 1.70802, inv_loss is 7135.57, class_loss is 1.68933\n",
            "step 113 train accuracy is 0.788, total_loss is 1.76067, inv_loss is 7019.34, class_loss is 1.70006\n",
            "step 114 train accuracy is 0.82, total_loss is 1.64956, inv_loss is 6302.98, class_loss is 1.65002\n",
            "step 115 train accuracy is 0.78, total_loss is 1.64081, inv_loss is 6655.69, class_loss is 1.65724\n",
            "step 116 train accuracy is 0.836, total_loss is 1.68305, inv_loss is 7331.62, class_loss is 1.65565\n",
            "step 117 train accuracy is 0.816, total_loss is 1.67419, inv_loss is 7197.07, class_loss is 1.62769\n",
            "step 118 train accuracy is 0.796, total_loss is 1.6305, inv_loss is 6571.7, class_loss is 1.67773\n",
            "step 119 train accuracy is 0.892, total_loss is 1.68896, inv_loss is 7088.2, class_loss is 1.62226\n",
            "step 120 train accuracy is 0.696, total_loss is 1.70727, inv_loss is 6940.06, class_loss is 1.73474\n",
            "step 121 train accuracy is 0.824, total_loss is 1.69338, inv_loss is 7460.22, class_loss is 1.68138\n",
            "step 122 train accuracy is 0.8, total_loss is 1.62909, inv_loss is 7189.29, class_loss is 1.64649\n",
            "step 123 train accuracy is 0.816, total_loss is 1.66349, inv_loss is 7642.83, class_loss is 1.638\n",
            "step 124 train accuracy is 0.808, total_loss is 1.67371, inv_loss is 6475.77, class_loss is 1.67007\n",
            "step 125 train accuracy is 0.768, total_loss is 1.6653, inv_loss is 8591.91, class_loss is 1.74238\n",
            "step 126 train accuracy is 0.82, total_loss is 1.63837, inv_loss is 7885.59, class_loss is 1.6527\n",
            "step 127 train accuracy is 0.816, total_loss is 1.65304, inv_loss is 7153.93, class_loss is 1.69821\n",
            "step 128 train accuracy is 0.756, total_loss is 1.69958, inv_loss is 8427.17, class_loss is 1.68595\n",
            "step 129 train accuracy is 0.764, total_loss is 1.69521, inv_loss is 6947.24, class_loss is 1.68152\n",
            "step 130 train accuracy is 0.804, total_loss is 1.7027, inv_loss is 7788.32, class_loss is 1.65429\n",
            "step 131 train accuracy is 0.756, total_loss is 1.63826, inv_loss is 6407.04, class_loss is 1.63998\n",
            "step 132 train accuracy is 0.812, total_loss is 1.61724, inv_loss is 7001.99, class_loss is 1.68873\n",
            "step 133 train accuracy is 0.732, total_loss is 1.65625, inv_loss is 7627.03, class_loss is 1.7636\n",
            "step 134 train accuracy is 0.808, total_loss is 1.60935, inv_loss is 7222.49, class_loss is 1.69551\n",
            "step 135 train accuracy is 0.776, total_loss is 1.65416, inv_loss is 6573.41, class_loss is 1.6839\n",
            "step 136 train accuracy is 0.78, total_loss is 1.70132, inv_loss is 6973, class_loss is 1.64106\n",
            "step 137 train accuracy is 0.792, total_loss is 1.69524, inv_loss is 6764.58, class_loss is 1.66922\n",
            "step 138 train accuracy is 0.772, total_loss is 1.67828, inv_loss is 8043.23, class_loss is 1.69125\n",
            "step 139 train accuracy is 0.764, total_loss is 1.66517, inv_loss is 6669.33, class_loss is 1.72777\n",
            "step 140 train accuracy is 0.72, total_loss is 1.68044, inv_loss is 6761.9, class_loss is 1.75064\n",
            "step 141 train accuracy is 0.732, total_loss is 1.78665, inv_loss is 6791.78, class_loss is 1.69073\n",
            "step 142 train accuracy is 0.84, total_loss is 1.60838, inv_loss is 6902.72, class_loss is 1.70066\n",
            "step 143 train accuracy is 0.768, total_loss is 1.66196, inv_loss is 7052.17, class_loss is 1.69661\n",
            "step 144 train accuracy is 0.812, total_loss is 1.72308, inv_loss is 8219.83, class_loss is 1.71431\n",
            "step 145 train accuracy is 0.784, total_loss is 1.69332, inv_loss is 7421.45, class_loss is 1.64082\n",
            "step 146 train accuracy is 0.808, total_loss is 1.69622, inv_loss is 7377.56, class_loss is 1.64869\n",
            "step 147 train accuracy is 0.768, total_loss is 1.71484, inv_loss is 6614.92, class_loss is 1.66045\n",
            "step 148 train accuracy is 0.784, total_loss is 1.69246, inv_loss is 7132.26, class_loss is 1.61046\n",
            "step 149 train accuracy is 0.736, total_loss is 1.71919, inv_loss is 6925.05, class_loss is 1.73742\n",
            "step 150 train accuracy is 0.768, total_loss is 1.69216, inv_loss is 6272.18, class_loss is 1.71977\n",
            "step 151 train accuracy is 0.808, total_loss is 1.72482, inv_loss is 8220.43, class_loss is 1.67799\n",
            "step 152 train accuracy is 0.816, total_loss is 1.70326, inv_loss is 7135.57, class_loss is 1.67428\n",
            "step 153 train accuracy is 0.828, total_loss is 1.72224, inv_loss is 7019.34, class_loss is 1.6754\n",
            "step 154 train accuracy is 0.796, total_loss is 1.68886, inv_loss is 6302.98, class_loss is 1.66651\n",
            "step 155 train accuracy is 0.748, total_loss is 1.6304, inv_loss is 6655.69, class_loss is 1.70576\n",
            "step 156 train accuracy is 0.808, total_loss is 1.72332, inv_loss is 7331.62, class_loss is 1.66446\n",
            "step 157 train accuracy is 0.82, total_loss is 1.65553, inv_loss is 7197.07, class_loss is 1.62864\n",
            "step 158 train accuracy is 0.776, total_loss is 1.61707, inv_loss is 6571.7, class_loss is 1.66068\n",
            "step 159 train accuracy is 0.896, total_loss is 1.7074, inv_loss is 7088.2, class_loss is 1.61103\n",
            "step 160 train accuracy is 0.768, total_loss is 1.69581, inv_loss is 6940.06, class_loss is 1.67222\n",
            "step 161 train accuracy is 0.808, total_loss is 1.67637, inv_loss is 7460.22, class_loss is 1.65932\n",
            "step 162 train accuracy is 0.792, total_loss is 1.64028, inv_loss is 7189.29, class_loss is 1.63879\n",
            "step 163 train accuracy is 0.8, total_loss is 1.66092, inv_loss is 7642.83, class_loss is 1.64473\n",
            "step 164 train accuracy is 0.768, total_loss is 1.75758, inv_loss is 6475.77, class_loss is 1.74086\n",
            "step 165 train accuracy is 0.776, total_loss is 1.66977, inv_loss is 8591.91, class_loss is 1.72513\n",
            "step 166 train accuracy is 0.784, total_loss is 1.65364, inv_loss is 7885.59, class_loss is 1.67034\n",
            "step 167 train accuracy is 0.816, total_loss is 1.67069, inv_loss is 7153.93, class_loss is 1.68565\n",
            "step 168 train accuracy is 0.752, total_loss is 1.67577, inv_loss is 8427.17, class_loss is 1.66057\n",
            "step 169 train accuracy is 0.736, total_loss is 1.69749, inv_loss is 6947.24, class_loss is 1.68566\n",
            "step 170 train accuracy is 0.812, total_loss is 1.70957, inv_loss is 7788.32, class_loss is 1.6568\n",
            "step 171 train accuracy is 0.78, total_loss is 1.61303, inv_loss is 6407.04, class_loss is 1.61453\n",
            "step 172 train accuracy is 0.848, total_loss is 1.62448, inv_loss is 7001.99, class_loss is 1.66526\n",
            "step 173 train accuracy is 0.728, total_loss is 1.59295, inv_loss is 7627.03, class_loss is 1.73765\n",
            "step 174 train accuracy is 0.772, total_loss is 1.65038, inv_loss is 7222.49, class_loss is 1.69692\n",
            "step 175 train accuracy is 0.768, total_loss is 1.67861, inv_loss is 6573.41, class_loss is 1.69563\n",
            "step 176 train accuracy is 0.796, total_loss is 1.6932, inv_loss is 6973, class_loss is 1.66186\n",
            "step 177 train accuracy is 0.792, total_loss is 1.70496, inv_loss is 6764.58, class_loss is 1.67299\n",
            "step 178 train accuracy is 0.796, total_loss is 1.65737, inv_loss is 8043.23, class_loss is 1.65848\n",
            "step 179 train accuracy is 0.776, total_loss is 1.63843, inv_loss is 6669.33, class_loss is 1.73233\n",
            "step 180 train accuracy is 0.764, total_loss is 1.66887, inv_loss is 6761.9, class_loss is 1.74002\n",
            "step 181 train accuracy is 0.764, total_loss is 1.74909, inv_loss is 6791.78, class_loss is 1.68496\n",
            "step 182 train accuracy is 0.82, total_loss is 1.60208, inv_loss is 6902.72, class_loss is 1.66995\n",
            "step 183 train accuracy is 0.804, total_loss is 1.62914, inv_loss is 7052.17, class_loss is 1.66827\n",
            "step 184 train accuracy is 0.776, total_loss is 1.7367, inv_loss is 8219.83, class_loss is 1.73105\n",
            "step 185 train accuracy is 0.808, total_loss is 1.68807, inv_loss is 7421.45, class_loss is 1.66267\n",
            "step 186 train accuracy is 0.756, total_loss is 1.74664, inv_loss is 7377.56, class_loss is 1.65624\n",
            "step 187 train accuracy is 0.788, total_loss is 1.7408, inv_loss is 6614.92, class_loss is 1.68208\n",
            "step 188 train accuracy is 0.728, total_loss is 1.73088, inv_loss is 7132.26, class_loss is 1.59793\n",
            "step 189 train accuracy is 0.76, total_loss is 1.68782, inv_loss is 6925.05, class_loss is 1.74396\n",
            "step 190 train accuracy is 0.76, total_loss is 1.6929, inv_loss is 6272.18, class_loss is 1.73303\n",
            "step 191 train accuracy is 0.8, total_loss is 1.71293, inv_loss is 8220.43, class_loss is 1.69077\n",
            "step 192 train accuracy is 0.828, total_loss is 1.68851, inv_loss is 7135.57, class_loss is 1.65544\n",
            "step 193 train accuracy is 0.836, total_loss is 1.71163, inv_loss is 7019.34, class_loss is 1.66325\n",
            "step 194 train accuracy is 0.792, total_loss is 1.67099, inv_loss is 6302.98, class_loss is 1.69371\n",
            "step 195 train accuracy is 0.804, total_loss is 1.59854, inv_loss is 6655.69, class_loss is 1.66043\n",
            "step 196 train accuracy is 0.804, total_loss is 1.70167, inv_loss is 7331.62, class_loss is 1.66822\n",
            "step 197 train accuracy is 0.816, total_loss is 1.68466, inv_loss is 7197.07, class_loss is 1.60987\n",
            "step 198 train accuracy is 0.816, total_loss is 1.60701, inv_loss is 6571.7, class_loss is 1.64497\n",
            "step 199 train accuracy is 0.888, total_loss is 1.73574, inv_loss is 7088.2, class_loss is 1.63242\n",
            "beta is 0, test accuracy is 0.7969\n",
            "Training...\n",
            "step 0 train accuracy is 0.456, total_loss is -5.4145, inv_loss is 6940.06, class_loss is 2.02891\n",
            "step 1 train accuracy is 0.644, total_loss is -5.38585, inv_loss is 7460.22, class_loss is 1.83367\n",
            "step 2 train accuracy is 0.604, total_loss is -5.08226, inv_loss is 7189.29, class_loss is 1.86305\n",
            "step 3 train accuracy is 0.48, total_loss is -6.08765, inv_loss is 7642.83, class_loss is 1.96985\n",
            "step 4 train accuracy is 0.588, total_loss is -4.95565, inv_loss is 6475.77, class_loss is 2.01436\n",
            "step 5 train accuracy is 0.576, total_loss is -6.029, inv_loss is 8591.91, class_loss is 1.86913\n",
            "step 6 train accuracy is 0.584, total_loss is -5.27802, inv_loss is 7885.59, class_loss is 1.86179\n",
            "step 7 train accuracy is 0.616, total_loss is -5.35791, inv_loss is 7153.93, class_loss is 1.87765\n",
            "step 8 train accuracy is 0.58, total_loss is -5.35258, inv_loss is 8427.17, class_loss is 1.91599\n",
            "step 9 train accuracy is 0.52, total_loss is -5.76917, inv_loss is 6947.24, class_loss is 1.94477\n",
            "step 10 train accuracy is 0.588, total_loss is -5.09993, inv_loss is 7788.32, class_loss is 1.89338\n",
            "step 11 train accuracy is 0.488, total_loss is -5.3503, inv_loss is 6407.04, class_loss is 1.92438\n",
            "step 12 train accuracy is 0.616, total_loss is -5.115, inv_loss is 7001.99, class_loss is 1.90812\n",
            "step 13 train accuracy is 0.58, total_loss is -5.3576, inv_loss is 7627.03, class_loss is 1.89722\n",
            "step 14 train accuracy is 0.636, total_loss is -6.85978, inv_loss is 7222.49, class_loss is 1.89091\n",
            "step 15 train accuracy is 0.524, total_loss is -5.80006, inv_loss is 6573.41, class_loss is 1.92325\n",
            "step 16 train accuracy is 0.508, total_loss is -4.89825, inv_loss is 6973, class_loss is 1.79644\n",
            "step 17 train accuracy is 0.632, total_loss is -4.71022, inv_loss is 6764.58, class_loss is 1.79423\n",
            "step 18 train accuracy is 0.64, total_loss is -5.40604, inv_loss is 8043.23, class_loss is 1.78606\n",
            "step 19 train accuracy is 0.64, total_loss is -6.74891, inv_loss is 6669.33, class_loss is 1.89092\n",
            "step 20 train accuracy is 0.62, total_loss is -4.97576, inv_loss is 6761.9, class_loss is 1.87894\n",
            "step 21 train accuracy is 0.624, total_loss is -6.29577, inv_loss is 6791.78, class_loss is 1.83275\n",
            "step 22 train accuracy is 0.66, total_loss is -5.8336, inv_loss is 6902.72, class_loss is 1.82255\n",
            "step 23 train accuracy is 0.604, total_loss is -5.5173, inv_loss is 7052.17, class_loss is 1.82072\n",
            "step 24 train accuracy is 0.652, total_loss is -5.9394, inv_loss is 8219.83, class_loss is 1.83649\n",
            "step 25 train accuracy is 0.664, total_loss is -5.03464, inv_loss is 7421.45, class_loss is 1.79727\n",
            "step 26 train accuracy is 0.564, total_loss is -4.59354, inv_loss is 7377.56, class_loss is 1.80197\n",
            "step 27 train accuracy is 0.6, total_loss is -5.40479, inv_loss is 6614.92, class_loss is 1.82647\n",
            "step 28 train accuracy is 0.62, total_loss is -5.5097, inv_loss is 7132.26, class_loss is 1.7414\n",
            "step 29 train accuracy is 0.7, total_loss is -5.40573, inv_loss is 6925.05, class_loss is 1.85482\n",
            "step 30 train accuracy is 0.656, total_loss is -4.81337, inv_loss is 6272.18, class_loss is 1.80501\n",
            "step 31 train accuracy is 0.656, total_loss is -5.12869, inv_loss is 8220.43, class_loss is 1.78297\n",
            "step 32 train accuracy is 0.692, total_loss is -5.23235, inv_loss is 7135.57, class_loss is 1.79882\n",
            "step 33 train accuracy is 0.696, total_loss is -5.10536, inv_loss is 7019.34, class_loss is 1.83054\n",
            "step 34 train accuracy is 0.736, total_loss is -5.10567, inv_loss is 6302.98, class_loss is 1.74554\n",
            "step 35 train accuracy is 0.732, total_loss is -5.24199, inv_loss is 6655.69, class_loss is 1.79599\n",
            "step 36 train accuracy is 0.688, total_loss is -5.50313, inv_loss is 7331.62, class_loss is 1.7962\n",
            "step 37 train accuracy is 0.72, total_loss is -4.81345, inv_loss is 7197.07, class_loss is 1.72667\n",
            "step 38 train accuracy is 0.724, total_loss is -4.89935, inv_loss is 6571.7, class_loss is 1.73707\n",
            "step 39 train accuracy is 0.848, total_loss is -8.44568, inv_loss is 7088.2, class_loss is 1.68815\n",
            "step 40 train accuracy is 0.684, total_loss is -5.65796, inv_loss is 6940.06, class_loss is 1.76401\n",
            "step 41 train accuracy is 0.776, total_loss is -5.51408, inv_loss is 7460.22, class_loss is 1.72761\n",
            "step 42 train accuracy is 0.756, total_loss is -5.2234, inv_loss is 7189.29, class_loss is 1.72442\n",
            "step 43 train accuracy is 0.728, total_loss is -6.42924, inv_loss is 7642.83, class_loss is 1.69336\n",
            "step 44 train accuracy is 0.808, total_loss is -5.24603, inv_loss is 6475.77, class_loss is 1.71569\n",
            "step 45 train accuracy is 0.744, total_loss is -6.26603, inv_loss is 8591.91, class_loss is 1.74916\n",
            "step 46 train accuracy is 0.8, total_loss is -5.47525, inv_loss is 7885.59, class_loss is 1.71171\n",
            "step 47 train accuracy is 0.768, total_loss is -5.49282, inv_loss is 7153.93, class_loss is 1.78255\n",
            "step 48 train accuracy is 0.684, total_loss is -5.49749, inv_loss is 8427.17, class_loss is 1.77395\n",
            "step 49 train accuracy is 0.68, total_loss is -5.85803, inv_loss is 6947.24, class_loss is 1.74805\n",
            "step 50 train accuracy is 0.732, total_loss is -5.32896, inv_loss is 7788.32, class_loss is 1.7052\n",
            "step 51 train accuracy is 0.724, total_loss is -5.64597, inv_loss is 6407.04, class_loss is 1.66468\n",
            "step 52 train accuracy is 0.784, total_loss is -5.28322, inv_loss is 7001.99, class_loss is 1.71474\n",
            "step 53 train accuracy is 0.7, total_loss is -5.51544, inv_loss is 7627.03, class_loss is 1.74676\n",
            "step 54 train accuracy is 0.72, total_loss is -7.0016, inv_loss is 7222.49, class_loss is 1.77327\n",
            "step 55 train accuracy is 0.716, total_loss is -5.98288, inv_loss is 6573.41, class_loss is 1.74777\n",
            "step 56 train accuracy is 0.712, total_loss is -4.97634, inv_loss is 6973, class_loss is 1.74154\n",
            "step 57 train accuracy is 0.732, total_loss is -4.77601, inv_loss is 6764.58, class_loss is 1.72506\n",
            "step 58 train accuracy is 0.772, total_loss is -5.52113, inv_loss is 8043.23, class_loss is 1.71744\n",
            "step 59 train accuracy is 0.712, total_loss is -6.88098, inv_loss is 6669.33, class_loss is 1.78204\n",
            "step 60 train accuracy is 0.752, total_loss is -5.13081, inv_loss is 6761.9, class_loss is 1.75592\n",
            "step 61 train accuracy is 0.692, total_loss is -6.46322, inv_loss is 6791.78, class_loss is 1.72162\n",
            "step 62 train accuracy is 0.76, total_loss is -5.91613, inv_loss is 6902.72, class_loss is 1.70359\n",
            "step 63 train accuracy is 0.768, total_loss is -5.69318, inv_loss is 7052.17, class_loss is 1.71194\n",
            "step 64 train accuracy is 0.796, total_loss is -6.07736, inv_loss is 8219.83, class_loss is 1.69968\n",
            "step 65 train accuracy is 0.776, total_loss is -5.06497, inv_loss is 7421.45, class_loss is 1.70459\n",
            "step 66 train accuracy is 0.784, total_loss is -4.76166, inv_loss is 7377.56, class_loss is 1.65081\n",
            "step 67 train accuracy is 0.792, total_loss is -5.53306, inv_loss is 6614.92, class_loss is 1.6674\n",
            "step 68 train accuracy is 0.728, total_loss is -5.58387, inv_loss is 7132.26, class_loss is 1.61145\n",
            "step 69 train accuracy is 0.78, total_loss is -5.44835, inv_loss is 6925.05, class_loss is 1.7551\n",
            "step 70 train accuracy is 0.716, total_loss is -4.87903, inv_loss is 6272.18, class_loss is 1.74186\n",
            "step 71 train accuracy is 0.776, total_loss is -5.24228, inv_loss is 8220.43, class_loss is 1.69555\n",
            "step 72 train accuracy is 0.828, total_loss is -5.33129, inv_loss is 7135.57, class_loss is 1.69352\n",
            "step 73 train accuracy is 0.8, total_loss is -5.17676, inv_loss is 7019.34, class_loss is 1.67498\n",
            "step 74 train accuracy is 0.796, total_loss is -5.19896, inv_loss is 6302.98, class_loss is 1.66361\n",
            "step 75 train accuracy is 0.756, total_loss is -5.2594, inv_loss is 6655.69, class_loss is 1.70954\n",
            "step 76 train accuracy is 0.776, total_loss is -5.58445, inv_loss is 7331.62, class_loss is 1.68349\n",
            "step 77 train accuracy is 0.752, total_loss is -4.81073, inv_loss is 7197.07, class_loss is 1.70353\n",
            "step 78 train accuracy is 0.776, total_loss is -4.91646, inv_loss is 6571.7, class_loss is 1.68831\n",
            "step 79 train accuracy is 0.884, total_loss is -8.5452, inv_loss is 7088.2, class_loss is 1.6135\n",
            "step 80 train accuracy is 0.732, total_loss is -5.72402, inv_loss is 6940.06, class_loss is 1.76768\n",
            "step 81 train accuracy is 0.796, total_loss is -5.5561, inv_loss is 7460.22, class_loss is 1.68585\n",
            "step 82 train accuracy is 0.788, total_loss is -5.29362, inv_loss is 7189.29, class_loss is 1.66452\n",
            "step 83 train accuracy is 0.78, total_loss is -6.46831, inv_loss is 7642.83, class_loss is 1.64855\n",
            "step 84 train accuracy is 0.816, total_loss is -5.27066, inv_loss is 6475.77, class_loss is 1.68988\n",
            "step 85 train accuracy is 0.752, total_loss is -6.27763, inv_loss is 8591.91, class_loss is 1.77681\n",
            "step 86 train accuracy is 0.736, total_loss is -5.46807, inv_loss is 7885.59, class_loss is 1.67769\n",
            "step 87 train accuracy is 0.82, total_loss is -5.55119, inv_loss is 7153.93, class_loss is 1.68693\n",
            "step 88 train accuracy is 0.736, total_loss is -5.56675, inv_loss is 8427.17, class_loss is 1.67338\n",
            "step 89 train accuracy is 0.744, total_loss is -5.91335, inv_loss is 6947.24, class_loss is 1.70718\n",
            "step 90 train accuracy is 0.776, total_loss is -5.31885, inv_loss is 7788.32, class_loss is 1.65244\n",
            "step 91 train accuracy is 0.78, total_loss is -5.66888, inv_loss is 6407.04, class_loss is 1.63512\n",
            "step 92 train accuracy is 0.832, total_loss is -5.3247, inv_loss is 7001.99, class_loss is 1.69646\n",
            "step 93 train accuracy is 0.768, total_loss is -5.56265, inv_loss is 7627.03, class_loss is 1.73635\n",
            "step 94 train accuracy is 0.804, total_loss is -7.03363, inv_loss is 7222.49, class_loss is 1.69346\n",
            "step 95 train accuracy is 0.776, total_loss is -6.03403, inv_loss is 6573.41, class_loss is 1.68861\n",
            "step 96 train accuracy is 0.768, total_loss is -5.01935, inv_loss is 6973, class_loss is 1.64214\n",
            "step 97 train accuracy is 0.78, total_loss is -4.79314, inv_loss is 6764.58, class_loss is 1.69316\n",
            "step 98 train accuracy is 0.76, total_loss is -5.52251, inv_loss is 8043.23, class_loss is 1.72576\n",
            "step 99 train accuracy is 0.748, total_loss is -6.9045, inv_loss is 6669.33, class_loss is 1.72486\n",
            "step 100 train accuracy is 0.764, total_loss is -5.14808, inv_loss is 6761.9, class_loss is 1.77998\n",
            "step 101 train accuracy is 0.76, total_loss is -6.44162, inv_loss is 6791.78, class_loss is 1.7227\n",
            "step 102 train accuracy is 0.824, total_loss is -5.97558, inv_loss is 6902.72, class_loss is 1.70695\n",
            "step 103 train accuracy is 0.748, total_loss is -5.71812, inv_loss is 7052.17, class_loss is 1.66581\n",
            "step 104 train accuracy is 0.804, total_loss is -6.06601, inv_loss is 8219.83, class_loss is 1.72438\n",
            "step 105 train accuracy is 0.772, total_loss is -5.12577, inv_loss is 7421.45, class_loss is 1.66074\n",
            "step 106 train accuracy is 0.74, total_loss is -4.78571, inv_loss is 7377.56, class_loss is 1.66253\n",
            "step 107 train accuracy is 0.772, total_loss is -5.51079, inv_loss is 6614.92, class_loss is 1.70295\n",
            "step 108 train accuracy is 0.76, total_loss is -5.60483, inv_loss is 7132.26, class_loss is 1.61207\n",
            "step 109 train accuracy is 0.772, total_loss is -5.46306, inv_loss is 6925.05, class_loss is 1.73067\n",
            "step 110 train accuracy is 0.772, total_loss is -4.90038, inv_loss is 6272.18, class_loss is 1.72805\n",
            "step 111 train accuracy is 0.808, total_loss is -5.28472, inv_loss is 8220.43, class_loss is 1.70463\n",
            "step 112 train accuracy is 0.824, total_loss is -5.34427, inv_loss is 7135.57, class_loss is 1.68933\n",
            "step 113 train accuracy is 0.788, total_loss is -5.14911, inv_loss is 7019.34, class_loss is 1.70006\n",
            "step 114 train accuracy is 0.82, total_loss is -5.22743, inv_loss is 6302.98, class_loss is 1.65002\n",
            "step 115 train accuracy is 0.78, total_loss is -5.29628, inv_loss is 6655.69, class_loss is 1.65724\n",
            "step 116 train accuracy is 0.836, total_loss is -5.61674, inv_loss is 7331.62, class_loss is 1.65565\n",
            "step 117 train accuracy is 0.816, total_loss is -4.8801, inv_loss is 7197.07, class_loss is 1.62769\n",
            "step 118 train accuracy is 0.796, total_loss is -4.94258, inv_loss is 6571.7, class_loss is 1.67773\n",
            "step 119 train accuracy is 0.892, total_loss is -8.54123, inv_loss is 7088.2, class_loss is 1.62226\n",
            "step 120 train accuracy is 0.696, total_loss is -5.74709, inv_loss is 6940.06, class_loss is 1.73474\n",
            "step 121 train accuracy is 0.824, total_loss is -5.53969, inv_loss is 7460.22, class_loss is 1.68138\n",
            "step 122 train accuracy is 0.8, total_loss is -5.31236, inv_loss is 7189.29, class_loss is 1.64649\n",
            "step 123 train accuracy is 0.816, total_loss is -6.46191, inv_loss is 7642.83, class_loss is 1.638\n",
            "step 124 train accuracy is 0.808, total_loss is -5.28839, inv_loss is 6475.77, class_loss is 1.67007\n",
            "step 125 train accuracy is 0.768, total_loss is -6.30441, inv_loss is 8591.91, class_loss is 1.74238\n",
            "step 126 train accuracy is 0.82, total_loss is -5.47395, inv_loss is 7885.59, class_loss is 1.6527\n",
            "step 127 train accuracy is 0.816, total_loss is -5.56372, inv_loss is 7153.93, class_loss is 1.69821\n",
            "step 128 train accuracy is 0.756, total_loss is -5.56936, inv_loss is 8427.17, class_loss is 1.68595\n",
            "step 129 train accuracy is 0.764, total_loss is -5.92661, inv_loss is 6947.24, class_loss is 1.68152\n",
            "step 130 train accuracy is 0.804, total_loss is -5.35103, inv_loss is 7788.32, class_loss is 1.65429\n",
            "step 131 train accuracy is 0.756, total_loss is -5.65066, inv_loss is 6407.04, class_loss is 1.63998\n",
            "step 132 train accuracy is 0.812, total_loss is -5.32482, inv_loss is 7001.99, class_loss is 1.68873\n",
            "step 133 train accuracy is 0.732, total_loss is -5.54268, inv_loss is 7627.03, class_loss is 1.7636\n",
            "step 134 train accuracy is 0.808, total_loss is -7.03873, inv_loss is 7222.49, class_loss is 1.69551\n",
            "step 135 train accuracy is 0.776, total_loss is -6.01941, inv_loss is 6573.41, class_loss is 1.6839\n",
            "step 136 train accuracy is 0.78, total_loss is -5.03047, inv_loss is 6973, class_loss is 1.64106\n",
            "step 137 train accuracy is 0.792, total_loss is -4.85901, inv_loss is 6764.58, class_loss is 1.66922\n",
            "step 138 train accuracy is 0.772, total_loss is -5.53878, inv_loss is 8043.23, class_loss is 1.69125\n",
            "step 139 train accuracy is 0.764, total_loss is -6.90371, inv_loss is 6669.33, class_loss is 1.72777\n",
            "step 140 train accuracy is 0.72, total_loss is -5.13219, inv_loss is 6761.9, class_loss is 1.75064\n",
            "step 141 train accuracy is 0.732, total_loss is -6.44841, inv_loss is 6791.78, class_loss is 1.69073\n",
            "step 142 train accuracy is 0.84, total_loss is -6.00302, inv_loss is 6902.72, class_loss is 1.70066\n",
            "step 143 train accuracy is 0.768, total_loss is -5.70485, inv_loss is 7052.17, class_loss is 1.69661\n",
            "step 144 train accuracy is 0.812, total_loss is -6.034, inv_loss is 8219.83, class_loss is 1.71431\n",
            "step 145 train accuracy is 0.784, total_loss is -5.11179, inv_loss is 7421.45, class_loss is 1.64082\n",
            "step 146 train accuracy is 0.808, total_loss is -4.79076, inv_loss is 7377.56, class_loss is 1.64869\n",
            "step 147 train accuracy is 0.768, total_loss is -5.54779, inv_loss is 6614.92, class_loss is 1.66045\n",
            "step 148 train accuracy is 0.784, total_loss is -5.61372, inv_loss is 7132.26, class_loss is 1.61046\n",
            "step 149 train accuracy is 0.736, total_loss is -5.43647, inv_loss is 6925.05, class_loss is 1.73742\n",
            "step 150 train accuracy is 0.768, total_loss is -4.89932, inv_loss is 6272.18, class_loss is 1.71977\n",
            "step 151 train accuracy is 0.808, total_loss is -5.27972, inv_loss is 8220.43, class_loss is 1.67799\n",
            "step 152 train accuracy is 0.816, total_loss is -5.34903, inv_loss is 7135.57, class_loss is 1.67428\n",
            "step 153 train accuracy is 0.828, total_loss is -5.18753, inv_loss is 7019.34, class_loss is 1.6754\n",
            "step 154 train accuracy is 0.796, total_loss is -5.18814, inv_loss is 6302.98, class_loss is 1.66651\n",
            "step 155 train accuracy is 0.748, total_loss is -5.30668, inv_loss is 6655.69, class_loss is 1.70576\n",
            "step 156 train accuracy is 0.808, total_loss is -5.57647, inv_loss is 7331.62, class_loss is 1.66446\n",
            "step 157 train accuracy is 0.82, total_loss is -4.89876, inv_loss is 7197.07, class_loss is 1.62864\n",
            "step 158 train accuracy is 0.776, total_loss is -4.95601, inv_loss is 6571.7, class_loss is 1.66068\n",
            "step 159 train accuracy is 0.896, total_loss is -8.52279, inv_loss is 7088.2, class_loss is 1.61103\n",
            "step 160 train accuracy is 0.768, total_loss is -5.75855, inv_loss is 6940.06, class_loss is 1.67222\n",
            "step 161 train accuracy is 0.808, total_loss is -5.55671, inv_loss is 7460.22, class_loss is 1.65932\n",
            "step 162 train accuracy is 0.792, total_loss is -5.30117, inv_loss is 7189.29, class_loss is 1.63879\n",
            "step 163 train accuracy is 0.8, total_loss is -6.46449, inv_loss is 7642.83, class_loss is 1.64473\n",
            "step 164 train accuracy is 0.768, total_loss is -5.20451, inv_loss is 6475.77, class_loss is 1.74086\n",
            "step 165 train accuracy is 0.776, total_loss is -6.29994, inv_loss is 8591.91, class_loss is 1.72513\n",
            "step 166 train accuracy is 0.784, total_loss is -5.45868, inv_loss is 7885.59, class_loss is 1.67034\n",
            "step 167 train accuracy is 0.816, total_loss is -5.54606, inv_loss is 7153.93, class_loss is 1.68565\n",
            "step 168 train accuracy is 0.752, total_loss is -5.59317, inv_loss is 8427.17, class_loss is 1.66057\n",
            "step 169 train accuracy is 0.736, total_loss is -5.92434, inv_loss is 6947.24, class_loss is 1.68566\n",
            "step 170 train accuracy is 0.812, total_loss is -5.34416, inv_loss is 7788.32, class_loss is 1.6568\n",
            "step 171 train accuracy is 0.78, total_loss is -5.6759, inv_loss is 6407.04, class_loss is 1.61453\n",
            "step 172 train accuracy is 0.848, total_loss is -5.31758, inv_loss is 7001.99, class_loss is 1.66526\n",
            "step 173 train accuracy is 0.728, total_loss is -5.60598, inv_loss is 7627.03, class_loss is 1.73765\n",
            "step 174 train accuracy is 0.772, total_loss is -6.99769, inv_loss is 7222.49, class_loss is 1.69692\n",
            "step 175 train accuracy is 0.768, total_loss is -5.99497, inv_loss is 6573.41, class_loss is 1.69563\n",
            "step 176 train accuracy is 0.796, total_loss is -5.03859, inv_loss is 6973, class_loss is 1.66186\n",
            "step 177 train accuracy is 0.792, total_loss is -4.84928, inv_loss is 6764.58, class_loss is 1.67299\n",
            "step 178 train accuracy is 0.796, total_loss is -5.55969, inv_loss is 8043.23, class_loss is 1.65848\n",
            "step 179 train accuracy is 0.776, total_loss is -6.93045, inv_loss is 6669.33, class_loss is 1.73233\n",
            "step 180 train accuracy is 0.764, total_loss is -5.14377, inv_loss is 6761.9, class_loss is 1.74002\n",
            "step 181 train accuracy is 0.764, total_loss is -6.48598, inv_loss is 6791.78, class_loss is 1.68496\n",
            "step 182 train accuracy is 0.82, total_loss is -6.00932, inv_loss is 6902.72, class_loss is 1.66995\n",
            "step 183 train accuracy is 0.804, total_loss is -5.73767, inv_loss is 7052.17, class_loss is 1.66827\n",
            "step 184 train accuracy is 0.776, total_loss is -6.02038, inv_loss is 8219.83, class_loss is 1.73105\n",
            "step 185 train accuracy is 0.808, total_loss is -5.11704, inv_loss is 7421.45, class_loss is 1.66267\n",
            "step 186 train accuracy is 0.756, total_loss is -4.74034, inv_loss is 7377.56, class_loss is 1.65624\n",
            "step 187 train accuracy is 0.788, total_loss is -5.52184, inv_loss is 6614.92, class_loss is 1.68208\n",
            "step 188 train accuracy is 0.728, total_loss is -5.5753, inv_loss is 7132.26, class_loss is 1.59793\n",
            "step 189 train accuracy is 0.76, total_loss is -5.46784, inv_loss is 6925.05, class_loss is 1.74396\n",
            "step 190 train accuracy is 0.76, total_loss is -4.89858, inv_loss is 6272.18, class_loss is 1.73303\n",
            "step 191 train accuracy is 0.8, total_loss is -5.29161, inv_loss is 8220.43, class_loss is 1.69077\n",
            "step 192 train accuracy is 0.828, total_loss is -5.36378, inv_loss is 7135.57, class_loss is 1.65544\n",
            "step 193 train accuracy is 0.836, total_loss is -5.19814, inv_loss is 7019.34, class_loss is 1.66325\n",
            "step 194 train accuracy is 0.792, total_loss is -5.206, inv_loss is 6302.98, class_loss is 1.69371\n",
            "step 195 train accuracy is 0.804, total_loss is -5.33854, inv_loss is 6655.69, class_loss is 1.66043\n",
            "step 196 train accuracy is 0.804, total_loss is -5.59812, inv_loss is 7331.62, class_loss is 1.66822\n",
            "step 197 train accuracy is 0.816, total_loss is -4.86963, inv_loss is 7197.07, class_loss is 1.60987\n",
            "step 198 train accuracy is 0.816, total_loss is -4.96607, inv_loss is 6571.7, class_loss is 1.64497\n",
            "step 199 train accuracy is 0.888, total_loss is -8.49445, inv_loss is 7088.2, class_loss is 1.63242\n",
            "beta is 0.001, test accuracy is 0.7969\n",
            "Training...\n",
            "step 0 train accuracy is 0.456, total_loss is -72.5037, inv_loss is 6940.06, class_loss is 2.02891\n",
            "step 1 train accuracy is 0.644, total_loss is -70.4835, inv_loss is 7460.22, class_loss is 1.83367\n",
            "step 2 train accuracy is 0.604, total_loss is -67.5553, inv_loss is 7189.29, class_loss is 1.86305\n",
            "step 3 train accuracy is 0.48, total_loss is -79.2163, inv_loss is 7642.83, class_loss is 1.96985\n",
            "step 4 train accuracy is 0.588, total_loss is -67.6145, inv_loss is 6475.77, class_loss is 2.01436\n",
            "step 5 train accuracy is 0.576, total_loss is -77.7564, inv_loss is 8591.91, class_loss is 1.86913\n",
            "step 6 train accuracy is 0.584, total_loss is -69.2888, inv_loss is 7885.59, class_loss is 1.86179\n",
            "step 7 train accuracy is 0.616, total_loss is -70.3087, inv_loss is 7153.93, class_loss is 1.87765\n",
            "step 8 train accuracy is 0.58, total_loss is -70.7731, inv_loss is 8427.17, class_loss is 1.91599\n",
            "step 9 train accuracy is 0.52, total_loss is -74.3656, inv_loss is 6947.24, class_loss is 1.94477\n",
            "step 10 train accuracy is 0.588, total_loss is -68.5835, inv_loss is 7788.32, class_loss is 1.89338\n",
            "step 11 train accuracy is 0.488, total_loss is -70.9506, inv_loss is 6407.04, class_loss is 1.92438\n",
            "step 12 train accuracy is 0.616, total_loss is -67.5935, inv_loss is 7001.99, class_loss is 1.90812\n",
            "step 13 train accuracy is 0.58, total_loss is -70.1479, inv_loss is 7627.03, class_loss is 1.89722\n",
            "step 14 train accuracy is 0.636, total_loss is -84.6925, inv_loss is 7222.49, class_loss is 1.89091\n",
            "step 15 train accuracy is 0.524, total_loss is -74.8622, inv_loss is 6573.41, class_loss is 1.92325\n",
            "step 16 train accuracy is 0.508, total_loss is -65.4843, inv_loss is 6973, class_loss is 1.79644\n",
            "step 17 train accuracy is 0.632, total_loss is -63.6984, inv_loss is 6764.58, class_loss is 1.79423\n",
            "step 18 train accuracy is 0.64, total_loss is -70.3596, inv_loss is 8043.23, class_loss is 1.78606\n",
            "step 19 train accuracy is 0.64, total_loss is -83.8688, inv_loss is 6669.33, class_loss is 1.89092\n",
            "step 20 train accuracy is 0.62, total_loss is -66.2895, inv_loss is 6761.9, class_loss is 1.87894\n",
            "step 21 train accuracy is 0.624, total_loss is -80.4114, inv_loss is 6791.78, class_loss is 1.83275\n",
            "step 22 train accuracy is 0.66, total_loss is -74.3362, inv_loss is 6902.72, class_loss is 1.82255\n",
            "step 23 train accuracy is 0.604, total_loss is -71.8186, inv_loss is 7052.17, class_loss is 1.82072\n",
            "step 24 train accuracy is 0.652, total_loss is -75.7531, inv_loss is 8219.83, class_loss is 1.83649\n",
            "step 25 train accuracy is 0.664, total_loss is -66.2806, inv_loss is 7421.45, class_loss is 1.79727\n",
            "step 26 train accuracy is 0.564, total_loss is -62.9764, inv_loss is 7377.56, class_loss is 1.80197\n",
            "step 27 train accuracy is 0.6, total_loss is -70.7685, inv_loss is 6614.92, class_loss is 1.82647\n",
            "step 28 train accuracy is 0.62, total_loss is -71.2653, inv_loss is 7132.26, class_loss is 1.7414\n",
            "step 29 train accuracy is 0.7, total_loss is -69.8067, inv_loss is 6925.05, class_loss is 1.85482\n",
            "step 30 train accuracy is 0.656, total_loss is -64.1367, inv_loss is 6272.18, class_loss is 1.80501\n",
            "step 31 train accuracy is 0.656, total_loss is -68.1695, inv_loss is 8220.43, class_loss is 1.78297\n",
            "step 32 train accuracy is 0.692, total_loss is -68.7029, inv_loss is 7135.57, class_loss is 1.79882\n",
            "step 33 train accuracy is 0.696, total_loss is -67.2933, inv_loss is 7019.34, class_loss is 1.83054\n",
            "step 34 train accuracy is 0.736, total_loss is -66.9986, inv_loss is 6302.98, class_loss is 1.74554\n",
            "step 35 train accuracy is 0.732, total_loss is -67.6758, inv_loss is 6655.69, class_loss is 1.79599\n",
            "step 36 train accuracy is 0.688, total_loss is -71.2012, inv_loss is 7331.62, class_loss is 1.7962\n",
            "step 37 train accuracy is 0.72, total_loss is -63.802, inv_loss is 7197.07, class_loss is 1.72667\n",
            "step 38 train accuracy is 0.724, total_loss is -64.0571, inv_loss is 6571.7, class_loss is 1.73707\n",
            "step 39 train accuracy is 0.848, total_loss is -100.517, inv_loss is 7088.2, class_loss is 1.68815\n",
            "step 40 train accuracy is 0.684, total_loss is -72.7472, inv_loss is 6940.06, class_loss is 1.76401\n",
            "step 41 train accuracy is 0.776, total_loss is -70.6118, inv_loss is 7460.22, class_loss is 1.72761\n",
            "step 42 train accuracy is 0.756, total_loss is -67.6965, inv_loss is 7189.29, class_loss is 1.72442\n",
            "step 43 train accuracy is 0.728, total_loss is -79.5579, inv_loss is 7642.83, class_loss is 1.69336\n",
            "step 44 train accuracy is 0.808, total_loss is -67.9049, inv_loss is 6475.77, class_loss is 1.71569\n",
            "step 45 train accuracy is 0.744, total_loss is -77.9934, inv_loss is 8591.91, class_loss is 1.74916\n",
            "step 46 train accuracy is 0.8, total_loss is -69.4861, inv_loss is 7885.59, class_loss is 1.71171\n",
            "step 47 train accuracy is 0.768, total_loss is -70.4436, inv_loss is 7153.93, class_loss is 1.78255\n",
            "step 48 train accuracy is 0.684, total_loss is -70.918, inv_loss is 8427.17, class_loss is 1.77395\n",
            "step 49 train accuracy is 0.68, total_loss is -74.4544, inv_loss is 6947.24, class_loss is 1.74805\n",
            "step 50 train accuracy is 0.732, total_loss is -68.8125, inv_loss is 7788.32, class_loss is 1.7052\n",
            "step 51 train accuracy is 0.724, total_loss is -71.2463, inv_loss is 6407.04, class_loss is 1.66468\n",
            "step 52 train accuracy is 0.784, total_loss is -67.7617, inv_loss is 7001.99, class_loss is 1.71474\n",
            "step 53 train accuracy is 0.7, total_loss is -70.3058, inv_loss is 7627.03, class_loss is 1.74676\n",
            "step 54 train accuracy is 0.72, total_loss is -84.8343, inv_loss is 7222.49, class_loss is 1.77327\n",
            "step 55 train accuracy is 0.716, total_loss is -75.045, inv_loss is 6573.41, class_loss is 1.74777\n",
            "step 56 train accuracy is 0.712, total_loss is -65.5624, inv_loss is 6973, class_loss is 1.74154\n",
            "step 57 train accuracy is 0.732, total_loss is -63.7642, inv_loss is 6764.58, class_loss is 1.72506\n",
            "step 58 train accuracy is 0.772, total_loss is -70.4747, inv_loss is 8043.23, class_loss is 1.71744\n",
            "step 59 train accuracy is 0.712, total_loss is -84.0009, inv_loss is 6669.33, class_loss is 1.78204\n",
            "step 60 train accuracy is 0.752, total_loss is -66.4445, inv_loss is 6761.9, class_loss is 1.75592\n",
            "step 61 train accuracy is 0.692, total_loss is -80.5788, inv_loss is 6791.78, class_loss is 1.72162\n",
            "step 62 train accuracy is 0.76, total_loss is -74.4188, inv_loss is 6902.72, class_loss is 1.70359\n",
            "step 63 train accuracy is 0.768, total_loss is -71.9945, inv_loss is 7052.17, class_loss is 1.71194\n",
            "step 64 train accuracy is 0.796, total_loss is -75.8911, inv_loss is 8219.83, class_loss is 1.69968\n",
            "step 65 train accuracy is 0.776, total_loss is -66.311, inv_loss is 7421.45, class_loss is 1.70459\n",
            "step 66 train accuracy is 0.784, total_loss is -63.1445, inv_loss is 7377.56, class_loss is 1.65081\n",
            "step 67 train accuracy is 0.792, total_loss is -70.8968, inv_loss is 6614.92, class_loss is 1.6674\n",
            "step 68 train accuracy is 0.728, total_loss is -71.3395, inv_loss is 7132.26, class_loss is 1.61145\n",
            "step 69 train accuracy is 0.78, total_loss is -69.8493, inv_loss is 6925.05, class_loss is 1.7551\n",
            "step 70 train accuracy is 0.716, total_loss is -64.2024, inv_loss is 6272.18, class_loss is 1.74186\n",
            "step 71 train accuracy is 0.776, total_loss is -68.2831, inv_loss is 8220.43, class_loss is 1.69555\n",
            "step 72 train accuracy is 0.828, total_loss is -68.8019, inv_loss is 7135.57, class_loss is 1.69352\n",
            "step 73 train accuracy is 0.8, total_loss is -67.3647, inv_loss is 7019.34, class_loss is 1.67498\n",
            "step 74 train accuracy is 0.796, total_loss is -67.0919, inv_loss is 6302.98, class_loss is 1.66361\n",
            "step 75 train accuracy is 0.756, total_loss is -67.6932, inv_loss is 6655.69, class_loss is 1.70954\n",
            "step 76 train accuracy is 0.776, total_loss is -71.2825, inv_loss is 7331.62, class_loss is 1.68349\n",
            "step 77 train accuracy is 0.752, total_loss is -63.7993, inv_loss is 7197.07, class_loss is 1.70353\n",
            "step 78 train accuracy is 0.776, total_loss is -64.0742, inv_loss is 6571.7, class_loss is 1.68831\n",
            "step 79 train accuracy is 0.884, total_loss is -100.617, inv_loss is 7088.2, class_loss is 1.6135\n",
            "step 80 train accuracy is 0.732, total_loss is -72.8133, inv_loss is 6940.06, class_loss is 1.76768\n",
            "step 81 train accuracy is 0.796, total_loss is -70.6538, inv_loss is 7460.22, class_loss is 1.68585\n",
            "step 82 train accuracy is 0.788, total_loss is -67.7667, inv_loss is 7189.29, class_loss is 1.66452\n",
            "step 83 train accuracy is 0.78, total_loss is -79.5969, inv_loss is 7642.83, class_loss is 1.64855\n",
            "step 84 train accuracy is 0.816, total_loss is -67.9295, inv_loss is 6475.77, class_loss is 1.68988\n",
            "step 85 train accuracy is 0.752, total_loss is -78.005, inv_loss is 8591.91, class_loss is 1.77681\n",
            "step 86 train accuracy is 0.736, total_loss is -69.4789, inv_loss is 7885.59, class_loss is 1.67769\n",
            "step 87 train accuracy is 0.82, total_loss is -70.502, inv_loss is 7153.93, class_loss is 1.68693\n",
            "step 88 train accuracy is 0.736, total_loss is -70.9872, inv_loss is 8427.17, class_loss is 1.67338\n",
            "step 89 train accuracy is 0.744, total_loss is -74.5097, inv_loss is 6947.24, class_loss is 1.70718\n",
            "step 90 train accuracy is 0.776, total_loss is -68.8024, inv_loss is 7788.32, class_loss is 1.65244\n",
            "step 91 train accuracy is 0.78, total_loss is -71.2692, inv_loss is 6407.04, class_loss is 1.63512\n",
            "step 92 train accuracy is 0.832, total_loss is -67.8032, inv_loss is 7001.99, class_loss is 1.69646\n",
            "step 93 train accuracy is 0.768, total_loss is -70.353, inv_loss is 7627.03, class_loss is 1.73635\n",
            "step 94 train accuracy is 0.804, total_loss is -84.8663, inv_loss is 7222.49, class_loss is 1.69346\n",
            "step 95 train accuracy is 0.776, total_loss is -75.0962, inv_loss is 6573.41, class_loss is 1.68861\n",
            "step 96 train accuracy is 0.768, total_loss is -65.6054, inv_loss is 6973, class_loss is 1.64214\n",
            "step 97 train accuracy is 0.78, total_loss is -63.7813, inv_loss is 6764.58, class_loss is 1.69316\n",
            "step 98 train accuracy is 0.76, total_loss is -70.4761, inv_loss is 8043.23, class_loss is 1.72576\n",
            "step 99 train accuracy is 0.748, total_loss is -84.0244, inv_loss is 6669.33, class_loss is 1.72486\n",
            "step 100 train accuracy is 0.764, total_loss is -66.4618, inv_loss is 6761.9, class_loss is 1.77998\n",
            "step 101 train accuracy is 0.76, total_loss is -80.5572, inv_loss is 6791.78, class_loss is 1.7227\n",
            "step 102 train accuracy is 0.824, total_loss is -74.4782, inv_loss is 6902.72, class_loss is 1.70695\n",
            "step 103 train accuracy is 0.748, total_loss is -72.0194, inv_loss is 7052.17, class_loss is 1.66581\n",
            "step 104 train accuracy is 0.804, total_loss is -75.8797, inv_loss is 8219.83, class_loss is 1.72438\n",
            "step 105 train accuracy is 0.772, total_loss is -66.3718, inv_loss is 7421.45, class_loss is 1.66074\n",
            "step 106 train accuracy is 0.74, total_loss is -63.1686, inv_loss is 7377.56, class_loss is 1.66253\n",
            "step 107 train accuracy is 0.772, total_loss is -70.8745, inv_loss is 6614.92, class_loss is 1.70295\n",
            "step 108 train accuracy is 0.76, total_loss is -71.3605, inv_loss is 7132.26, class_loss is 1.61207\n",
            "step 109 train accuracy is 0.772, total_loss is -69.864, inv_loss is 6925.05, class_loss is 1.73067\n",
            "step 110 train accuracy is 0.772, total_loss is -64.2237, inv_loss is 6272.18, class_loss is 1.72805\n",
            "step 111 train accuracy is 0.808, total_loss is -68.3256, inv_loss is 8220.43, class_loss is 1.70463\n",
            "step 112 train accuracy is 0.824, total_loss is -68.8149, inv_loss is 7135.57, class_loss is 1.68933\n",
            "step 113 train accuracy is 0.788, total_loss is -67.3371, inv_loss is 7019.34, class_loss is 1.70006\n",
            "step 114 train accuracy is 0.82, total_loss is -67.1204, inv_loss is 6302.98, class_loss is 1.65002\n",
            "step 115 train accuracy is 0.78, total_loss is -67.73, inv_loss is 6655.69, class_loss is 1.65724\n",
            "step 116 train accuracy is 0.836, total_loss is -71.3148, inv_loss is 7331.62, class_loss is 1.65565\n",
            "step 117 train accuracy is 0.816, total_loss is -63.8687, inv_loss is 7197.07, class_loss is 1.62769\n",
            "step 118 train accuracy is 0.796, total_loss is -64.1003, inv_loss is 6571.7, class_loss is 1.67773\n",
            "step 119 train accuracy is 0.892, total_loss is -100.613, inv_loss is 7088.2, class_loss is 1.62226\n",
            "step 120 train accuracy is 0.696, total_loss is -72.8363, inv_loss is 6940.06, class_loss is 1.73474\n",
            "step 121 train accuracy is 0.824, total_loss is -70.6374, inv_loss is 7460.22, class_loss is 1.68138\n",
            "step 122 train accuracy is 0.8, total_loss is -67.7854, inv_loss is 7189.29, class_loss is 1.64649\n",
            "step 123 train accuracy is 0.816, total_loss is -79.5905, inv_loss is 7642.83, class_loss is 1.638\n",
            "step 124 train accuracy is 0.808, total_loss is -67.9472, inv_loss is 6475.77, class_loss is 1.67007\n",
            "step 125 train accuracy is 0.768, total_loss is -78.0318, inv_loss is 8591.91, class_loss is 1.74238\n",
            "step 126 train accuracy is 0.82, total_loss is -69.4848, inv_loss is 7885.59, class_loss is 1.6527\n",
            "step 127 train accuracy is 0.816, total_loss is -70.5145, inv_loss is 7153.93, class_loss is 1.69821\n",
            "step 128 train accuracy is 0.756, total_loss is -70.9898, inv_loss is 8427.17, class_loss is 1.68595\n",
            "step 129 train accuracy is 0.764, total_loss is -74.523, inv_loss is 6947.24, class_loss is 1.68152\n",
            "step 130 train accuracy is 0.804, total_loss is -68.8346, inv_loss is 7788.32, class_loss is 1.65429\n",
            "step 131 train accuracy is 0.756, total_loss is -71.251, inv_loss is 6407.04, class_loss is 1.63998\n",
            "step 132 train accuracy is 0.812, total_loss is -67.8033, inv_loss is 7001.99, class_loss is 1.68873\n",
            "step 133 train accuracy is 0.732, total_loss is -70.333, inv_loss is 7627.03, class_loss is 1.7636\n",
            "step 134 train accuracy is 0.808, total_loss is -84.8714, inv_loss is 7222.49, class_loss is 1.69551\n",
            "step 135 train accuracy is 0.776, total_loss is -75.0816, inv_loss is 6573.41, class_loss is 1.6839\n",
            "step 136 train accuracy is 0.78, total_loss is -65.6165, inv_loss is 6973, class_loss is 1.64106\n",
            "step 137 train accuracy is 0.792, total_loss is -63.8472, inv_loss is 6764.58, class_loss is 1.66922\n",
            "step 138 train accuracy is 0.772, total_loss is -70.4923, inv_loss is 8043.23, class_loss is 1.69125\n",
            "step 139 train accuracy is 0.764, total_loss is -84.0236, inv_loss is 6669.33, class_loss is 1.72777\n",
            "step 140 train accuracy is 0.72, total_loss is -66.4459, inv_loss is 6761.9, class_loss is 1.75064\n",
            "step 141 train accuracy is 0.732, total_loss is -80.564, inv_loss is 6791.78, class_loss is 1.69073\n",
            "step 142 train accuracy is 0.84, total_loss is -74.5057, inv_loss is 6902.72, class_loss is 1.70066\n",
            "step 143 train accuracy is 0.768, total_loss is -72.0062, inv_loss is 7052.17, class_loss is 1.69661\n",
            "step 144 train accuracy is 0.812, total_loss is -75.8477, inv_loss is 8219.83, class_loss is 1.71431\n",
            "step 145 train accuracy is 0.784, total_loss is -66.3578, inv_loss is 7421.45, class_loss is 1.64082\n",
            "step 146 train accuracy is 0.808, total_loss is -63.1736, inv_loss is 7377.56, class_loss is 1.64869\n",
            "step 147 train accuracy is 0.768, total_loss is -70.9115, inv_loss is 6614.92, class_loss is 1.66045\n",
            "step 148 train accuracy is 0.784, total_loss is -71.3693, inv_loss is 7132.26, class_loss is 1.61046\n",
            "step 149 train accuracy is 0.736, total_loss is -69.8374, inv_loss is 6925.05, class_loss is 1.73742\n",
            "step 150 train accuracy is 0.768, total_loss is -64.2226, inv_loss is 6272.18, class_loss is 1.71977\n",
            "step 151 train accuracy is 0.808, total_loss is -68.3206, inv_loss is 8220.43, class_loss is 1.67799\n",
            "step 152 train accuracy is 0.816, total_loss is -68.8196, inv_loss is 7135.57, class_loss is 1.67428\n",
            "step 153 train accuracy is 0.828, total_loss is -67.3755, inv_loss is 7019.34, class_loss is 1.6754\n",
            "step 154 train accuracy is 0.796, total_loss is -67.0811, inv_loss is 6302.98, class_loss is 1.66651\n",
            "step 155 train accuracy is 0.748, total_loss is -67.7404, inv_loss is 6655.69, class_loss is 1.70576\n",
            "step 156 train accuracy is 0.808, total_loss is -71.2745, inv_loss is 7331.62, class_loss is 1.66446\n",
            "step 157 train accuracy is 0.82, total_loss is -63.8874, inv_loss is 7197.07, class_loss is 1.62864\n",
            "step 158 train accuracy is 0.776, total_loss is -64.1137, inv_loss is 6571.7, class_loss is 1.66068\n",
            "step 159 train accuracy is 0.896, total_loss is -100.594, inv_loss is 7088.2, class_loss is 1.61103\n",
            "step 160 train accuracy is 0.768, total_loss is -72.8478, inv_loss is 6940.06, class_loss is 1.67222\n",
            "step 161 train accuracy is 0.808, total_loss is -70.6544, inv_loss is 7460.22, class_loss is 1.65932\n",
            "step 162 train accuracy is 0.792, total_loss is -67.7743, inv_loss is 7189.29, class_loss is 1.63879\n",
            "step 163 train accuracy is 0.8, total_loss is -79.5931, inv_loss is 7642.83, class_loss is 1.64473\n",
            "step 164 train accuracy is 0.768, total_loss is -67.8634, inv_loss is 6475.77, class_loss is 1.74086\n",
            "step 165 train accuracy is 0.776, total_loss is -78.0274, inv_loss is 8591.91, class_loss is 1.72513\n",
            "step 166 train accuracy is 0.784, total_loss is -69.4695, inv_loss is 7885.59, class_loss is 1.67034\n",
            "step 167 train accuracy is 0.816, total_loss is -70.4968, inv_loss is 7153.93, class_loss is 1.68565\n",
            "step 168 train accuracy is 0.752, total_loss is -71.0136, inv_loss is 8427.17, class_loss is 1.66057\n",
            "step 169 train accuracy is 0.736, total_loss is -74.5207, inv_loss is 6947.24, class_loss is 1.68566\n",
            "step 170 train accuracy is 0.812, total_loss is -68.8278, inv_loss is 7788.32, class_loss is 1.6568\n",
            "step 171 train accuracy is 0.78, total_loss is -71.2762, inv_loss is 6407.04, class_loss is 1.61453\n",
            "step 172 train accuracy is 0.848, total_loss is -67.7961, inv_loss is 7001.99, class_loss is 1.66526\n",
            "step 173 train accuracy is 0.728, total_loss is -70.3963, inv_loss is 7627.03, class_loss is 1.73765\n",
            "step 174 train accuracy is 0.772, total_loss is -84.8304, inv_loss is 7222.49, class_loss is 1.69692\n",
            "step 175 train accuracy is 0.768, total_loss is -75.0571, inv_loss is 6573.41, class_loss is 1.69563\n",
            "step 176 train accuracy is 0.796, total_loss is -65.6246, inv_loss is 6973, class_loss is 1.66186\n",
            "step 177 train accuracy is 0.792, total_loss is -63.8375, inv_loss is 6764.58, class_loss is 1.67299\n",
            "step 178 train accuracy is 0.796, total_loss is -70.5132, inv_loss is 8043.23, class_loss is 1.65848\n",
            "step 179 train accuracy is 0.776, total_loss is -84.0504, inv_loss is 6669.33, class_loss is 1.73233\n",
            "step 180 train accuracy is 0.764, total_loss is -66.4575, inv_loss is 6761.9, class_loss is 1.74002\n",
            "step 181 train accuracy is 0.764, total_loss is -80.6016, inv_loss is 6791.78, class_loss is 1.68496\n",
            "step 182 train accuracy is 0.82, total_loss is -74.512, inv_loss is 6902.72, class_loss is 1.66995\n",
            "step 183 train accuracy is 0.804, total_loss is -72.039, inv_loss is 7052.17, class_loss is 1.66827\n",
            "step 184 train accuracy is 0.776, total_loss is -75.8341, inv_loss is 8219.83, class_loss is 1.73105\n",
            "step 185 train accuracy is 0.808, total_loss is -66.3631, inv_loss is 7421.45, class_loss is 1.66267\n",
            "step 186 train accuracy is 0.756, total_loss is -63.1232, inv_loss is 7377.56, class_loss is 1.65624\n",
            "step 187 train accuracy is 0.788, total_loss is -70.8856, inv_loss is 6614.92, class_loss is 1.68208\n",
            "step 188 train accuracy is 0.728, total_loss is -71.3309, inv_loss is 7132.26, class_loss is 1.59793\n",
            "step 189 train accuracy is 0.76, total_loss is -69.8688, inv_loss is 6925.05, class_loss is 1.74396\n",
            "step 190 train accuracy is 0.76, total_loss is -64.2219, inv_loss is 6272.18, class_loss is 1.73303\n",
            "step 191 train accuracy is 0.8, total_loss is -68.3325, inv_loss is 8220.43, class_loss is 1.69077\n",
            "step 192 train accuracy is 0.828, total_loss is -68.8344, inv_loss is 7135.57, class_loss is 1.65544\n",
            "step 193 train accuracy is 0.836, total_loss is -67.3861, inv_loss is 7019.34, class_loss is 1.66325\n",
            "step 194 train accuracy is 0.792, total_loss is -67.0989, inv_loss is 6302.98, class_loss is 1.69371\n",
            "step 195 train accuracy is 0.804, total_loss is -67.7723, inv_loss is 6655.69, class_loss is 1.66043\n",
            "step 196 train accuracy is 0.804, total_loss is -71.2962, inv_loss is 7331.62, class_loss is 1.66822\n",
            "step 197 train accuracy is 0.816, total_loss is -63.8582, inv_loss is 7197.07, class_loss is 1.60987\n",
            "step 198 train accuracy is 0.816, total_loss is -64.1238, inv_loss is 6571.7, class_loss is 1.64497\n",
            "step 199 train accuracy is 0.888, total_loss is -100.566, inv_loss is 7088.2, class_loss is 1.63242\n",
            "beta is 0.01, test accuracy is 0.7969\n",
            "Training...\n",
            "step 0 train accuracy is 0.456, total_loss is -743.396, inv_loss is 6940.06, class_loss is 2.02891\n",
            "step 1 train accuracy is 0.644, total_loss is -721.461, inv_loss is 7460.22, class_loss is 1.83367\n",
            "step 2 train accuracy is 0.604, total_loss is -692.286, inv_loss is 7189.29, class_loss is 1.86305\n",
            "step 3 train accuracy is 0.48, total_loss is -810.502, inv_loss is 7642.83, class_loss is 1.96985\n",
            "step 4 train accuracy is 0.588, total_loss is -694.203, inv_loss is 6475.77, class_loss is 2.01436\n",
            "step 5 train accuracy is 0.576, total_loss is -795.031, inv_loss is 8591.91, class_loss is 1.86913\n",
            "step 6 train accuracy is 0.584, total_loss is -709.397, inv_loss is 7885.59, class_loss is 1.86179\n",
            "step 7 train accuracy is 0.616, total_loss is -719.817, inv_loss is 7153.93, class_loss is 1.87765\n",
            "step 8 train accuracy is 0.58, total_loss is -724.978, inv_loss is 8427.17, class_loss is 1.91599\n",
            "step 9 train accuracy is 0.52, total_loss is -760.329, inv_loss is 6947.24, class_loss is 1.94477\n",
            "step 10 train accuracy is 0.588, total_loss is -703.419, inv_loss is 7788.32, class_loss is 1.89338\n",
            "step 11 train accuracy is 0.488, total_loss is -726.954, inv_loss is 6407.04, class_loss is 1.92438\n",
            "step 12 train accuracy is 0.616, total_loss is -692.379, inv_loss is 7001.99, class_loss is 1.90812\n",
            "step 13 train accuracy is 0.58, total_loss is -718.051, inv_loss is 7627.03, class_loss is 1.89722\n",
            "step 14 train accuracy is 0.636, total_loss is -863.019, inv_loss is 7222.49, class_loss is 1.89091\n",
            "step 15 train accuracy is 0.524, total_loss is -765.484, inv_loss is 6573.41, class_loss is 1.92325\n",
            "step 16 train accuracy is 0.508, total_loss is -671.345, inv_loss is 6973, class_loss is 1.79644\n",
            "step 17 train accuracy is 0.632, total_loss is -653.58, inv_loss is 6764.58, class_loss is 1.79423\n",
            "step 18 train accuracy is 0.64, total_loss is -719.895, inv_loss is 8043.23, class_loss is 1.78606\n",
            "step 19 train accuracy is 0.64, total_loss is -855.068, inv_loss is 6669.33, class_loss is 1.89092\n",
            "step 20 train accuracy is 0.62, total_loss is -679.427, inv_loss is 6761.9, class_loss is 1.87894\n",
            "step 21 train accuracy is 0.624, total_loss is -821.568, inv_loss is 6791.78, class_loss is 1.83275\n",
            "step 22 train accuracy is 0.66, total_loss is -759.363, inv_loss is 6902.72, class_loss is 1.82255\n",
            "step 23 train accuracy is 0.604, total_loss is -734.832, inv_loss is 7052.17, class_loss is 1.82072\n",
            "step 24 train accuracy is 0.652, total_loss is -773.89, inv_loss is 8219.83, class_loss is 1.83649\n",
            "step 25 train accuracy is 0.664, total_loss is -678.741, inv_loss is 7421.45, class_loss is 1.79727\n",
            "step 26 train accuracy is 0.564, total_loss is -646.805, inv_loss is 7377.56, class_loss is 1.80197\n",
            "step 27 train accuracy is 0.6, total_loss is -724.406, inv_loss is 6614.92, class_loss is 1.82647\n",
            "step 28 train accuracy is 0.62, total_loss is -728.821, inv_loss is 7132.26, class_loss is 1.7414\n",
            "step 29 train accuracy is 0.7, total_loss is -713.816, inv_loss is 6925.05, class_loss is 1.85482\n",
            "step 30 train accuracy is 0.656, total_loss is -657.37, inv_loss is 6272.18, class_loss is 1.80501\n",
            "step 31 train accuracy is 0.656, total_loss is -698.578, inv_loss is 8220.43, class_loss is 1.78297\n",
            "step 32 train accuracy is 0.692, total_loss is -703.409, inv_loss is 7135.57, class_loss is 1.79882\n",
            "step 33 train accuracy is 0.696, total_loss is -689.173, inv_loss is 7019.34, class_loss is 1.83054\n",
            "step 34 train accuracy is 0.736, total_loss is -685.928, inv_loss is 6302.98, class_loss is 1.74554\n",
            "step 35 train accuracy is 0.732, total_loss is -692.013, inv_loss is 6655.69, class_loss is 1.79599\n",
            "step 36 train accuracy is 0.688, total_loss is -728.182, inv_loss is 7331.62, class_loss is 1.7962\n",
            "step 37 train accuracy is 0.72, total_loss is -653.688, inv_loss is 7197.07, class_loss is 1.72667\n",
            "step 38 train accuracy is 0.724, total_loss is -655.634, inv_loss is 6571.7, class_loss is 1.73707\n",
            "step 39 train accuracy is 0.848, total_loss is -1021.23, inv_loss is 7088.2, class_loss is 1.68815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-1f8fd9b7894d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mtest_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   \u001b[0mtest_accs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logreg_acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-1f8fd9b7894d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loss_beta, learning_rate, Epoch)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverter_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}